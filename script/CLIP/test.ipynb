{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "from data import *\n",
    "from encoders import *\n",
    "from config import *\n",
    "from models import *\n",
    "from evals import *\n",
    "print(\"using device: \", device)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import importlib\n",
    "# import models\n",
    "# importlib.reload(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ugly engineering here\n",
    "df = pd.read_csv('../../data/HR_events.csv')\n",
    "df_y = pd.read_excel('../../data/PAS Challenge Outcome Data.xlsx', engine=\"calamine\")\n",
    "df_y = df_y[['VitalID', 'Died']]\n",
    "df = df.merge(df_y, on='VitalID', how='left')\n",
    "df['label'] = df.index.to_series()\n",
    "df['text'] = df['Died'].apply(lambda x: 'This infant will die in 7 days. ' if x == 1 else 'This infant will survive. ')\n",
    "# df['text'] = df['text'] +' '+ df['event_description'].astype(str)\n",
    "\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, stratify=df['text'])    \n",
    "# df_train['text'] = df_train['text'] +' '+ df_train['event_description'].astype(str)\n",
    "# df_test['text'] = df_test['text'] +' '+ df_test['event_description'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_train\n",
    "df_new['outcome'] = df_new['Died'].apply(lambda x: 'class1' if x == 1 else 'class2')\n",
    "df_new_y = pd.get_dummies(df_new['outcome'])\n",
    "y_true_train = torch.tensor(df_new_y.values)\n",
    "ts_df_train = df_new.loc[:,'1':'300']\n",
    "\n",
    "\n",
    "df_new = df_test\n",
    "df_new['outcome'] = df_new['Died'].apply(lambda x: 'class1' if x == 1 else 'class2')\n",
    "df_new_y = pd.get_dummies(df_new['outcome'])\n",
    "y_true_test = torch.tensor(df_new_y.values)\n",
    "ts_df_test = df_new.loc[:,'1':'300']\n",
    "\n",
    "txt_ls = ['die in 7 days', 'survive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# text_encoder_name = 'dmis-lab/biobert-base-cased-v1.2'\n",
    "text_encoder_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "ts_encoder_name = 'hr_vae_linear_medium'\n",
    "ts_f_train, tx_f_train, labels_train = get_features(df_train,ts_encoder_name,text_encoder_name)\n",
    "train_dataloader = CLIPDataset(ts_f_train, tx_f_train, labels_train).dataloader(batch_size=128)\n",
    "\n",
    "ts_f_test, tx_f_test, labels_test = get_features(df_test, ts_encoder_name,text_encoder_name)\n",
    "test_dataloader = CLIPDataset(ts_f_test, tx_f_test, labels_test).dataloader(batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "CLIPModel                                1\n",
      "├─Sequential: 1-1                        --\n",
      "│    └─Linear: 2-1                       4,224\n",
      "│    └─LeakyReLU: 2-2                    --\n",
      "│    └─Linear: 2-3                       33,024\n",
      "│    └─LeakyReLU: 2-4                    --\n",
      "│    └─Linear: 2-5                       32,896\n",
      "│    └─LeakyReLU: 2-6                    --\n",
      "│    └─Linear: 2-7                       16,512\n",
      "├─Sequential: 1-2                        --\n",
      "│    └─Linear: 2-8                       98,432\n",
      "│    └─LeakyReLU: 2-9                    --\n",
      "│    └─Linear: 2-10                      33,024\n",
      "│    └─LeakyReLU: 2-11                   --\n",
      "│    └─Linear: 2-12                      65,792\n",
      "│    └─LeakyReLU: 2-13                   --\n",
      "│    └─Linear: 2-14                      32,896\n",
      "│    └─LeakyReLU: 2-15                   --\n",
      "│    └─Linear: 2-16                      16,512\n",
      "=================================================================\n",
      "Total params: 333,313\n",
      "Trainable params: 333,313\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "overwrite = True\n",
    "model_path = './results/clip_hr_death_binary_by_similarity.pth' \n",
    "loss_path = './results/clip_hr_death_binary_by_similarity_losses.pth'\n",
    "eval_metrics_path = './results/clip_hr_death_binary_by_similarity_eval_metrics.pth'\n",
    "# Initialize model\n",
    "model = CLIPModel(\n",
    "        ts_dim=ts_f_train.shape[1],    # 32\n",
    "        text_dim=tx_f_train.shape[1],  # 768\n",
    "        projection_dim=128\n",
    "    )\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',           # Reduce LR when metric stops decreasing\n",
    "    factor=0.9,          # Multiply LR by this factor\n",
    "    patience=50,          # Number of epochs to wait before reducing LR\n",
    "    min_lr=1e-10         # Don't reduce LR below this value\n",
    ")\n",
    "\n",
    "num_epochs = 50\n",
    "num_saves = 200 # total epochs will be num_saves * num_epochs\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "train_eval_metrics_list = []\n",
    "test_eval_metrics_list = []\n",
    "loss_type = 'similarity_org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "\tTraining Loss: 230.220790\n",
      "\tTesting Loss: 204.151367\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 221.885682\n",
      "\tTesting Loss: 204.151291\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 221.885599\n",
      "\tTesting Loss: 204.151077\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 221.885492\n",
      "\tTesting Loss: 204.150899\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 221.885446\n",
      "\tTesting Loss: 204.150833\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 221.885413\n",
      "\tTesting Loss: 204.150803\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 221.885372\n",
      "\tTesting Loss: 204.150772\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 221.885320\n",
      "\tTesting Loss: 204.150711\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 221.885238\n",
      "\tTesting Loss: 204.150564\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 221.885097\n",
      "\tTesting Loss: 204.150330\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 221.884900\n",
      "\tTesting Loss: 204.150040\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 221.884745\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 221.884645\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 221.884623\n",
      "\tTesting Loss: 204.149938\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 221.884622\n",
      "\tTesting Loss: 204.149913\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 221.884616\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 221.884604\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 221.884605\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 221.884599\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 221.884600\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 221.884598\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 221.884596\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 221.884595\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 221.884596\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 221.884597\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 221.884595\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 221.884598\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 221.884589\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 221.884594\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 221.884595\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 221.884591\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 221.884595\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 221.884593\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 221.884593\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 221.884593\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 221.884597\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 221.884593\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 221.884593\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 221.884594\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 221.884593\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 221.884594\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 221.884598\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 221.884591\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 221.884600\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 221.884600\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 221.884598\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 221.884597\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 221.884599\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 221.884596\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 221.884597\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 221.884595\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 221.884592\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 221.884585\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 221.884591\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 221.884597\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 221.884593\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 221.884593\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 221.884595\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 221.884595\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 221.884592\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 221.884591\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 221.884595\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 221.884593\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 221.884594\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 221.884593\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 221.884594\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 221.884588\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 221.884591\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 221.884590\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 221.884591\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 221.884588\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 221.884594\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 221.884594\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 221.884588\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 221.884590\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 221.884590\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 221.884589\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 221.884591\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 221.884595\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 221.884588\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 221.884590\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 221.884584\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 221.884591\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 221.884588\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 221.884586\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 221.884590\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 221.884591\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 221.884589\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 221.884590\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 221.884594\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 221.884591\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 221.884588\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 221.884591\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 221.884592\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 221.884590\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 221.884591\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 221.884590\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 221.884586\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 221.884590\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 221.884589\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 221.884590\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 221.884593\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 221.884587\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 221.884582\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 221.884580\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 221.884588\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 221.884670\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 221.884577\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 221.884585\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 221.884582\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 221.884581\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 221.884586\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 221.884584\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 221.884583\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 221.884584\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 221.884581\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 221.884585\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 221.884583\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 221.884585\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 221.884588\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 221.884580\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 221.884583\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 221.884581\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 221.884585\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 221.884583\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 221.884581\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 221.884584\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 221.884584\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 221.884582\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 221.884583\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 221.884580\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 221.884585\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 221.884581\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 221.884584\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 221.884585\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 221.884584\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 221.884583\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 221.884583\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 221.884584\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 221.884581\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 221.884588\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 221.884585\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 221.884584\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 221.884584\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 221.884582\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 221.884586\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 221.884582\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 221.884583\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 221.884583\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 221.884586\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 221.884577\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 221.884581\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 221.884577\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 221.884574\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 221.884571\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 221.884571\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 221.884581\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 221.884577\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 221.884583\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 221.884571\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 221.884577\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 221.884581\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 221.884580\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 221.884575\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 221.884575\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 221.884580\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 221.884647\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 221.884577\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 221.884580\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 221.884583\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 221.884576\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 221.884581\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 221.884577\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 221.884581\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 221.884585\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 221.884580\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 221.884585\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 221.884581\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 221.884583\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 221.884576\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 221.884577\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 221.884578\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 221.884576\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 221.884580\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 221.884574\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 221.884576\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 221.884575\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 221.884576\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 221.884576\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 221.884581\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 221.884572\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 221.884576\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 221.884567\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 221.884570\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 221.884570\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 221.884572\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 221.884572\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 221.884576\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 221.884576\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 221.884575\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 221.884577\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 221.884575\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 221.884580\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 221.884574\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 221.884575\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 221.884577\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 221.884577\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 221.884576\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 221.884574\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 221.884574\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 221.884576\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 221.884580\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 221.884575\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 221.884575\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 221.884577\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 221.884576\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 221.884577\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 221.884575\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 221.884572\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 221.884576\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 221.884576\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 221.884574\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 221.884576\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 221.884575\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 221.884576\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 221.884575\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 221.884573\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 221.884576\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 221.884573\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 221.884576\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 221.884576\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 221.884579\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 221.884572\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 221.884570\n",
      "\tTesting Loss: 204.149872\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 221.884570\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 221.884574\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 221.884574\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 221.884572\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 221.884572\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 221.884574\n",
      "\tTesting Loss: 204.149877\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 221.884574\n",
      "\tTesting Loss: 204.149877\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 221.884574\n",
      "\tTesting Loss: 204.149877\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 221.884576\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 221.884572\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 221.884572\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 221.884574\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 221.884575\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 221.884575\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 221.884574\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 221.884574\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 221.884574\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 221.884570\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 221.884571\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 221.884572\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 221.884572\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 221.884574\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 221.884572\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 221.884571\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 221.884569\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 221.884575\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 221.884572\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 221.884575\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 221.884574\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 221.884567\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 221.884567\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 221.884571\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 221.884572\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 221.884567\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 221.884573\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 221.884570\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 221.884571\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 221.884571\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 221.884580\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 221.884571\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 221.884572\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 221.884575\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 221.884570\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 221.884569\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 221.884570\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 221.884577\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 221.884577\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 221.884571\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 221.884571\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 221.884563\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 221.884567\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 221.884571\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 221.884569\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 221.884569\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 221.884571\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 221.884567\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 221.884565\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 221.884566\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 221.884574\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 221.884567\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 221.884565\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 221.884574\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 221.884566\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 221.884571\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 221.884572\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 221.884574\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 221.884570\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 221.884565\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 221.884569\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 221.884571\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 221.884566\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 221.884574\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 221.884569\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 221.884568\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 221.884567\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 221.884563\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 221.884569\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 221.884570\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 221.884568\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 221.884570\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 221.884566\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 221.884569\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 221.884565\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 221.884567\n",
      "\tTesting Loss: 204.149872\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 221.884563\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 221.884567\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 221.884567\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 221.884565\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 221.884569\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 221.884567\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 221.884572\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 221.884569\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 221.884569\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 221.884571\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 221.884569\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 221.884574\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 221.884566\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 221.884568\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 221.884565\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 221.884569\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 221.884567\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 221.884566\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 221.884562\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 221.884562\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 221.884569\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 221.884561\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 221.884567\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 221.884565\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 221.884566\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 221.884565\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 221.884565\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 221.884569\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 221.884562\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 221.884564\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 221.884563\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 221.884565\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 221.884565\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 221.884562\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 221.884558\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 221.884563\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 221.884563\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 221.884561\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 221.884565\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 221.884562\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 221.884562\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 221.884561\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 221.884562\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 221.884563\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 221.884563\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 221.884560\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 221.884562\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 221.884562\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 221.884560\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 221.884566\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 221.884560\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 221.884560\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 221.884565\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 221.884566\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 221.884565\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 221.884565\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 221.884563\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 221.884564\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 221.884564\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 221.884565\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 221.884563\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 221.884566\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 221.884563\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 221.884565\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 221.884560\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 221.884560\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 221.884564\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 221.884559\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 221.884563\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 221.884560\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 221.884557\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 221.884562\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 221.884560\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 221.884560\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 221.884558\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 221.884557\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 221.884560\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 221.884561\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 221.884560\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 221.884560\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 221.884563\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 221.884560\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 221.884562\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 221.884556\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 221.884560\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 221.884557\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 221.884557\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 221.884560\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 221.884558\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 221.884557\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 221.884557\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 221.884556\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 221.884562\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 221.884560\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 221.884562\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 221.884561\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 221.884560\n",
      "\tTesting Loss: 204.149877\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 221.884561\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 221.884560\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 221.884562\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 221.884558\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 221.884563\n",
      "\tTesting Loss: 204.149877\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 221.884559\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 221.884560\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 221.884563\n",
      "\tTesting Loss: 204.149877\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 221.884562\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 221.884562\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 221.884559\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 221.884566\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 221.884562\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 221.884563\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 221.884568\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 221.884562\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 221.884561\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 221.884561\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 221.884560\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 221.884565\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 221.884556\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 221.884555\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 221.884554\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 221.884556\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 221.884556\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 221.884556\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 221.884555\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 221.884553\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 221.884556\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 221.884553\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 221.884548\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 221.884551\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 221.884565\n",
      "\tTesting Loss: 204.149877\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 221.884558\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 221.884558\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 221.884555\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 221.884556\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 221.884555\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 221.884552\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 221.884555\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 221.884554\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 221.884559\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 221.884557\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 221.884555\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 221.884555\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 221.884556\n",
      "\tTesting Loss: 204.149882\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 221.884553\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 221.884555\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 221.884558\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 221.884553\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 221.884553\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 221.884555\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 221.884553\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 221.884556\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 221.884556\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 221.884556\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 221.884555\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 221.884555\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 221.884552\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 221.884549\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 221.884549\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 221.884553\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 221.884551\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 221.884551\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 221.884553\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 221.884549\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 221.884549\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 221.884549\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 221.884552\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 221.884549\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 221.884552\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 221.884553\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 221.884543\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 221.884549\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 221.884547\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 221.884546\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 221.884544\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 221.884544\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 221.884547\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 221.884543\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 221.884546\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 221.884546\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 221.884545\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 221.884544\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 221.884542\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 221.884547\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 221.884546\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 221.884546\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 221.884546\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 221.884544\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 221.884539\n",
      "\tTesting Loss: 204.149918\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 221.884557\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 221.884543\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 221.884539\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 221.884548\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 221.884545\n",
      "\tTesting Loss: 204.149913\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 221.884543\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 221.884535\n",
      "\tTesting Loss: 204.149892\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 221.884543\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 221.884538\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 221.884543\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 221.884544\n",
      "\tTesting Loss: 204.149913\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 221.884545\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 221.884544\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 221.884543\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 221.884542\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 221.884541\n",
      "\tTesting Loss: 204.149887\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 221.884538\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 221.884541\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 221.884543\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 221.884548\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 221.884542\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 221.884543\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 221.884543\n",
      "\tTesting Loss: 204.149913\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 221.884539\n",
      "\tTesting Loss: 204.149913\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 221.884544\n",
      "\tTesting Loss: 204.149913\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 221.884541\n",
      "\tTesting Loss: 204.149913\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 221.884543\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 221.884538\n",
      "\tTesting Loss: 204.149913\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 221.884535\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 221.884535\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 221.884541\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 221.884539\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 221.884535\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 221.884534\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 221.884539\n",
      "\tTesting Loss: 204.149918\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 221.884536\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 221.884538\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 221.884535\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 221.884539\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 221.884539\n",
      "\tTesting Loss: 204.149938\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 221.884548\n",
      "\tTesting Loss: 204.149963\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 221.884546\n",
      "\tTesting Loss: 204.149928\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 221.884539\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 221.884535\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 221.884530\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 221.884531\n",
      "\tTesting Loss: 204.149918\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 221.884529\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 221.884533\n",
      "\tTesting Loss: 204.149918\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 221.884529\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 221.884529\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 221.884527\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 221.884527\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 221.884533\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 221.884530\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 221.884535\n",
      "\tTesting Loss: 204.149933\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 221.884535\n",
      "\tTesting Loss: 204.149989\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 221.884549\n",
      "\tTesting Loss: 204.149999\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 221.884566\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 221.884563\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 221.884538\n",
      "\tTesting Loss: 204.149902\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 221.884539\n",
      "\tTesting Loss: 204.149907\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 221.884528\n",
      "\tTesting Loss: 204.149913\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 221.884526\n",
      "\tTesting Loss: 204.149918\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 221.884526\n",
      "\tTesting Loss: 204.149913\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 221.884523\n",
      "\tTesting Loss: 204.149913\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 221.884523\n",
      "\tTesting Loss: 204.149918\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 221.884519\n",
      "\tTesting Loss: 204.149918\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 221.884521\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 221.884525\n",
      "\tTesting Loss: 204.149928\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 221.884527\n",
      "\tTesting Loss: 204.149933\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 221.884525\n",
      "\tTesting Loss: 204.149943\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 221.884523\n",
      "\tTesting Loss: 204.149953\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 221.884513\n",
      "\tTesting Loss: 204.149958\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 221.884506\n",
      "\tTesting Loss: 204.149948\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 221.884511\n",
      "\tTesting Loss: 204.149943\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 221.884530\n",
      "\tTesting Loss: 204.149913\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 221.884488\n",
      "\tTesting Loss: 204.149943\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 221.884514\n",
      "\tTesting Loss: 204.149989\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 221.884595\n",
      "\tTesting Loss: 204.150004\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 221.884544\n",
      "\tTesting Loss: 204.149953\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 221.884686\n",
      "\tTesting Loss: 204.150009\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 221.884593\n",
      "\tTesting Loss: 204.149994\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 221.884566\n",
      "\tTesting Loss: 204.149918\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 221.884539\n",
      "\tTesting Loss: 204.149938\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 221.884538\n",
      "\tTesting Loss: 204.149933\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 221.884535\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 221.884536\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 221.884528\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 221.884517\n",
      "\tTesting Loss: 204.149897\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 221.884527\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 221.884534\n",
      "\tTesting Loss: 204.149928\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 221.884532\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 221.884531\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 221.884530\n",
      "\tTesting Loss: 204.149913\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 221.884525\n",
      "\tTesting Loss: 204.149913\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 221.884527\n",
      "\tTesting Loss: 204.149918\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 221.884530\n",
      "\tTesting Loss: 204.149913\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 221.884527\n",
      "\tTesting Loss: 204.149913\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 221.884522\n",
      "\tTesting Loss: 204.149918\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 221.884526\n",
      "\tTesting Loss: 204.149918\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 221.884528\n",
      "\tTesting Loss: 204.149918\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 221.884527\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 221.884518\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 221.884518\n",
      "\tTesting Loss: 204.149928\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 221.884521\n",
      "\tTesting Loss: 204.149918\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 221.884518\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 221.884520\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 221.884519\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 221.884515\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 221.884520\n",
      "\tTesting Loss: 204.149918\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 221.884517\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 221.884514\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 221.884516\n",
      "\tTesting Loss: 204.149928\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 221.884515\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 221.884519\n",
      "\tTesting Loss: 204.149913\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 221.884518\n",
      "\tTesting Loss: 204.149918\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 221.884517\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 221.884518\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 221.884519\n",
      "\tTesting Loss: 204.149928\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 221.884519\n",
      "\tTesting Loss: 204.149928\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 221.884517\n",
      "\tTesting Loss: 204.149928\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 221.884515\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 221.884516\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 221.884509\n",
      "\tTesting Loss: 204.149933\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 221.884515\n",
      "\tTesting Loss: 204.149933\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 221.884516\n",
      "\tTesting Loss: 204.149938\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 221.884513\n",
      "\tTesting Loss: 204.149948\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 221.884514\n",
      "\tTesting Loss: 204.149974\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 221.884520\n",
      "\tTesting Loss: 204.150070\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 221.884539\n",
      "\tTesting Loss: 204.150172\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 221.884610\n",
      "\tTesting Loss: 204.149984\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 221.884570\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 221.884523\n",
      "\tTesting Loss: 204.149928\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 221.884519\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 221.884509\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 221.884510\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 221.884500\n",
      "\tTesting Loss: 204.149928\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 221.884499\n",
      "\tTesting Loss: 204.149913\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 221.884498\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 221.884502\n",
      "\tTesting Loss: 204.149913\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 221.884499\n",
      "\tTesting Loss: 204.149918\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 221.884495\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 221.884492\n",
      "\tTesting Loss: 204.149918\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 221.884486\n",
      "\tTesting Loss: 204.149923\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 221.884481\n",
      "\tTesting Loss: 204.149933\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 221.884459\n",
      "\tTesting Loss: 204.149913\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 221.884409\n",
      "\tTesting Loss: 204.149928\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 221.884394\n",
      "\tTesting Loss: 204.149958\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 221.884462\n",
      "\tTesting Loss: 204.149933\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 221.884420\n",
      "\tTesting Loss: 204.150055\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 221.884478\n",
      "\tTesting Loss: 204.150019\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 221.884505\n",
      "\tTesting Loss: 204.149948\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 221.884457\n",
      "\tTesting Loss: 204.149933\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 221.884453\n",
      "\tTesting Loss: 204.149933\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 221.884368\n",
      "\tTesting Loss: 204.150065\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 221.884286\n",
      "\tTesting Loss: 204.150080\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 221.884284\n",
      "\tTesting Loss: 204.149994\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 221.884420\n",
      "\tTesting Loss: 204.150192\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 221.884365\n",
      "\tTesting Loss: 204.150126\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 221.883968\n",
      "\tTesting Loss: 204.150157\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 221.884460\n",
      "\tTesting Loss: 204.150411\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 221.883881\n",
      "\tTesting Loss: 204.150406\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 221.883977\n",
      "\tTesting Loss: 204.151174\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 221.884492\n",
      "\tTesting Loss: 204.150431\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 221.884600\n",
      "\tTesting Loss: 204.149979\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 221.884272\n",
      "\tTesting Loss: 204.150299\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 221.884050\n",
      "\tTesting Loss: 204.150365\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 221.883972\n",
      "\tTesting Loss: 204.150375\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 221.883873\n",
      "\tTesting Loss: 204.150726\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 221.883811\n",
      "\tTesting Loss: 204.150889\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 221.883631\n",
      "\tTesting Loss: 204.150772\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 221.883511\n",
      "\tTesting Loss: 204.151123\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 221.883441\n",
      "\tTesting Loss: 204.151291\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 221.883389\n",
      "\tTesting Loss: 204.151210\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 221.883387\n",
      "\tTesting Loss: 204.151388\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 221.883310\n",
      "\tTesting Loss: 204.151388\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 221.883298\n",
      "\tTesting Loss: 204.151408\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 221.883304\n",
      "\tTesting Loss: 204.151484\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 221.883281\n",
      "\tTesting Loss: 204.151474\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 221.883238\n",
      "\tTesting Loss: 204.151357\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 221.883249\n",
      "\tTesting Loss: 204.151586\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 221.883268\n",
      "\tTesting Loss: 204.151321\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 221.883251\n",
      "\tTesting Loss: 204.151413\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 221.883312\n",
      "\tTesting Loss: 204.151850\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 221.883400\n",
      "\tTesting Loss: 204.151759\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 221.883686\n",
      "\tTesting Loss: 204.151082\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 221.884066\n",
      "\tTesting Loss: 204.150538\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 221.884179\n",
      "\tTesting Loss: 204.150401\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 221.883641\n",
      "\tTesting Loss: 204.150716\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 221.883703\n",
      "\tTesting Loss: 204.151072\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 221.883816\n",
      "\tTesting Loss: 204.150462\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 221.884074\n",
      "\tTesting Loss: 204.150457\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 221.883481\n",
      "\tTesting Loss: 204.151026\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 221.883705\n",
      "\tTesting Loss: 204.151169\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 221.883999\n",
      "\tTesting Loss: 204.150731\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 221.884485\n",
      "\tTesting Loss: 204.150121\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 221.884079\n",
      "\tTesting Loss: 204.150182\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 221.883611\n",
      "\tTesting Loss: 204.150894\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 221.883481\n",
      "\tTesting Loss: 204.151123\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 221.883870\n",
      "\tTesting Loss: 204.150681\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 221.884102\n",
      "\tTesting Loss: 204.150533\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 221.883555\n",
      "\tTesting Loss: 204.150904\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 221.883724\n",
      "\tTesting Loss: 204.151047\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 221.883931\n",
      "\tTesting Loss: 204.150762\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 221.884113\n",
      "\tTesting Loss: 204.150742\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 221.883787\n",
      "\tTesting Loss: 204.150655\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 221.883767\n",
      "\tTesting Loss: 204.150935\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 221.883457\n",
      "\tTesting Loss: 204.151169\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 221.883533\n",
      "\tTesting Loss: 204.151199\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 221.883651\n",
      "\tTesting Loss: 204.150940\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 221.883771\n",
      "\tTesting Loss: 204.150670\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 221.883399\n",
      "\tTesting Loss: 204.151255\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 221.883646\n",
      "\tTesting Loss: 204.151204\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 221.883649\n",
      "\tTesting Loss: 204.150736\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 221.883701\n",
      "\tTesting Loss: 204.150889\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 221.883372\n",
      "\tTesting Loss: 204.151449\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 221.883564\n",
      "\tTesting Loss: 204.151240\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 221.883583\n",
      "\tTesting Loss: 204.150833\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 221.883609\n",
      "\tTesting Loss: 204.151067\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 221.883350\n",
      "\tTesting Loss: 204.151520\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 221.883525\n",
      "\tTesting Loss: 204.151301\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 221.883565\n",
      "\tTesting Loss: 204.150930\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 221.883560\n",
      "\tTesting Loss: 204.151169\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 221.883348\n",
      "\tTesting Loss: 204.151540\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 221.883423\n",
      "\tTesting Loss: 204.151459\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 221.883539\n",
      "\tTesting Loss: 204.151276\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 221.883512\n",
      "\tTesting Loss: 204.151138\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 221.883388\n",
      "\tTesting Loss: 204.151382\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 221.883357\n",
      "\tTesting Loss: 204.151540\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 221.883502\n",
      "\tTesting Loss: 204.151443\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 221.883531\n",
      "\tTesting Loss: 204.151230\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 221.883435\n",
      "\tTesting Loss: 204.151316\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 221.883315\n",
      "\tTesting Loss: 204.151571\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 221.883344\n",
      "\tTesting Loss: 204.151586\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 221.883455\n",
      "\tTesting Loss: 204.151499\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 221.883469\n",
      "\tTesting Loss: 204.151418\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 221.883347\n",
      "\tTesting Loss: 204.151372\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 221.883268\n",
      "\tTesting Loss: 204.151683\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 221.883307\n",
      "\tTesting Loss: 204.151688\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 221.883373\n",
      "\tTesting Loss: 204.151560\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 221.883373\n",
      "\tTesting Loss: 204.151499\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 221.883296\n",
      "\tTesting Loss: 204.151606\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 221.883301\n",
      "\tTesting Loss: 204.151718\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 221.883355\n",
      "\tTesting Loss: 204.151657\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 221.883347\n",
      "\tTesting Loss: 204.151525\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 221.883311\n",
      "\tTesting Loss: 204.151667\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 221.883320\n",
      "\tTesting Loss: 204.151698\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 221.883352\n",
      "\tTesting Loss: 204.151637\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 221.883288\n",
      "\tTesting Loss: 204.151550\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 221.883260\n",
      "\tTesting Loss: 204.151744\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 221.883284\n",
      "\tTesting Loss: 204.151728\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 221.883338\n",
      "\tTesting Loss: 204.151688\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 221.883279\n",
      "\tTesting Loss: 204.151571\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 221.883244\n",
      "\tTesting Loss: 204.151820\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 221.883247\n",
      "\tTesting Loss: 204.151744\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 221.883293\n",
      "\tTesting Loss: 204.151744\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 221.883322\n",
      "\tTesting Loss: 204.151733\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 221.883318\n",
      "\tTesting Loss: 204.151632\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 221.883279\n",
      "\tTesting Loss: 204.151616\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 221.883231\n",
      "\tTesting Loss: 204.151784\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 221.883287\n",
      "\tTesting Loss: 204.151805\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 221.883299\n",
      "\tTesting Loss: 204.151698\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 221.883266\n",
      "\tTesting Loss: 204.151713\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 221.883245\n",
      "\tTesting Loss: 204.151749\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 221.883265\n",
      "\tTesting Loss: 204.151805\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 221.883283\n",
      "\tTesting Loss: 204.151738\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 221.883265\n",
      "\tTesting Loss: 204.151733\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 221.883225\n",
      "\tTesting Loss: 204.151784\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 221.883255\n",
      "\tTesting Loss: 204.151820\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 221.883263\n",
      "\tTesting Loss: 204.151754\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 221.883244\n",
      "\tTesting Loss: 204.151769\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 221.883237\n",
      "\tTesting Loss: 204.151820\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 221.883243\n",
      "\tTesting Loss: 204.151800\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 221.883228\n",
      "\tTesting Loss: 204.151769\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 221.883160\n",
      "\tTesting Loss: 204.151825\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 221.883211\n",
      "\tTesting Loss: 204.151896\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 221.883249\n",
      "\tTesting Loss: 204.151789\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 221.883233\n",
      "\tTesting Loss: 204.151744\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 221.883160\n",
      "\tTesting Loss: 204.151840\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 221.883186\n",
      "\tTesting Loss: 204.151850\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 221.883219\n",
      "\tTesting Loss: 204.151861\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 221.883233\n",
      "\tTesting Loss: 204.151744\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 221.883185\n",
      "\tTesting Loss: 204.151891\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 221.883185\n",
      "\tTesting Loss: 204.151820\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 221.883237\n",
      "\tTesting Loss: 204.151871\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 221.883242\n",
      "\tTesting Loss: 204.151749\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 221.883175\n",
      "\tTesting Loss: 204.151850\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 221.883143\n",
      "\tTesting Loss: 204.151850\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 221.883169\n",
      "\tTesting Loss: 204.151866\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 221.883170\n",
      "\tTesting Loss: 204.151866\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 221.883160\n",
      "\tTesting Loss: 204.151835\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 221.883134\n",
      "\tTesting Loss: 204.151850\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 221.883109\n",
      "\tTesting Loss: 204.151881\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 221.883134\n",
      "\tTesting Loss: 204.151922\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 221.883124\n",
      "\tTesting Loss: 204.151871\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 221.883130\n",
      "\tTesting Loss: 204.151881\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 221.883108\n",
      "\tTesting Loss: 204.151876\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 221.883132\n",
      "\tTesting Loss: 204.151932\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 221.883125\n",
      "\tTesting Loss: 204.151861\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 221.883152\n",
      "\tTesting Loss: 204.151896\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 221.883162\n",
      "\tTesting Loss: 204.151886\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 221.883160\n",
      "\tTesting Loss: 204.151825\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 221.883135\n",
      "\tTesting Loss: 204.151866\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 221.883135\n",
      "\tTesting Loss: 204.151896\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 221.883120\n",
      "\tTesting Loss: 204.151866\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 221.883115\n",
      "\tTesting Loss: 204.151881\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 221.883087\n",
      "\tTesting Loss: 204.151881\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 221.883121\n",
      "\tTesting Loss: 204.151932\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 221.883129\n",
      "\tTesting Loss: 204.151896\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 221.883143\n",
      "\tTesting Loss: 204.151886\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 221.883161\n",
      "\tTesting Loss: 204.151805\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 221.883146\n",
      "\tTesting Loss: 204.151855\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 221.883120\n",
      "\tTesting Loss: 204.151896\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 221.883106\n",
      "\tTesting Loss: 204.151845\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 221.883101\n",
      "\tTesting Loss: 204.151942\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 221.883109\n",
      "\tTesting Loss: 204.151871\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 221.883104\n",
      "\tTesting Loss: 204.151901\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 221.883095\n",
      "\tTesting Loss: 204.151917\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 221.883096\n",
      "\tTesting Loss: 204.151911\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 221.883109\n",
      "\tTesting Loss: 204.151896\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 221.883111\n",
      "\tTesting Loss: 204.151896\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 221.883123\n",
      "\tTesting Loss: 204.151911\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 221.883109\n",
      "\tTesting Loss: 204.151876\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 221.883092\n",
      "\tTesting Loss: 204.151901\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 221.883091\n",
      "\tTesting Loss: 204.151922\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 221.883109\n",
      "\tTesting Loss: 204.151906\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 221.883105\n",
      "\tTesting Loss: 204.151927\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 221.883151\n",
      "\tTesting Loss: 204.151891\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 221.883144\n",
      "\tTesting Loss: 204.151876\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 221.883095\n",
      "\tTesting Loss: 204.151906\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 221.883066\n",
      "\tTesting Loss: 204.151876\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 221.883078\n",
      "\tTesting Loss: 204.151942\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 221.883102\n",
      "\tTesting Loss: 204.151896\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 221.883078\n",
      "\tTesting Loss: 204.151891\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 221.883083\n",
      "\tTesting Loss: 204.151942\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 221.883097\n",
      "\tTesting Loss: 204.151886\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 221.883063\n",
      "\tTesting Loss: 204.151876\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 221.883078\n",
      "\tTesting Loss: 204.151967\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 221.883082\n",
      "\tTesting Loss: 204.151911\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 221.883111\n",
      "\tTesting Loss: 204.151881\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 221.883060\n",
      "\tTesting Loss: 204.151972\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 221.883041\n",
      "\tTesting Loss: 204.151922\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 221.883039\n",
      "\tTesting Loss: 204.151952\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 221.883034\n",
      "\tTesting Loss: 204.151952\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 221.883024\n",
      "\tTesting Loss: 204.151967\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 221.883027\n",
      "\tTesting Loss: 204.151972\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 221.883047\n",
      "\tTesting Loss: 204.151972\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 221.883036\n",
      "\tTesting Loss: 204.151932\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 221.883029\n",
      "\tTesting Loss: 204.151967\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 221.883031\n",
      "\tTesting Loss: 204.151962\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 221.883039\n",
      "\tTesting Loss: 204.151952\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 221.883032\n",
      "\tTesting Loss: 204.151947\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 221.883024\n",
      "\tTesting Loss: 204.151962\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 221.883025\n",
      "\tTesting Loss: 204.151978\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 221.883030\n",
      "\tTesting Loss: 204.151962\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 221.883024\n",
      "\tTesting Loss: 204.151967\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 221.883031\n",
      "\tTesting Loss: 204.151978\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 221.883038\n",
      "\tTesting Loss: 204.151962\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 221.883032\n",
      "\tTesting Loss: 204.151962\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 221.883034\n",
      "\tTesting Loss: 204.151962\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 221.883046\n",
      "\tTesting Loss: 204.151967\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 221.883053\n",
      "\tTesting Loss: 204.151947\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 221.883069\n",
      "\tTesting Loss: 204.151937\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 221.883088\n",
      "\tTesting Loss: 204.151906\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 221.883060\n",
      "\tTesting Loss: 204.151952\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 221.883030\n",
      "\tTesting Loss: 204.151957\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 221.883025\n",
      "\tTesting Loss: 204.151962\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 221.883029\n",
      "\tTesting Loss: 204.151978\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 221.883033\n",
      "\tTesting Loss: 204.151967\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 221.883029\n",
      "\tTesting Loss: 204.151978\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 221.883030\n",
      "\tTesting Loss: 204.151967\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 221.883027\n",
      "\tTesting Loss: 204.151978\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 221.883030\n",
      "\tTesting Loss: 204.151972\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 221.883030\n",
      "\tTesting Loss: 204.151972\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 221.883021\n",
      "\tTesting Loss: 204.151967\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 221.883040\n",
      "\tTesting Loss: 204.151967\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 221.883040\n",
      "\tTesting Loss: 204.151952\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 221.883043\n",
      "\tTesting Loss: 204.151937\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 221.883029\n",
      "\tTesting Loss: 204.151967\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 221.883043\n",
      "\tTesting Loss: 204.151937\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 221.883035\n",
      "\tTesting Loss: 204.151952\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 221.883012\n",
      "\tTesting Loss: 204.151972\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 221.883018\n",
      "\tTesting Loss: 204.151957\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 221.883016\n",
      "\tTesting Loss: 204.151962\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 221.883018\n",
      "\tTesting Loss: 204.151967\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 221.882991\n",
      "\tTesting Loss: 204.151988\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 221.883018\n",
      "\tTesting Loss: 204.151983\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 221.883017\n",
      "\tTesting Loss: 204.151957\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 221.883013\n",
      "\tTesting Loss: 204.151967\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 221.882998\n",
      "\tTesting Loss: 204.151983\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 221.882994\n",
      "\tTesting Loss: 204.151972\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 221.882975\n",
      "\tTesting Loss: 204.152003\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 221.882981\n",
      "\tTesting Loss: 204.152003\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 221.882985\n",
      "\tTesting Loss: 204.151993\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 221.882974\n",
      "\tTesting Loss: 204.152023\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 221.882979\n",
      "\tTesting Loss: 204.152003\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 221.882981\n",
      "\tTesting Loss: 204.151988\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 221.882965\n",
      "\tTesting Loss: 204.152039\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 221.882978\n",
      "\tTesting Loss: 204.152008\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 221.882973\n",
      "\tTesting Loss: 204.152008\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 221.882973\n",
      "\tTesting Loss: 204.152018\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 221.882959\n",
      "\tTesting Loss: 204.152023\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 221.882957\n",
      "\tTesting Loss: 204.152018\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 221.882970\n",
      "\tTesting Loss: 204.152023\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 221.882976\n",
      "\tTesting Loss: 204.152008\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 221.882972\n",
      "\tTesting Loss: 204.152013\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 221.882968\n",
      "\tTesting Loss: 204.152018\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 221.882977\n",
      "\tTesting Loss: 204.152018\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 221.882982\n",
      "\tTesting Loss: 204.151978\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 221.882977\n",
      "\tTesting Loss: 204.152018\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 221.882960\n",
      "\tTesting Loss: 204.152018\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 221.882952\n",
      "\tTesting Loss: 204.152013\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 221.882961\n",
      "\tTesting Loss: 204.152018\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 221.882966\n",
      "\tTesting Loss: 204.152028\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 221.882973\n",
      "\tTesting Loss: 204.152013\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 221.882973\n",
      "\tTesting Loss: 204.152013\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 221.882977\n",
      "\tTesting Loss: 204.152008\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 221.882977\n",
      "\tTesting Loss: 204.152008\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 221.882980\n",
      "\tTesting Loss: 204.152018\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 221.882980\n",
      "\tTesting Loss: 204.152003\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 221.882977\n",
      "\tTesting Loss: 204.152008\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 221.882961\n",
      "\tTesting Loss: 204.152013\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 221.882956\n",
      "\tTesting Loss: 204.152023\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 221.882966\n",
      "\tTesting Loss: 204.152003\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 221.882973\n",
      "\tTesting Loss: 204.152013\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 221.882971\n",
      "\tTesting Loss: 204.152003\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 221.882957\n",
      "\tTesting Loss: 204.152018\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 221.882956\n",
      "\tTesting Loss: 204.152023\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 221.882966\n",
      "\tTesting Loss: 204.152008\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 221.882971\n",
      "\tTesting Loss: 204.152013\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 221.882971\n",
      "\tTesting Loss: 204.152018\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 221.882957\n",
      "\tTesting Loss: 204.152018\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 221.882957\n",
      "\tTesting Loss: 204.152018\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 221.882954\n",
      "\tTesting Loss: 204.152018\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 221.882952\n",
      "\tTesting Loss: 204.152018\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 221.882942\n",
      "\tTesting Loss: 204.152059\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 221.882951\n",
      "\tTesting Loss: 204.152023\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 221.882952\n",
      "\tTesting Loss: 204.152044\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 221.882956\n",
      "\tTesting Loss: 204.152049\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 221.882954\n",
      "\tTesting Loss: 204.152033\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 221.882940\n",
      "\tTesting Loss: 204.152049\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 221.882931\n",
      "\tTesting Loss: 204.152059\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 221.882917\n",
      "\tTesting Loss: 204.152069\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 221.882927\n",
      "\tTesting Loss: 204.152023\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 221.882914\n",
      "\tTesting Loss: 204.152105\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 221.882929\n",
      "\tTesting Loss: 204.152059\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 221.882937\n",
      "\tTesting Loss: 204.152074\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 221.882936\n",
      "\tTesting Loss: 204.152044\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 221.882908\n",
      "\tTesting Loss: 204.152089\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 221.882924\n",
      "\tTesting Loss: 204.152054\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 221.882926\n",
      "\tTesting Loss: 204.152110\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 221.882927\n",
      "\tTesting Loss: 204.152089\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 221.882925\n",
      "\tTesting Loss: 204.152079\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 221.882915\n",
      "\tTesting Loss: 204.152084\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 221.882915\n",
      "\tTesting Loss: 204.152074\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 221.882905\n",
      "\tTesting Loss: 204.152084\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 221.882933\n",
      "\tTesting Loss: 204.152059\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 221.882900\n",
      "\tTesting Loss: 204.152135\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 221.882933\n",
      "\tTesting Loss: 204.152054\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 221.882931\n",
      "\tTesting Loss: 204.152079\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 221.882936\n",
      "\tTesting Loss: 204.151998\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 221.882933\n",
      "\tTesting Loss: 204.152084\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 221.882940\n",
      "\tTesting Loss: 204.152089\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 221.882919\n",
      "\tTesting Loss: 204.152069\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 221.882935\n",
      "\tTesting Loss: 204.152079\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 221.882922\n",
      "\tTesting Loss: 204.152074\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 221.882930\n",
      "\tTesting Loss: 204.152059\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 221.882924\n",
      "\tTesting Loss: 204.152074\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 221.882915\n",
      "\tTesting Loss: 204.152074\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 221.882919\n",
      "\tTesting Loss: 204.152064\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 221.882927\n",
      "\tTesting Loss: 204.152089\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 221.882932\n",
      "\tTesting Loss: 204.152044\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 221.882917\n",
      "\tTesting Loss: 204.152074\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 221.882912\n",
      "\tTesting Loss: 204.152074\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 221.882922\n",
      "\tTesting Loss: 204.152074\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 221.882928\n",
      "\tTesting Loss: 204.152064\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 221.882929\n",
      "\tTesting Loss: 204.152069\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 221.882926\n",
      "\tTesting Loss: 204.152059\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 221.882928\n",
      "\tTesting Loss: 204.152059\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 221.882931\n",
      "\tTesting Loss: 204.152069\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 221.882926\n",
      "\tTesting Loss: 204.152064\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 221.882929\n",
      "\tTesting Loss: 204.152059\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 221.882922\n",
      "\tTesting Loss: 204.152059\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 221.882914\n",
      "\tTesting Loss: 204.152074\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 221.882915\n",
      "\tTesting Loss: 204.152074\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 221.882918\n",
      "\tTesting Loss: 204.152074\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 221.882919\n",
      "\tTesting Loss: 204.152064\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 221.882917\n",
      "\tTesting Loss: 204.152064\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 221.882913\n",
      "\tTesting Loss: 204.152059\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 221.882908\n",
      "\tTesting Loss: 204.152069\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 221.882900\n",
      "\tTesting Loss: 204.152084\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 221.882905\n",
      "\tTesting Loss: 204.152084\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 221.882905\n",
      "\tTesting Loss: 204.152064\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 221.882875\n",
      "\tTesting Loss: 204.152115\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 221.882882\n",
      "\tTesting Loss: 204.152095\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 221.882886\n",
      "\tTesting Loss: 204.152105\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 221.882886\n",
      "\tTesting Loss: 204.152100\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 221.882890\n",
      "\tTesting Loss: 204.152100\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 221.882899\n",
      "\tTesting Loss: 204.152089\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 221.882900\n",
      "\tTesting Loss: 204.152079\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 221.882901\n",
      "\tTesting Loss: 204.152095\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 221.882896\n",
      "\tTesting Loss: 204.152089\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 221.882897\n",
      "\tTesting Loss: 204.152089\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 221.882905\n",
      "\tTesting Loss: 204.152100\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 221.882907\n",
      "\tTesting Loss: 204.152074\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 221.882890\n",
      "\tTesting Loss: 204.152084\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 221.882890\n",
      "\tTesting Loss: 204.152115\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 221.882886\n",
      "\tTesting Loss: 204.152089\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 221.882885\n",
      "\tTesting Loss: 204.152095\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 221.882885\n",
      "\tTesting Loss: 204.152110\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 221.882887\n",
      "\tTesting Loss: 204.152100\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 221.882892\n",
      "\tTesting Loss: 204.152084\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 221.882882\n",
      "\tTesting Loss: 204.152089\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 221.882877\n",
      "\tTesting Loss: 204.152105\n",
      "\tLearning Rate: 0.001215767\n"
     ]
    }
   ],
   "source": [
    "if overwrite or not os.path.exists(model_path):\n",
    "    for i in range(num_saves): \n",
    "        train_losses_tmp, test_losses_tmp = train(model, \n",
    "                                                train_dataloader,\n",
    "                                                test_dataloader, \n",
    "                                                optimizer, \n",
    "                                                scheduler,\n",
    "                                                num_epochs, \n",
    "                                                device,\n",
    "                                                loss_type)\n",
    "        train_losses = train_losses + train_losses_tmp\n",
    "        test_losses = test_losses + test_losses_tmp\n",
    "        # every num_epochs, evaluate the model\n",
    "        train_eval_metrics = eval_model(model, y_true_train, ts_df_train, txt_ls, ts_encoder_name, text_encoder_name)\n",
    "        test_eval_metrics = eval_model(model, y_true_test, ts_df_test, txt_ls, ts_encoder_name, text_encoder_name)\n",
    "        train_eval_metrics_list.append(train_eval_metrics)\n",
    "        test_eval_metrics_list.append(test_eval_metrics)\n",
    "        # save model and losses\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        torch.save({\n",
    "            'train_losses': train_losses,\n",
    "            'test_losses': test_losses }, loss_path)\n",
    "        # save train_eval_metrics_list and test_eval_metrics_list\n",
    "        torch.save({\n",
    "            'train_evals': train_eval_metrics_list,\n",
    "            'test_evals': test_eval_metrics_list }, eval_metrics_path)  \n",
    "    \n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    train_losses = torch.load(loss_path)['train_losses']\n",
    "    test_losses = torch.load(loss_path)['test_losses']\n",
    "    train_eval_metrics_list = torch.load(eval_metrics_path)['train_evals']\n",
    "    test_eval_metrics_list = torch.load(eval_metrics_path)['test_evals']\n",
    "    confusion_matrices_train = [eval_metrics['confusion_matrix'] for eval_metrics in train_eval_metrics_list]\n",
    "    confusion_matrices_test = [eval_metrics['confusion_matrix'] for eval_metrics in test_eval_metrics_list]\n",
    "\n",
    "\n",
    "    # Calculate metrics for training data\n",
    "    train_metrics = []\n",
    "    for cm in confusion_matrices_train:\n",
    "        metrics = calculate_f1_precision_recall_from_cm(cm)\n",
    "        train_metrics.append(metrics)\n",
    "\n",
    "    # Calculate metrics for test data\n",
    "    test_metrics = []\n",
    "    for cm in confusion_matrices_test:\n",
    "        metrics = calculate_f1_precision_recall_from_cm(cm)\n",
    "        test_metrics.append(metrics)\n",
    "\n",
    "    # Extract metrics\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    train_f1 = [m['f1'] for m in train_metrics]\n",
    "    train_precision = [m['precision'] for m in train_metrics]\n",
    "    train_recall = [m['recall'] for m in train_metrics]\n",
    "    test_f1 = [m['f1'] for m in test_metrics]\n",
    "    test_precision = [m['precision'] for m in test_metrics]\n",
    "    test_recall = [m['recall'] for m in test_metrics]\n",
    "\n",
    "    # Create figure with two subplots side by side\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Plot losses on the left subplot\n",
    "    ax1.plot(train_losses, 'b-', label='Train Loss')\n",
    "    ax1.plot(test_losses, 'r-', label='Test Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Test Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot metrics on the right subplot\n",
    "    ax2.plot(epochs, train_f1, 'b-', label='Train F1')\n",
    "    ax2.plot(epochs, train_precision, 'g-', label='Train Precision')\n",
    "    ax2.plot(epochs, train_recall, 'r-', label='Train Recall')\n",
    "    ax2.plot(epochs, test_f1, 'b--', label='Test F1')\n",
    "    ax2.plot(epochs, test_precision, 'g--', label='Test Precision')\n",
    "    ax2.plot(epochs, test_recall, 'r--', label='Test Recall')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.set_title('Training and Test Metrics')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAB8YAAAOYCAYAAACnzJnEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAB0+ElEQVR4nOzdf2zc933Y/xcp2pTciGdZJ1lidLK8yHaiNHIdL07aJlHWZZqb/sDS1sqy/jKSOu2KoCiMrYCGbJ23ofTyxfrNV2iyzqm9pkgbyajtrBvQQnAaO7baLUrStElsxXJoUTTTyKZs8zwqPP267x8tKcv3Ucu7zx0/n3vf4wEQBeUP33yTvWdfKF4iNdRsNpsBAAAAAAAAAIkaLvoCAAAAAAAAANBLFuMAAAAAAAAAJM1iHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjJffxj388jhw50vLnx44di6effjoiIv7qr/4qHn744ZW+GhRmOV18/etfj4ceemilrwaFWk4bTzzxhDYYKMvpYn5+Pn77t397pa8GhVpOGw888EB87WtfW+mrQWGW08X//t//Ox555JEVvhkUazltPPnkk/Fbv/VbK301KMxyuvjSl74Un/rUp1b6alCo5bTx0EMPxec///mVvhoUop0d34EDB+LP/uzPVvqKA8divOSGh4fj8ssvj4985CPx5S9/OT7xiU/EZz/72Wg2m7F///548cUXY+fOnUVfE1bUcrp44xvfWPQ1YcUtp43x8fGYn58v+qqwYpbTxVe/+tXYvn170VeFFbWcNjZs2GBmMFCW08Xx48fjzJkzRV8VVtRy2njDG94Qr3vd64q+KqyY5XTRaDRibm6u6KvCilpOG1u2bFlaCELq2tnxVavVWFhYKPjG6bMYL7nz58/HwsJCDA//zf+qVq1aFUNDQ3H8+PF429veFidPnozJycmYnJws+KawcpbTxf79+2Pt2rUF3xRW1nLamJ6ejiuvvLLYi8IKWk4Xp06diqeeeqrgm8LKWk4b69ati6mpqYJvCitnOV1cd911MTMzU/BNYWUtp41vfetb/qIhA2U5XVx22WXxmte8puCbwspaThuvec1rYnx8vOCbwspoZ8c3Ozsbq1evLvjG6RtqNpvNoi/B3+/hhx+Od7/73UVfA0pFF5BNG9BKF5BNG9BKF5BNG9BKF5BNG3AxTZSHxTgAAAAAAAAASfOr1AEAAAAAAABImsU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApI208/ChQ4diZmYm9uzZE/fcc0/ceuut8eCDD8aePXtifHy85flGoxGNRmPp/fPnz8cLL7wQ69evj6Ghofy3hw40m814+eWXY3x8PIaHu/N3Q7RBCrrdhi5IgZkB2cwMaGVmQDYzA1qZGZDNzIBWZgZk67SNthbjCwsLUa1WIyJiw4YNcfz48di9e3ccPXo0M5aJiYm466672vkUsGKmp6djy5YtXTlLG6SkW23ogpSYGZDNzIBWZgZkMzOglZkB2cwMaGVmQLZ22xhqNpvN5Tx47733xv79++OOO+6It771rTE5ORmnT5+OJ598Mm677bZ47Wtf2/Ixr/5bJHNzc7F169Z4+pnpWDs2tuxLcrFTjbNFX6GvvfxyPXbecG289NJLUalUcp+njfLQRj7dbEMX5aGLfMyMdGkjHzMjTbrIx8xIlzbyMTPSpIt8zIx0aSMfMyNNusjHzEiXNvLptI1lL8a7oV6vR6VSiRMn52JMLB0TSz71ej2uHV8fc3PleR1qozu0kU/Z2tBFd+gin7J1EaGNbtFGPmVrQxfdoYt8ytZFhDa6RRv5lK0NXXSHLvIpWxcR2ugWbeRTtjZ00R26yKdsXURoo1u0kU+nbXTnHyQAAAAAAAAAgJKyGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkmYxDgAAAAAAAEDSLMYBAAAAAAAASJrFOAAAAAAAAABJsxgHAAAAAAAAIGkW4wAAAAAAAAAkzWIcAAAAAAAAgKRZjAMAAAAAAACQNItxAAAAAAAAAJJmMQ4AAAAAAABA0izGAQAAAAAAAEjaSDsPHzp0KGZmZmLPnj3xyCOPxPDwcBw6dCjuvPPOGB0dbXm+0WhEo9FYer9er+e/MZSQNqCVLiCbNqCVLiCbNqCVLiCbNqCVLiCbNhhkbf3E+MLCQlSr1YiI2LFjRwwPD0elUokzZ85kPj8xMRGVSmXprVar5b8xlJA2oJUuIJs2oJUuIJs2oJUuIJs2oJUuIJs2GGTLXozfe++9cffdd8fs7GxMTU3FfffdFxs3boy1a9fG888/n/kxe/fujbm5uaW36enprl0cykIb0EoXkE0b0EoXkE0b0EoXkE0b0EoXkE0bDLqhZrPZXKlPVq/Xo1KpxImTczE2NrZSnzY5pxpni75CX6vX63Ht+PqYmyvP61Ab3aGNfMrWhi66Qxf5lK2LCG10izbyKVsbuugOXeRTti4itNEt2sinbG3oojt0kU/ZuojQRrdoI5+ytaGL7tBFPmXrIkIb3aKNfDpto61fpQ4AAAAAAAAA/cZiHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkmYxDgAAAAAAAEDSLMZf4emjR+Nd7/iBeNOO6+Pt339LPPnEE0VfqS80Go34tTt/Jd5y4xviB/7hjfGLH/y5iIj4ypcPxw+/+53xjrfeFLu+/+b4wiOfL/imdEIXndFF+rTRGW2kTRed0UX6tNEZbaRNF53RRfq00RltpE0XndFF+rTRGW2kTRedGaQuRoq+QJl8+Jd/MT74Cx+Kn/352+PBB/4wfulDH4xHH//zoq9Vev/h3/2bGB4eji9+9YkYGhqK73znr6PZbMbPvf+2+K+f/N14x653xVPfPBI/+eM/HF/86hOxZs2aoq9MG3TRGV2kTxud0UbadNEZXaRPG53RRtp00RldpE8bndFG2nTRGV2kTxud0UbadNGZQerCT4z/reeeey6++hdfiff/9M9ERMR7f+InY+rYMzF17FixFyu5+fn5+INPfyo+8u//UwwNDUVExKZNm+OFkyfjpRdfiHfseldERFx/w+ujUqnEwwf/pMDb0i5ddEYX6dNGZ7SRNl10Rhfp00ZntJE2XXRGF+nTRme0kTZddEYX6dNGZ7SRNl10ZtC6aHsxfuDAgTh06FB87GMfi0cffTTuv//+XtxrxT07PR2bx8djZORvfoh+aGgottS2xvT08YJvVm7HnvlWXHXV+vgvH/2N+KF3vDV+5J+8Kx79/J/G+mo1Nmy8Ov7osw9GRMSXDv+f+NbTR2P6+LFiL9xDKbahi87o4mLaYJE2LtAFi3RxMW2wSBsX6IJFuriYNlikjQt0wSJdXEwbLNLGBbpg0aB10fZivFqtxuc+97nYvXt3HDt2LKrV6iWfbTQaUa/XL3ors8W/CbGo2WwWdJP+cebMmTj2zGTc8Po3xJ8+9n/iP//m/xd33P7TMfv88/HpAw/Epz91X/yjH3xL3PfJ3463fv8PxsjIZUVfuWdSbUMX7dPFxZbbRj91EaGNTmjjAjODRbq4mJnBIm1cYGawSBcXMzNYpI0LzAwW6eJiZgaLtHGBmcGiQeui7X9jfHZ2Nnbt2hUHDx6Mm266KU6cOHHJZycmJuKuu+7KdcGVsqVWi5lnn42zZ8/GyMhINJvNmHl2Omq1rUVfrdRqtWtieHg4bnvfv4iIiO99042xddu2OPLkE/H2d+6K+x/6X0vPvu3Nb4obXr+jqKv2XIpt6KIzurjYctvoly4itNEpbVxgZrBIFxczM1ikjQvMDBbp4mJmBou0cYGZwSJdXMzMYJE2LjAzWDRoXbT9E+Pve9/7YteuXfGrv/qrsWvXrtizZ88ln927d2/Mzc0tvU1PT+e6bC9t3Lgxbvy+m+Izv//piIh46MEHYus12+KabduKvVjJra9W453v+qH404cPRkTE9PGpOH7sWGy//vo4ceI7S8/93n//nbjie66Id77rHxV11Z5LsQ1ddEYXF1tuG/3SRYQ2OqWNC8wMFuniYmYGi7RxgZnBIl1czMxgkTYuMDNYpIuLmRks0sYFZgaLBq2LoeYK/h6Ber0elUolTpyci7GxsZX6tMv21De/GXd88PZ44YWTMbZ2LD5536dixxvfWPS1WpxqnC36Chc59sxk/Mq/vCNeeOFkrFq1Kv713n8bP/rj/yw++hv/Mf7w/s9Es9mM6294fXz0N/fFa7fUir5u1Ov1uHZ8fczNled1WOY2+qWLiHK10W9dRJSvjTJ3EdE/bZSpi4j+a6NsXUSUu41+6SKiXG30WxcR5WujzF1E9E8bZeoiov/aKFsXEeVuo1+6iChXG/3WRUT52ihzFxH900aZuojovzbK1kVEudvoly4iytVGv3URUb42ytxFRP+0UaYuIvqvjbJ1EVHuNvqli4hytdFvXUR03obFeB8qUyz9yCBJlzbyKVsbuugOXeRTti4itNEt2sinbG3oojt0kU/ZuojQRrdoI5+ytaGL7tBFPmXrIkIb3aKNfMrWhi66Qxf5lK2LCG10izby6bSNtn+VOgAAAAAAAAD0E4txAAAAAAAAAJJmMQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJM1iHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACRtpN0POHDgQGzevDmOHDkStVot1q9fH7fcckvms41GIxqNxtL79Xq985tCyWkDsi23DV0wSMwMyGZmQCszA7KZGdDKzIBsZga0MjMYVG3/xHi1Wo2zZ8/G5s2bo1qtxvz8/CWfnZiYiEqlsvRWq9VyXRbKTBuQbblt6IJBYmZANjMDWpkZkM3MgFZmBmQzM6CVmcGgansxPjs7G6tWrYrDhw/HmjVrYmpq6pLP7t27N+bm5pbepqenc10WykwbkG25beiCQWJmQDYzA1qZGZDNzIBWZgZkMzOglZnBoBpqNpvNlfpk9Xo9KpVKnDg5F2NjYyv1aZNzqnG26Cv0tXq9HteOr4+5ufK8DrXRHdrIp2xt6KI7dJFP2bqI0Ea3aCOfsrWhi+7QRT5l6yJCG92ijXzK1oYuukMX+ZStiwhtdIs28ilbG7roDl3kU7YuIrTRLdrIp9M22v6JcQAAAAAAAADoJxbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkmYxDgAAAAAAAEDSLMYBAAAAAAAASJrFOAAAAAAAAABJsxgHAAAAAAAAIGkW4wAAAAAAAAAkzWIcAAAAAAAAgKRZjAMAAAAAAACQNItxAAAAAAAAAJJmMQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASRtp9wMOHDgQtVotnnrqqbjuuutiZmYm9uzZk/lso9GIRqOx9H69Xu/8pix57dt/tegr9LXmudM9OVcbxdNGPkW3oYve0EU+RXcRoY1e0UY+Rbehi97QRT5FdxGhjV7RRj5Ft6GL3tBFPkV3EaGNXtFGPkW3oYve0EU+RXcRoY1e0UY+nbbR9k+MV6vVmJ6ejuHh4VhYWIhqtXrJZycmJqJSqSy91Wq1ji4J/UAbkG25beiCQWJmQDYzA1qZGZDNzIBWZgZkMzOglZnBoGp7MT47OxubNm2K+fn5WL16dczOzl7y2b1798bc3NzS2/T0dK7LQplpA7Ittw1dMEjMDMhmZkArMwOymRnQysyAbGYGtDIzGFRt/yr1973vfRERsWvXrr/32dHR0RgdHW3/VtCHtAHZltuGLhgkZgZkMzOglZkB2cwMaGVmQDYzA1qZGQyqtn9iHAAAAAAAAAD6icU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkmYxDgAAAAAAAEDSLMYBAAAAAAAASJrFOAAAAAAAAABJsxgHAAAAAAAAIGkW4wAAAAAAAAAkzWIcAAAAAAAAgKRZjAMAAAAAAACQNItxAAAAAAAAAJJmMQ4AAAAAAABA0kba/YADBw7Epk2b4rHHHosbb7wxrr766rjlllsyn200GtFoNJber9frnd8USk4bkG25beiCQWJmQDYzA1qZGZDNzIBWZgZkMzOglZnBoGr7J8ar1WqcO3cubr755ti0aVPMz89f8tmJiYmoVCpLb7VaLddlocy0AdmW24YuGCRmBmQzM6CVmQHZzAxoZWZANjMDWpkZDKq2F+Ozs7OxatWqGBoainXr1sXU1NQln927d2/Mzc0tvU1PT+e6LJSZNiDbctvQBYPEzIBsZga0MjMgm5kBrcwMyGZmQCszg0HV9q9Sf9/73nfR+9u3b7/ks6OjozE6Otr+raAPaQOyLbcNXTBIzAzIZmZAKzMDspkZ0MrMgGxmBrQyMxhUbf/EOAAAAAAAAAD0E4txAAAAAAAAAJJmMQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJM1iHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEkbKfoCZfL00aPxCx/4+Th5cjYqlSvjk/f+brxhx46ir1W4M89+Ic7NHYs483JcfsM/j+E16y/67+deOBJnjn8uLrv2R2JVZVtERJw9+USce/4vo7nwYoy89u0xsmHn0vPNZjPOzjwW5+tTERGxauP3xUj1TSv15dAmXWTTBdrIpo3BpotsukAb2bQx2HSRTRdoI5s2BpsusukCbWTTxmDTRTZdXOAnxl/hw7/8i/HBX/hQfO2Jp+LOf/Vr8Usf+mDRVyqFVVduj9HrfiLisrUt/615+v/G2dlvxNAVV1/058NXbIzLtv3TGF53fcvHnH/xm9FceCEuf8NPx+XX3xZnT/xFnF94sWf3Jx9dZNMF2simjcGmi2y6QBvZtDHYdJFNF2gjmzYGmy6y6QJtZNPGYNNFNl1cYDH+t5577rn46l98Jd7/0z8TERHv/YmfjKljz8TUsWPFXqwEhl8zHkOXvybzv52Z/nxc9tofjBhadfHHrKnG8OqrImKo5WPOvfR0rKp+bwwNDcfQyOpYtW57nHvxaC+uTk66uDRdDDZtXJo2BpcuLk0Xg00bl6aNwaWLS9PFYNPGpWljcOni0nQx2LRxadoYXLq4NF1c0NFifN++fTE9PR0f+9jH4tFHH43777+/2/dacc9OT8fm8fEYGfmb3y4/NDQUW2pbY3r6eME3K6+zs1+PodVXxfD3bGrr45qnX46hV/ytlKHL10acebnb11txuiBCF1m0QYQ2Xk0XROgiizaI0Mar6YIIXWTRBhHaeDVdEKGLLNogQhuvpgsiBrOLjhbjO3fujKmpqdi9e3ccO3YsqtVq5nONRiPq9fpFb2U2NHTx33poNpsF3aT8zjfqce7kN2Jk81s7O+CV3+pEvs3L7SKiv9rQxfLpIpuZgTZamRnoIpuZgTZamRnoIpuZgTZamRnoIpuZgTZamRkMahcdLcYnJyfj3LlzcfDgwdi2bVvMzs5mPjcxMRGVSmXprVar5bpsL22p1WLm2Wfj7NmzEfE3scw8Ox212taCb1ZOzVPfieaZ+Wg8+Qex8I3fi+apE3Fm+k/j7Mlv/L0fO3T52mievvA3R5pnXs78dw36zXK7iOifNnTRHl1kMzPQRiszA11kMzPQRiszA11kMzPQRiszA11kMzPQRiszg0HtYqSTD/rABz4QERG7du36O5/bu3dv3HnnnUvv1+v10gazcePGuPH7borP/P6n42d//vZ46MEHYus12+KabduKvloprVp3faxad/3S+42jD8XIxptiVWXb3/uxw5XXxbnZb8Rw5R9EnDsd5148Gpe/7sd6eNuVsdwuIvqnDV20RxfZzAy00crMQBfZzAy00crMQBfZzAy00crMQBfZzAy00crMYFC76Ggxvlyjo6MxOjray0/RVb/1if8Wd3zw9vjof/6NGFs7Fp+871NFX6kUzjz7aJybeybizKk4/a3/EUPDl8Xojp/9Oz/m3AvfjDN//ecR5xpxvv5MnH3uK3H5tT8Sw1dsiFVX3RDN7z4Xp5/8/YiIGNl4UwyvvmolvpTS6Kc2dJFNF93XT11EaONStNF9/dSGLrLpovv6qYsIbVyKNrqvn9rQRTZddF8/dRGhjUvRRvf1Uxu6yKaL7uunLiK0cSna6L5+akMX2XRxwVBzBX/Bfr1ej0qlEidOzsXY2NhKfdrkrHvLh4u+Ql9rnjsdja99MubmyvM61EZ3aCOfsrWhi+7QRT5l6yJCG92ijXzK1oYuukMX+ZStiwhtdIs28ilbG7roDl3kU7YuIrTRLdrIp2xt6KI7dJFP2bqI0Ea3aCOfTtvo6N8YBwAAAAAAAIB+YTEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkmYxDgAAAAAAAEDSLMYBAAAAAAAASJrFOAAAAAAAAABJsxgHAAAAAAAAIGkW4wAAAAAAAAAkzWIcAAAAAAAAgKR1tBjft29f1Ov1uOeee+KP//iP44tf/GLmc41GI+r1+kVvkKrldhGhDQaLmQGtzAzIZmZAKzMDspkZ0MrMgGxmBrQyMxhUHS3Gd+7cGVdccUVs3rw5qtVqzM/PZz43MTERlUpl6a1Wq+W6LJTZcruI0AaDxcyAVmYGZDMzoJWZAdnMDGhlZkA2MwNamRkMqo4W45OTk3Ho0KE4fPhwrFmzJqampjKf27t3b8zNzS29TU9P57oslNlyu4jQBoPFzIBWZgZkMzOglZkB2cwMaGVmQDYzA1qZGQyqoWaz2VypT1av16NSqcSJk3MxNja2Up82Oeve8uGir9DXmudOR+Nrn4y5ufK8DrXRHdrIp2xt6KI7dJFP2bqI0Ea3aCOfsrWhi+7QRT5l6yJCG92ijXzK1oYuukMX+ZStiwhtdIs28ilbG7roDl3kU7YuIrTRLdrIp9M2OvqJcQAAAAAAAADoFxbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkmYxDgAAAAAAAEDSLMYBAAAAAAAASJrFOAAAAAAAAABJsxgHAAAAAAAAIGkW4wAAAAAAAAAkzWIcAAAAAAAAgKRZjAMAAAAAAACQNItxAAAAAAAAAJJmMQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASRvp5IP27dsX73nPe+Lxxx+P6667LmZmZmLPnj0tzzUajWg0Gkvvz83NRUTEy/V6h9clIqJ57nTRV+hri9+/ZrPZ1XOX20WENnpFG/kU3YYuekMX+RTdRYQ2ekUb+RTdhi56Qxf5FN1FhDZ6RRv5FN2GLnpDF/kU3UWENnpFG/kU3YYuekMX+RTdRYQ2ekUb+XTaRkeL8Z07d8batWtjeHg4FhYWolqtZj43MTERd911V8ufb7+21smnha46efJkVCqVrp233C4itEG5FdWGLigzMwOymRnQysyAbGYGtDIzIJuZAa3MDMjWbhsdLcYnJydjaGgo5ufnY/Xq1TEzM5P53N69e+POO+9cev+ll16Ka665Jo4fP96VgOv1etRqtZieno6xsbHc5/XizLKf14szy37e3NxcbN26Na666qrcZ73ScruI6L82vO7Kd14vziy6jX7rohdnlv28XpxZ9vOK7iKi/9rwuivfeb04s+g2+q2LXpxZ9vN6cWbZzyu6i4j+a8Prrnzn9eLMotvoty56cWbZz+vFmWU/r+guIvqvDa+78p3XizOLbqPfuujFmWU/rxdnlv28oruI6L82vO7Kd14vzuy0jY4W4x/4wAciImLXrl1/53Ojo6MxOjra8ueVSqVr38iIiLGxsa6e14szy35eL84s+3nDw8NdOyti+V1E9G8bXnflO68XZxbVRr920Yszy35eL84s+3lmRvHn9eLMQTuvF2eaGcWfWfbzenFm2c8zM4o/rxdnDtp5vTjTzCj+zLKf14szy36emVH8eb04c9DO68WZZkbxZ5b9vF6cWfbzzIziz+vFmYN2Xi/ObLeN7pYEAAAAAAAAACVjMQ4AAAAAAABA0lZ0MT46Ohq//uu/nvkrF8pwXi/OLPt5vThz0M7rhrJ/jV535TuvF2eWrY1++PrKfkdfc/nO64ayf41ed+U7rxdnlq2Nfvj6yn5HX3P5zuuGsn+NXnflO68XZ5atjX74+sp+R19z+c7rhrJ/jV535TuvF2eWrY1++PrKfkdfc/nO64ayf41ed+U7rxdndnreULPZbHblBgAAAAAAAABQQn6VOgAAAAAAAABJsxgHAAAAAAAAIGkW4wAAAAAAAAAkbWQlPsmhQ4diZmYm9uzZE/fcc0/ceuut8eCDD8aePXtifHw895mPPPJIDA8Px6FDh+LOO+/s6B9uP3DgQGzZsiUOHz4cN910U5w4cSL27NnT0d0Wz9u8eXMcOXIkarVarF+/Pm655ZZc59VqtXjqqafiuuuuW/ra8zhw4EBs2rQpHnvssbjxxhvj6quvznXHffv2xXvf+9544IEHuvI93LdvX9x+++2xf//+rnwPF898z3veE48//njXvo95dLuNsnexeGaZ2+h2FxHlbyP1Ll59ZhnbKHsXi2eaGWm1UfYuFs8scxtmRnpdvPrMMrZR9i4WzzQz0mqj7F0snlnmNsyM9Lp49ZllbKPsXSyeaWak1UbZu1g8s8xtmBnpdfHqM8vYRtm7WDzTzEirjbJ3sXhmmdswM9r/Hq7IT4wvLCxEtVqNiIgNGzbE8ePHY/fu3XH06NGunLljx44YHh6OSqUSZ86c6ei8arUan/vc52L37t1x7NixpbM7Va1W4+zZs7F58+aoVqsxPz+f+7zp6ekYHh6+6GvPe+a5c+fi5ptvjk2bNuW+486dO2Nqaqpr38OdO3fGFVdc0bXv4eKZa9eu7er3MY9ut1H2LhbPLHMb3e4iovxtpN7Fq88sYxtl72LxTDMjrTbK3sXimWVuw8xIr4tXn1nGNsrexeKZZkZabZS9i8Uzy9yGmZFeF68+s4xtlL2LxTPNjLTaKHsXi2eWuQ0zI70uXn1mGdsoexeLZ5oZabVR9i4WzyxzG2ZG+9/Dni/G77333rj77rtjdnY2pqam4sorr4z5+fk4ePBgbN++vStn3nfffbFx48ZYu3ZtPP/88x2dOTs7G7t27YqDBw/Gtm3bYnZ2tqNzXnneqlWr4vDhw7FmzZqYmprKfd7ii3r16tW57/fKOw4NDcW6dety33FycjLOnTvXte/h5ORkHDp0qGvfw8Uzjxw50tXvY6e63UY/dLF4Zpnb6HYXEeVvI+Uuss4sYxtl7+KVdzQzimFmlLMNMyOtLrLOLGMbZe/ilXc0M4phZpSzDTMjrS6yzixjG2Xv4pV3NDOKYWaUsw0zI60uss4sYxtl7+KVdzQzimFmlLMNM6P97+FQs9ls5roBAAAAAAAAAJTYivwqdQAAAAAAAAAoisU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkmYxDgAAAAAAAEDSLMYBAAAAAAAASJrFOAAAAAAAAABJsxgHAAAAAAAAIGkW4wAAAAAAAAAkzWIcAAAAAAAAgKRZjAMAAAAAAACQNItxAAAAAAAAAJJmMQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJM1iHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkmYxDgAAAAAAAEDSLMYBAAAAAAAASJrFOAAAAAAAAABJsxgHAAAAAAAAIGkW4wAAAAAAAAAkzWIcAAAAAAAAgKRZjAMAAAAAAACQNItxAAAAAAAAAJJmMQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJM1iHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkmYxDgAAAAAAAEDSLMYBAAAAAAAASJrFOAAAAAAAAABJsxgHAAAAAAAAIGkW4wAAAAAAAAAkzWIcAAAAAAAAgKRZjAMAAAAAAACQNItxAAAAAAAAAJJmMQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJM1iHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkmYxDgAAAAAAAEDSLMYBAAAAAAAASJrFOAAAAAAAAABJsxgHAAAAAAAAIGkW4wAAAAAAAAAkzWIcAAAAAAAAgKRZjAMAAAAAAACQNItxAAAAAAAAAJJmMQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJM1iHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkmYxDgAAAAAAAEDSLMYBAAAAAAAASJrFOAAAAAAAAABJsxgHAAAAAAAAIGkW4wAAAAAAAAAkzWIcAAAAAAAAgKRZjAMAAAAAAACQNItxAAAAAAAAAJJmMQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJM1iHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkmYxDgAAAAAAAEDSLMYBAAAAAAAASJrFOAAAAAAAAABJsxgHAAAAAAAAIGkW4wAAAAAAAAAkzWIcAAAAAAAAgKRZjAMAAAAAAACQNItxAAAAAAAAAJJmMQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJM1iHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkmYxDgAAAAAAAEDSLMYBAAAAAAAASJrFOAAAAAAAAABJsxgHAAAAAAAAIGkW4wAAAAAAAAAkzWIcAAAAAAAAgKRZjAMAAAAAAACQNItxAAAAAAAAAJJmMQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJM1iHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEkbKfoCXNrHP/7x+Mf/+B/H61//+ov+/NixY3H27NnYvn17/NVf/VU899xz8cQTT8Qv//Ivx8iI/5WStna6GBkZieHh4XjnO99Z0G1h5bTTxuWXXx5DQ0Pxjne8o6Dbwspop4tqtRqzs7Px7ne/u6Dbwsppp43Jycn40R/90RgfHy/otrAy2uni1KlTsX379tixY0dBt4WV004bq1evjunp6Xj/+99f0G1hZbTTxfnz56PRaMSP/diPFXRbWDnttHHs2LF4y1veEjfeeGNBt4WVsZwuvv71r8fRo0fj9OnTUavV4gd+4AcKuu1g8BPjJTY8PByXX355fOQjH4kvf/nL8YlPfCI++9nPRrPZjP3798eLL74YO3fujIhY+p+Quna62LFjRwwP+z9zDIZ22ti4cWM0Go2Cbwy9104X8/PzBd8WVk47bWzYsCFOnTpV8I2h99rp4tvf/nZ897vfLfjGsDLaaWN8fDze/OY3F3xj6L12ujh16pT/X4OB0U4btVotnn766YJvDL23nC7e+MY3RkREtVqNhYWFgm+cPj9eXGLnz5+PhYWFpcXeqlWrYmhoKI4fPx5ve9vb4uTJk/Hiiy/G5ORkjIyMxPj4eFx//fUF3xp6q50uvvSlL8VP/dRPFXxjWBnttHHZZZfFFVdcUfCNoffa6aJSqUSz2Sz4xrAy2mnjuuuui6mpqdi+fXvBt4beaqeL7du3x8zMTNx8880F3xp6r502zp49G7feemvBN4bea6eLrVu3+v8zGBjttPH6178+1q1bV/CNofeW08Wf/MmfxIYNG2J2djZqtVrBN07fUNNkLr2HH37Yr/WEV9EFZNMGtNIFZNMGtNIFZNMGtNIFZNMGtNJFeViMAwAAAAAAAJA0//guAAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApLW1GD906FDcf//9ERFxzz33xPHjx+NjH/tYfPvb3+7J5aBfaANa6QKyaQNa6QKyaQNa6QKyaQNa6QKyaYNBNtLOwwsLC1GtViMiYsOGDXH8+PHYvXt3HD16NMbHx1uebzQa0Wg0lt4/f/58vPDCC7F+/foYGhrKeXXoTLPZjJdffjnGx8djeLg7vzRBG6Sg223oghSYGZDNzIBWZgZkMzOglZkB2cwMaGVmQLZO21j2Yvzee++N/fv3xx133BFTU1Nx5ZVXxvz8fHzpS1+K2267LfNjJiYm4q677lr2ZWAlTU9Px5YtW3Kfow1S0402dEFqzAzIZmZAKzMDspkZ0MrMgGxmBrQyMyBbu20MNZvNZq8u8+q/RTI3Nxdbt26Np5+ZjrVjY736tMk71Thb9BX62ssv12PnDdfGSy+9FJVKpZA7aKM3tJFP0W3oojd0kU/RXURoo1e0kU/RbeiiN3SRT9FdRGijV7SRT9Ft6KI3dJFP0V1EaKNXtJFP0W3oojd0kU/RXURoo1e0kU+nbbT1q9TbNTo6GqOjoy1/vnZsLMbE0rERsXRFkb/iQxu9oY3uKKoNXfSGLrrDzEiPNrrDzEiLLrrDzEiPNrrDzEiLLrrDzEiPNrrDzEiLLrrDzEiPNrqj3Ta68w8SAAAAAAAAAEBJWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkmYxDgAAAAAAAEDSLMYBAAAAAAAASJrFOAAAAAAAAABJsxgHAAAAAAAAIGkW4wAAAAAAAAAkbaSdhw8dOhQzMzOxZ8+eeOSRR2J4eDgOHToUd955Z4yOjrY832g0otFoLL1fr9fz3xhKSBvQSheQTRvQSheQTRvQSheQTRvQSheQTRsMsrZ+YnxhYSGq1WpEROzYsSOGh4ejUqnEmTNnMp+fmJiISqWy9Far1fLfGEpIG9BKF5BNG9BKF5BNG9BKF5BNG9BKF5BNGwyyZS/G77333rj77rtjdnY2pqam4r777ouNGzfG2rVr4/nnn8/8mL1798bc3NzS2/T0dNcuDmWhDWilC8imDWilC8imDWilC8imDWilC8imDQbdULPZbK7UJ6vX61GpVOLEybkYGxtbqU+bnFONs0Vfoa/V6/W4dnx9zM2V53Woje7QRj5la0MX3aGLfMrWRYQ2ukUb+ZStDV10hy7yKVsXEdroFm3kU7Y2dNEdusinbF1EaKNbtJFP2drQRXfoIp+ydRGhjW7RRj6dttHWr1IHAAAAAAAAgH5jMQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJM1iHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEmzGH+Fp48ejXe94wfiTTuuj7d//y3x5BNPFH2lvtBoNOLX7vyVeMuNb4gf+Ic3xi9+8OciIuIrXz4cP/zud8Y73npT7Pr+m+MLj3y+4JvSCV10Rhfp00ZntJE2XXRGF+nTRme0kTZddEYX6dNGZ7SRNl10Rhfp00ZntJE2XXRmkLoYKfoCZfLhX/7F+OAvfCh+9udvjwcf+MP4pQ99MB59/M+Lvlbp/Yd/929ieHg4vvjVJ2JoaCi+852/jmazGT/3/tviv37yd+Mdu94VT33zSPzkj/9wfPGrT8SaNWuKvjJt0EVndJE+bXRGG2nTRWd0kT5tdEYbadNFZ3SRPm10Rhtp00VndJE+bXRGG2nTRWcGqQs/Mf63nnvuufjqX3wl3v/TPxMREe/9iZ+MqWPPxNSxY8VerOTm5+fjDz79qfjIv/9PMTQ0FBERmzZtjhdOnoyXXnwh3rHrXRERcf0Nr49KpRIPH/yTAm9Lu3TRGV2kTxud0UbadNEZXaRPG53RRtp00RldpE8bndFG2nTRGV2kTxud0UbadNGZQevCYvxvPTs9HZvHx2Nk5G9+iH5oaCi21LbG9PTxgm9Wbsee+VZcddX6+C8f/Y34oXe8NX7kn7wrHv38n8b6ajU2bLw6/uizD0ZExJcO/5/41tNHY/r4sWIvTFt00RldpE8bndFG2nTRGV2kTxud0UbadNEZXaRPG53RRtp00RldpE8bndFG2nTRmUHrou1fpX7gwIHYsmVLHD58OG666aY4ceJE7NmzJ/PZRqMRjUZj6f16vd75TVfA4t+EWNRsNgu6Sf84c+ZMHHtmMm54/Rvi1//Db8TXv/aX8RM/emv82Zf+Kj594IG469/+m/h//5+JeMMbvzfe+v0/GCMjlxV95Z5JtQ1dtE8XF1tuG/3URYQ2OqGNC8wMFuniYmYGi7RxgZnBIl1czMxgkTYuMDNYpIuLmRks0sYFZgaLBq2Lthfj1Wo1Pve5z8VP/dRPxeHDh6NWq13y2YmJibjrrrtyXXClbKnVYubZZ+Ps2bMxMjISzWYzZp6djlpta9FXK7Va7ZoYHh6O2973LyIi4nvfdGNs3bYtjjz5RLz9nbvi/of+19Kzb3vzm+KG1+8o6qo9l2IbuuiMLi623Db6pYsIbXRKGxeYGSzSxcXMDBZp4wIzg0W6uJiZwSJtXGBmsEgXFzMzWKSNC8wMFg1aF23/KvXZ2dnYtWtXHDx4MLZt2xazs7OXfHbv3r0xNze39DY9PZ3rsr20cePGuPH7borP/P6nIyLioQcfiK3XbItrtm0r9mIlt75ajXe+64fiTx8+GBER08en4vixY7H9+uvjxInvLD33e//9d+KK77ki3vmuf1TUVXsuxTZ00RldXGy5bfRLFxHa6JQ2LjAzWKSLi5kZLNLGBWYGi3RxMTODRdq4wMxgkS4uZmawSBsXmBksGrQuhpor+HsE6vV6VCqVOHFyLsbGxlbq0y7bU9/8ZtzxwdvjhRdOxtjasfjkfZ+KHW98Y9HXanGqcbboK1zk2DOT8Sv/8o544YWTsWrVqvjXe/9t/OiP/7P46G/8x/jD+z8TzWYzrr/h9fHR39wXr91y6b91tFLq9XpcO74+5ubK8zoscxv90kVEudroty4iytdGmbuI6J82ytRFRP+1UbYuIsrdRr90EVGuNvqti4jytVHmLiL6p40ydRHRf22UrYuIcrfRL11ElKuNfusionxtlLmLiP5po0xdRPRfG2XrIqLcbfRLFxHlaqPfuogoXxtl7iKif9ooUxcR/ddG2bqIKHcb/dJFRLna6LcuIjpvw2K8D5Upln5kkKRLG/mUrQ1ddIcu8ilbFxHa6BZt5FO2NnTRHbrIp2xdRGijW7SRT9na0EV36CKfsnURoY1u0UY+ZWtDF92hi3zK1kWENrpFG/l02kbbv0odAAAAAAAAAPqJxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJM1iHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkjbS7gccOHAgNm/eHEeOHIlarRbr16+PW265JfPZRqMRjUZj6f16vd75TaHktAHZltuGLhgkZgZkMzOglZkB2cwMaGVmQDYzA1qZGQyqtn9ivFqtxtmzZ2Pz5s1RrVZjfn7+ks9OTExEpVJZeqvVarkuC2WmDci23DZ0wSAxMyCbmQGtzAzIZmZAKzMDspkZ0MrMYFC1vRifnZ2NVatWxeHDh2PNmjUxNTV1yWf37t0bc3NzS2/T09O5Lgtlpg3Ittw2dMEgMTMgm5kBrcwMyGZmQCszA7KZGdDKzGBQDTWbzeZKfbJ6vR6VSiVOnJyLsbGxlfq0yTnVOFv0FfpavV6Pa8fXx9xceV6H2ugObeRTtjZ00R26yKdsXURoo1u0kU/Z2tBFd+gin7J1EaGNbtFGPmVrQxfdoYt8ytZFhDa6RRv5lK0NXXSHLvIpWxcR2ugWbeTTaRtt/8Q4AAAAAAAAAPQTi3EAAAAAAAAAkmYxDgAAAAAAAEDSLMYBAAAAAAAASJrFOAAAAAAAAABJsxgHAAAAAAAAIGkW4wAAAAAAAAAkzWIcAAAAAAAAgKRZjAMAAAAAAACQNItxAAAAAAAAAJJmMQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJM1iHAAAAAAAAICkjbT7AQcOHIharRZPPfVUXHfddTEzMxN79uzpxd2gr2gDsmkDWukCsmkDWukCsmkDWukCsmkDWumCQdX2Yrxarcb09HQMDw/HwsJCVKvVSz7baDSi0WgsvV+v1zu7JRd57dt/tegr9LXmudM9OVcbxdNGPkW3oYve0EU+RXcRoY1e0UY+Rbehi97QRT5FdxGhjV7RRj5Ft6GL3tBFPkV3EaGNXtFGPkW3oYve0EU+RXcRoY1e0UY+nbbR9q9Sn52djU2bNsX8/HysXr06ZmdnL/nsxMREVCqVpbdardbRJaEfaAOyLbcNXTBIzAzIZmZAKzMDspkZ0MrMgGxmBrQyMxhUQ81ms9mrw7P+FkmtVosTJ+dibGysV582eeve8uGir9DXmudOR+Nrn4y5ueJeh9roDW3kU3QbuugNXeRTdBcR2ugVbeRTdBu66A1d5FN0FxHa6BVt5FN0G7roDV3kU3QXEdroFW3kU3QbuugNXeRTdBcR2ugVbeTTaRtt/yr1doyOjsbo6GgvPwX0JW1AK11ANm1AK11ANm1AK11ANm1AK11ANm2QkrZ/lToAAAAAAAAA9BOLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkmYxDgAAAAAAAEDSLMYBAAAAAAAASJrFOAAAAAAAAABJsxgHAAAAAAAAIGkW4wAAAAAAAAAkzWIcAAAAAAAAgKSNtPsBBw4ciE2bNsVjjz0WN954Y1x99dVxyy23ZD7baDSi0WgsvV+v1zu/KZScNiDbctvQBYPEzIBsZga0MjMgm5kBrcwMyGZmQCszg0HV9k+MV6vVOHfuXNx8882xadOmmJ+fv+SzExMTUalUlt5qtVquy0KZaQOyLbcNXTBIzAzIZmZAKzMDspkZ0MrMgGxmBrQyMxhUbS/GZ2dnY9WqVTE0NBTr1q2LqampSz67d+/emJubW3qbnp7OdVkoM21AtuW2oQsGiZkB2cwMaGVmQDYzA1qZGZDNzIBWZgaDqu1fpf6+973vove3b99+yWdHR0djdHS0/VtBH9IGZFtuG7pgkJgZkM3MgFZmBmQzM6CVmQHZzAxoZWYwqNr+iXEAAAAAAAAA6CcW4wAAAAAAAAAkzWIcAAAAAAAAgKRZjAMAAAAAAACQNItxAAAAAAAAAJJmMQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJM1iHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSNlL0Bcrk6aNH4xc+8PNx8uRsVCpXxifv/d14w44dRV+rcGee/UKcmzsWcebluPyGfx7Da9Zf9N/PvXAkzhz/XFx27Y/Eqsq2iIg4e/KJOPf8X0Zz4cUYee3bY2TDzqXnm81mnJ15LM7XpyIiYtXG74uR6ptW6suhTbrIpgu0kU0bg00X2XSBNrJpY7DpIpsu0EY2bQw2XWTTBdrIpo3BpotsurjAT4y/wod/+Rfjg7/wofjaE0/Fnf/q1+KXPvTBoq9UCquu3B6j1/1ExGVrW/5b8/T/jbOz34ihK66+6M+Hr9gYl237pzG87vqWjzn/4jejufBCXP6Gn47Lr78tzp74izi/8GLP7k8+usimC7SRTRuDTRfZdIE2smljsOkimy7QRjZtDDZdZNMF2simjcGmi2y6uMBi/G8999xz8dW/+Eq8/6d/JiIi3vsTPxlTx56JqWPHir1YCQy/ZjyGLn9N5n87M/35uOy1PxgxtOrij1lTjeHVV0XEUMvHnHvp6VhV/d4YGhqOoZHVsWrd9jj34tFeXJ2cdHFpuhhs2rg0bQwuXVyaLgabNi5NG4NLF5emi8GmjUvTxuDSxaXpYrBp49K0Mbh0cWm6uKCjxfi+fftieno6Pvaxj8Wjjz4a999/f+ZzjUYj6vX6RW9l9ez0dGweH4+Rkb/57fJDQ0OxpbY1pqePF3yz8jo7+/UYWn1VDH/PprY+rnn65Rh6xd9KGbp8bcSZl7t9vRW33C4i+qcNXbRPF63MDCK08WpmBhG6yGJmEKGNVzMziNBFFjODCG28mplBhC6ymBlEaOPVzAwiBrOLjhbjO3fujKmpqdi9e3ccO3YsqtVq5nMTExNRqVSW3mq1Wq7L9trQ0MV/66HZbBZ0k/I736jHuZPfiJHNb+3sgFd+qxP5Ni+3i4j+akMXy6eLbGYG2mhlZqCLbGYG2mhlZqCLbGYG2mhlZqCLbGYG2mhlZjCoXXS0GJ+cnIxz587FwYMHY9u2bTE7O5v53N69e2Nubm7pbXp6Otdle2lLrRYzzz4bZ8+ejYi/iWXm2emo1bYWfLNyap76TjTPzEfjyT+IhW/8XjRPnYgz038aZ09+4+/92KHL10bz9IW/OdI883Lmv2vQb5bbRUT/tKGL9ugim5mBNlqZGegim5mBNlqZGegim5mBNlqZGegim5mBNlqZGQxqFyOdfNAHPvCBiIjYtWvX3/nc6OhojI6OdvIpVtzGjRvjxu+7KT7z+5+On/352+OhBx+Irddsi2u2bSv6aqW0at31sWrd9UvvN44+FCMbb4pVlW1/78cOV14X52a/EcOVfxBx7nSce/FoXP66H+vhbVfGcruI6J82dNEeXWQzM9BGKzMDXWQzM9BGKzMDXWQzM9BGKzMDXWQzM9BGKzODQe2io8V4qn7rE/8t7vjg7fHR//wbMbZ2LD5536eKvlIpnHn20Tg390zEmVNx+lv/I4aGL4vRHT/7d37MuRe+GWf++s8jzjXifP2ZOPvcV+Lya38khq/YEKuuuiGa330uTj/5+xERMbLxphhefdVKfCl0QBfZdIE2smljsOkimy7QRjZtDDZdZNMF2simjcGmi2y6QBvZtDHYdJFNFxcMNVfwF+zX6/WoVCpx4uRcjI2NrdSnTc66t3y46Cv0tea509H42idjbq48r0NtdIc28ilbG7roDl3kU7YuIrTRLdrIp2xt6KI7dJFP2bqI0Ea3aCOfsrWhi+7QRT5l6yJCG92ijXzK1oYuukMX+ZStiwhtdIs28um0jY7+jXEAAAAAAAAA6BcW4wAAAAAAAAAkzWIcAAAAAAAAgKRZjAMAAAAAAACQNItxAAAAAAAAAJJmMQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJM1iHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNI6Wozv27cv6vV63HPPPfHHf/zH8cUvfjHzuUajEfV6/aI3SNVyu4jQBoPFzIBWZgZkMzOglZkB2cwMaGVmQDYzA1qZGQyqjhbjO3fujCuuuCI2b94c1Wo15ufnM5+bmJiISqWy9Far1XJdFspsuV1EaIPBYmZAKzMDspkZ0MrMgGxmBrQyMyCbmQGtzAwGVUeL8cnJyTh06FAcPnw41qxZE1NTU5nP7d27N+bm5pbepqenc10Wymy5XURog8FiZkArMwOymRnQysyAbGYGtDIzIJuZAa3MDAbVULPZbK7UJ6vX61GpVOLEybkYGxtbqU+bnHVv+XDRV+hrzXOno/G1T8bcXHleh9roDm3kU7Y2dNEdusinbF1EaKNbtJFP2drQRXfoIp+ydRGhjW7RRj5la0MX3aGLfMrWRYQ2ukUb+ZStDV10hy7yKVsXEdroFm3k02kbHf3EOAAAAAAAAAD0C4txAAAAAAAAAJJmMQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJM1iHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJC0kU4+aN++ffGe97wnHn/88bjuuutiZmYm9uzZ0/Jco9GIRqOx9P7c3FxERLxcr3d4XSIimudOF32Fvrb4/Ws2m109d7ldRGijV7SRT9Ft6KI3dJFP0V1EaKNXtJFP0W3oojd0kU/RXURoo1e0kU/RbeiiN3SRT9FdRGijV7SRT9Ft6KI3dJFP0V1EaKNXtJFPp210tBjfuXNnrF27NoaHh2NhYSGq1WrmcxMTE3HXXXe1/Pn2a2udfFroqpMnT0alUunaecvtIkIblFtRbeiCMjMzIJuZAa3MDMhmZkArMwOymRnQysyAbO220dFifHJyMoaGhmJ+fj5Wr14dMzMzmc/t3bs37rzzzqX3X3rppbjmmmvi+PHjXQm4Xq9HrVaL6enpGBsby31eL84s+3m9OLPs583NzcXWrVvjqquuyn3WKy23i4j+a8Prrnzn9eLMotvoty56cWbZz+vFmWU/r+guIvqvDa+78p3XizOLbqPfuujFmWU/rxdnlv28oruI6L82vO7Kd14vziy6jX7rohdnlv28XpxZ9vOK7iKi/9rwuivfeb04s+g2+q2LXpxZ9vN6cWbZzyu6i4j+a8Prrnzn9eLMTtvoaDH+gQ98ICIidu3a9Xc+Nzo6GqOjoy1/XqlUuvaNjIgYGxvr6nm9OLPs5/XizLKfNzw83LWzIpbfRUT/tuF1V77zenFmUW30axe9OLPs5/XizLKfZ2YUf14vzhy083pxpplR/JllP68XZ5b9PDOj+PN6ceagndeLM82M4s8s+3m9OLPs55kZxZ/XizMH7bxenGlmFH9m2c/rxZllP8/MKP68Xpw5aOf14sx22+huSQAAAAAAAABQMhbjAAAAAAAAACRtRRfjo6Oj8eu//uuZv3KhDOf14syyn9eLMwftvG4o+9fodVe+83pxZtna6Ievr+x39DWX77xuKPvX6HVXvvN6cWbZ2uiHr6/sd/Q1l++8bij71+h1V77zenFm2droh6+v7Hf0NZfvvG4o+9fodVe+83pxZtna6Ievr+x39DWX77xuKPvX6HVXvvN6cWan5w01m81mV24AAAAAAAAAACXkV6kDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNJGVuKTHDp0KGZmZmLPnj1xzz33xK233hoPPvhg7NmzJ8bHx3Of+cgjj8Tw8HAcOnQo7rzzzo7+4fYDBw7Eli1b4vDhw3HTTTfFiRMnYs+ePR3dbfG8zZs3x5EjR6JWq8X69evjlltuyXVerVaLp556Kq677rqlrz2PAwcOxKZNm+Kxxx6LG2+8Ma6++upcd9y3b1+8973vjQceeKAr38N9+/bF7bffHvv37+/K93DxzPe85z3x+OOPd+37mEe32yh7F4tnlrmNbncRUf42Uu/i1WeWsY2yd7F4ppmRVhtl72LxzDK3YWak18WrzyxjG2XvYvFMMyOtNsrexeKZZW7DzEivi1efWcY2yt7F4plmRlptlL2LxTPL3IaZkV4Xrz6zjG2UvYvFM82MtNooexeLZ5a5DTOj/e/hivzE+MLCQlSr1YiI2LBhQxw/fjx2794dR48e7cqZO3bsiOHh4ahUKnHmzJmOzqtWq/G5z30udu/eHceOHVs6u1PVajXOnj0bmzdvjmq1GvPz87nPm56ejuHh4Yu+9rxnnjt3Lm6++ebYtGlT7jvu3LkzpqamuvY93LlzZ1xxxRVd+x4unrl27dqufh/z6HYbZe9i8cwyt9HtLiLK30bqXbz6zDK2UfYuFs80M9Jqo+xdLJ5Z5jbMjPS6ePWZZWyj7F0snmlmpNVG2btYPLPMbZgZ6XXx6jPL2EbZu1g808xIq42yd7F4ZpnbMDPS6+LVZ5axjbJ3sXimmZFWG2XvYvHMMrdhZrT/Pez5Yvzee++Nu+++O2ZnZ2NqaiquvPLKmJ+fj4MHD8b27du7cuZ9990XGzdujLVr18bzzz/f0Zmzs7Oxa9euOHjwYGzbti1mZ2c7OueV561atSoOHz4ca9asiampqdznLb6oV69enft+r7zj0NBQrFu3LvcdJycn49y5c137Hk5OTsahQ4e69j1cPPPIkSNd/T52qttt9EMXi2eWuY1udxFR/jZS7iLrzDK2UfYuXnlHM6MYZkY52zAz0uoi68wytlH2Ll55RzOjGGZGOdswM9LqIuvMMrZR9i5eeUczoxhmRjnbMDPS6iLrzDK2UfYuXnlHM6MYZkY52zAz2v8eDjWbzWauGwAAAAAAAABAia3Ir1IHAAAAAAAAgKJYjAMAAAAAAACQNItxAAAAAAAAAJJmMQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJM1iHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkmYxDgAAAAAAAEDSLMYBAAAAAAAASJrFOAAAAAAAAABJsxgHAAAAAAAAIGkW4wAAAAAAAAAkzWIcAAAAAAAAgKRZjAMAAAAAAACQNItxAAAAAAAAAJJmMQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJM1iHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkmYxDgAAAAAAAEDSLMYBAAAAAAAASJrFOAAAAAAAAABJsxgHAAAAAAAAIGkW4wAAAAAAAAAkzWIcAAAAAAAAgKRZjAMAAAAAAACQNItxAAAAAAAAAJJmMQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJM1iHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkmYxDgAAAAAAAEDSLMYBAAAAAAAASJrFOAAAAAAAAABJsxgHAAAAAAAAIGkW4wAAAAAAAAAkzWIcAAAAAAAAgKRZjAMAAAAAAACQNItxAAAAAAAAAJJmMQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJM1iHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkmYxDgAAAAAAAEDSLMYBAAAAAAAASJrFOAAAAAAAAABJsxgHAAAAAAAAIGkW4wAAAAAAAAAkzWIcAAAAAAAAgKRZjAMAAAAAAACQNItxAAAAAAAAAJJmMQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJM1iHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkmYxDgAAAAAAAEDSLMYBAAAAAAAASJrFOAAAAAAAAABJsxgHAAAAAAAAIGkW4wAAAAAAAAAkzWIcAAAAAAAAgKRZjAMAAAAAAACQNItxAAAAAAAAAJJmMQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJM1iHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkmYxDgAAAAAAAEDSLMYBAAAAAAAASJrFOAAAAAAAAABJsxgHAAAAAAAAIGkW4wAAAAAAAAAkzWIcAAAAAAAAgKRZjAMAAAAAAACQNItxAAAAAAAAAJJmMQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJM1iHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkmYxDgAAAAAAAEDSLMYBAAAAAAAASJrFOAAAAAAAAABJsxgHAAAAAAAAIGkW4wAAAAAAAAAkzWIcAAAAAAAAgKRZjJfYxz/+8Thy5EjLnx87diyefvrpiIj4+te/Hg899FDs27cvzp49u9JXhBXXThePPPJIfOELX1jpK0Ih2mnjC1/4Qjz22GMrfUVYce108dWvfjUefvjhlb4iFKKdNu6555749re/vdJXhBXXThd/9Ed/FE888cRKXxEK0U4bjz/+eHzmM59Z6SvCimuni4MHD8b//J//c6WvCIVop43f+Z3fib/8y79c6SvCiltOF0888UQ89NBDceDAgfizP/uzlb7iwLEYL7Hh4eG4/PLL4yMf+Uh8+ctfjk984hPx2c9+NprNZuzfvz9efPHFeOMb3xgRETt37iz4trAy2ulix44dMTzs/8wxGNppY+PGjdFoNAq+MfReO13Mz88XfFtYOe20sWHDhjh16lTBN4bea6eLb3/72/Hd73634BvDyminjfHx8Xjzm99c8I2h99rp4tSpU/5/DQZGO23UarWlpSCkbDldjI+Px/z8fFSr1VhYWCj6ysmzMSqx8+fPx8LCwtJib9WqVTE0NBTHjx+Pt73tbXHy5MnYv39/rF27NiYnJ2NycrLgG0PvtdPFfffdF5s2bSr4xrAy2mnjxIkTccUVVxR8Y+i9drp49tlnY3Z2tuAbw8pop40rr7wypqamCr4x9F47XWzfvj1mZmYKvjGsjHbaeOqpp+KGG24o+MbQe+10sXr16hgdHS34xrAy2mljzZo1UavVCr4x9N5yupieno4rr7wyZmdnY/Xq1QXfOH1DzWazWfQl+Ls9/PDD8e53v7voa0Cp6AKyaQNa6QKyaQNa6QKyaQNa6QKyaQNa6aI8LMYBAAAAAAAASJpfpQ4AAAAAAABA0izGAQAAAAAAAEiaxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJG2knYcPHToUMzMzsWfPnrjnnnvi1ltvjQcffDD27NkT4+PjLc83Go1oNBpL758/fz5eeOGFWL9+fQwNDeW/PXSg2WzGyy+/HOPj4zE83J2/G6INUtDtNnRBCswMyGZmQCszA7KZGdDKzIBsZga0MjMgW6dttLUYX1hYiGq1GhERGzZsiOPHj8fu3bvj6NGjmbFMTEzEXXfd1c6ngBUzPT0dW7Zs6cpZ2iAl3WpDF6TEzIBsZga0MjMgm5kBrcwMyGZmQCszA7K128ZQs9lsLufBe++9N/bv3x933HFHvPWtb43Jyck4ffp0PPnkk3HbbbfFa1/72paPefXfIpmbm4utW7fG089Mx9qxsWVfkoudapwt+gp97eWX67HzhmvjpZdeikqlkvs8bZSHNvLpZhu6KA9d5GNmpEsb+ZgZadJFPmZGurSRj5mRJl3kY2akSxv5mBlp0kU+Zka6tJFPp20sezHeDfV6PSqVSpw4ORdjYumYWPKp1+tx7fj6mJsrz+tQG92hjXzK1oYuukMX+ZStiwhtdIs28ilbG7roDl3kU7YuIrTRLdrIp2xt6KI7dJFP2bqI0Ea3aCOfsrWhi+7QRT5l6yJCG92ijXw6baM7/yABAAAAAAAAAJSUxTgAAAAAAAAASbMYBwAAAAAAACBpFuMAAAAAAAAAJM1iHAAAAAAAAICkWYwDAAAAAAAAkDSLcQAAAAAAAACSZjEOAAAAAAAAQNIsxgEAAAAAAABImsU4AAAAAAAAAEmzGAcAAAAAAAAgaRbjAAAAAAAAACTNYhwAAAAAAACApFmMAwAAAAAAAJA0i3EAAAAAAAAAkmYxDgAAAAAAAEDSRtp5+NChQzEzMxN79uyJRx55JIaHh+PQoUNx5513xujoaMvzjUYjGo3G0vv1ej3/jaGEtAGtdAHZtAGtdAHZtAGtdAHZtAGtdAHZtMEga+snxhcWFqJarUZExI4dO2J4eDgqlUqcOXMm8/mJiYmoVCpLb7VaLf+NoYS0Aa10Adm0Aa10Adm0Aa10Adm0Aa10Adm0wSBb9mL83nvvjbvvvjtmZ2djamoq7rvvvti4cWOsXbs2nn/++cyP2bt3b8zNzS29TU9Pd+3iUBbagFa6gGzagFa6gGzagFa6gGzagFa6gGzaYNANNZvN5kp9snq9HpVKJU6cnIuxsbGV+rTJOdU4W/QV+lq9Xo9rx9fH3Fx5Xofa6A5t5FO2NnTRHbrIp2xdRGijW7SRT9na0EV36CKfsnURoY1u0UY+ZWtDF92hi3zK1kWENrpFG/mUrQ1ddIcu8ilbFxHa6BZt5NNpG239KnUAAAAAAAAA6DcW4wAAAAAAAAAkzWIcAID/v737j5H7rBM7/tkfydq+8w6OJ44dMolRnQAJ2KQpoRw/NodalwvVqYBwSrnjIsKP6wmdUNSetFVbmrY6U6SeUv/TKinpceIOHF1CVVWisi5ALrFOh1ugogQn5pxdT6zDyTphhy7yxLuZ/gGz9ma+LrvznfE888zrJc0fTmY/+8xo3vr88XjXAAAAAABZczEOAAAAAAAAQNZcjAMAAAAAAACQNRfjAAAAAAAAAGTNxTgAAAAAAAAAWXMxDgAAAAAAAEDWXIwDAAAAAAAAkDUX4wAAAAAAAABkzcU4AAAAAAAAAFlzMQ4AAAAAAABA1lyMAwAAAAAAAJA1F+MX+eGJE3HHu34l3nzzTfHOt98eP3jqqUEfaSg0m834vXt/N966743xK39rX3zqno9GRMS3/9ex+LW/8+5419tujZm33xZ//s1vDPikdEMX3dFF/rTRHW3kTRfd0UX+tNEdbeRNF93RRf600R1t5E0X3dFF/rTRHW3kTRfdGaUuJgd9gJR8+nc+Ffd8/JPxm791dzz6yJ/Gb3/ynnj8yb8Y9LGS96//5T+L8fHx+NZ3n4qxsbH40Y/+OlqtVnz0wx+K//jgH8a7Zu6IZ54+Hh/89V+Lb333qdi8efOgj8wG6KI7usifNrqjjbzpoju6yJ82uqONvOmiO7rInza6o4286aI7usifNrqjjbzpojuj1IWfGP+5559/Pr77nW/Hhz/yGxER8f4PfDDm556N+bm5wR4scUtLS/EnX/pi/PN/9W9jbGwsIiJ27twVL549Gz9+6cV418wdERFx0+vfEJVKJf7syP8Y4GnZKF10Rxf500Z3tJE3XXRHF/nTRne0kTdddEcX+dNGd7SRN110Rxf500Z3tJE3XXRn1LrY8MX44cOH4+jRo3H//ffH448/Hg8//HA/znXZPVevx65rr43JyZ/9EP3Y2FhcV7s+6vVTAz5Z2uae/au46qrt8e8///vxnne9Ld73d++Ix7/x9dhercbVO66J//ZfH42IiP957C/jr354Iuqn5gZ74D7KsQ1ddEcXa2mDNm1coAvadLGWNmjTxgW6oE0Xa2mDNm1coAvadLGWNmjTxgW6oG3UutjwxXi1Wo3HHnss9u/fH3Nzc1GtVi/53GazGY1GY80jZe2/CdHWarUGdJLhcf78+Zh79mS8/g1vjK8/8Zfx7/7gP8Qn7v5ILLzwQnzp8CPxpS8+FL/6jrfGQw/+p3jb298Rk5NXDPrIfZNrG7rYOF2std42hqmLCG10QxsX2Bm06WItO4M2bVxgZ9Cmi7XsDNq0cYGdQZsu1rIzaNPGBXYGbaPWxYb/jfGFhYWYmZmJI0eOxK233hpnzpy55HMPHjwY9913X6kDXi7X1Wpx+rnnYnl5OSYnJ6PVasXp5+pRq10/6KMlrVa7IcbHx+NDd/2jiIh405v3xfW7d8fxHzwV73z3TDz81f+++ty//TffHK9/w82DOmrf5diGLrqji7XW28awdBGhjW5p4wI7gzZdrGVn0KaNC+wM2nSxlp1BmzYusDNo08VadgZt2rjAzqBt1LrY8E+M33XXXTEzMxOf+cxnYmZmJg4cOHDJ587Ozsbi4uLqo16vlzpsP+3YsSP2veXW+PIffykiIr766CNx/Q2744bduwd7sMRtr1bj3Xe8J77+Z0ciIqJ+aj5Ozc3FnptuijNnfrT6vD/6L/85tvzSlnj3Hb86qKP2XY5t6KI7ulhrvW0MSxcR2uiWNi6wM2jTxVp2Bm3auMDOoE0Xa9kZtGnjAjuDNl2sZWfQpo0L7AzaRq2LsdZl/D0CjUYjKpVKnDm7GNPT05fr267bM08/HZ+45+548cWzMb11Oh586Itx8y23DPpYHX7aXB70EdaYe/Zk/O4//kS8+OLZmJiYiH86+y/i7//6P4jP//6/iT99+MvRarXipte/IT7/B4fitdfVBn3caDQa8bprt8fiYjqfw5TbGJYuItJqY9i6iEivjZS7iBieNlLqImL42kiti4i02xiWLiLSamPYuohIr42Uu4gYnjZS6iJi+NpIrYuItNsYli4i0mpj2LqISK+NlLuIGJ42UuoiYvjaSK2LiLTbGJYuItJqY9i6iEivjZS7iBieNlLqImL42kiti4i02xiWLiLSamPYuojovg0X40MopViGkUWSL22Uk1obuugNXZSTWhcR2ugVbZSTWhu66A1dlJNaFxHa6BVtlJNaG7roDV2Uk1oXEdroFW2Uk1obuugNXZSTWhcR2ugVbZTTbRsb/lXqAAAAAAAAADBMXIwDAAAAAAAAkDUX4wAAAAAAAABkzcU4AAAAAAAAAFlzMQ4AAAAAAABA1lyMAwAAAAAAAJA1F+MAAAAAAAAAZM3FOAAAAAAAAABZczEOAAAAAAAAQNZcjAMAAAAAAACQNRfjAAAAAAAAAGTNxTgAAAAAAAAAWXMxDgAAAAAAAEDWXIwDAAAAAAAAkDUX4wAAAAAAAABkbXKjX3D48OHYtWtXHD9+PGq1Wmzfvj1uv/32wuc2m81oNpurf240Gt2fFBKnDSi23jZ0wSixM6CYnQGd7AwoZmdAJzsDitkZ0MnOYFRt+CfGq9VqLC8vx65du6JarcbS0tIln3vw4MGoVCqrj1qtVuqwkDJtQLH1tqELRomdAcXsDOhkZ0AxOwM62RlQzM6ATnYGo2rDF+MLCwsxMTERx44di82bN8f8/Pwlnzs7OxuLi4urj3q9XuqwkDJtQLH1tqELRomdAcXsDOhkZ0AxOwM62RlQzM6ATnYGo2qs1Wq1Ltc3azQaUalU4szZxZienr5c3zY7P20uD/oIQ63RaMTrrt0ei4vpfA610RvaKCe1NnTRG7ooJ7UuIrTRK9ooJ7U2dNEbuigntS4itNEr2igntTZ00Ru6KCe1LiK00SvaKCe1NnTRG7ooJ7UuIrTRK9oop9s2NvwT4wAAAAAAAAAwTFyMAwAAAAAAAJA1F+MAAAAAAAAAZM3FOAAAAAAAAABZczEOAAAAAAAAQNZcjAMAAAAAAACQNRfjAAAAAAAAAGTNxTgAAAAAAAAAWXMxDgAAAAAAAEDWXIwDAAAAAAAAkDUX4wAAAAAAAABkzcU4AAAAAAAAAFlzMQ4AAAAAAABA1lyMAwAAAAAAAJA1F+MAAAAAAAAAZM3FOAAAAAAAAABZm9zoFxw+fDhqtVo888wzceONN8bp06fjwIEDhc9tNpvRbDZX/9xoNLo/Kate+87PDPoIQ6218nJf5mpj8LRRzqDb0EV/6KKcQXcRoY1+0UY5g25DF/2hi3IG3UWENvpFG+UMug1d9Icuyhl0FxHa6BdtlDPoNnTRH7ooZ9BdRGijX7RRTrdtbPgnxqvVatTr9RgfH49z585FtVq95HMPHjwYlUpl9VGr1bo6JAwDbUCx9bahC0aJnQHF7AzoZGdAMTsDOtkZUMzOgE52BqNqwxfjCwsLsXPnzlhaWopNmzbFwsLCJZ87Ozsbi4uLq496vV7qsJAybUCx9bahC0aJnQHF7AzoZGdAMTsDOtkZUMzOgE52BqNqw79K/a677oqIiJmZmV/43KmpqZiamtr4qWAIaQOKrbcNXTBK7AwoZmdAJzsDitkZ0MnOgGJ2BnSyMxhVG/6JcQAAAAAAAAAYJi7GAQAAAAAAAMiai3EAAAAAAAAAsuZiHAAAAAAAAICsuRgHAAAAAAAAIGsuxgEAAAAAAADImotxAAAAAAAAALLmYhwAAAAAAACArLkYBwAAAAAAACBrLsYBAAAAAAAAyJqLcQAAAAAAAACy5mIcAAAAAAAAgKy5GAcAAAAAAAAgay7GAQAAAAAAAMiai3EAAAAAAAAAsuZiHAAAAAAAAICsTW70Cw4fPhw7d+6MJ554Ivbt2xfXXHNN3H777YXPbTab0Ww2V//caDS6PykkThtQbL1t6IJRYmdAMTsDOtkZUMzOgE52BhSzM6CTncGo2vBPjFer1VhZWYnbbrstdu7cGUtLS5d87sGDB6NSqaw+arVaqcNCyrQBxdbbhi4YJXYGFLMzoJOdAcXsDOhkZ0AxOwM62RmMqg1fjC8sLMTExESMjY3Ftm3bYn5+/pLPnZ2djcXFxdVHvV4vdVhImTag2Hrb0AWjxM6AYnYGdLIzoJidAZ3sDChmZ0AnO4NRteFfpX7XXXet+fOePXsu+dypqamYmpra+KlgCGkDiq23DV0wSuwMKGZnQCc7A4rZGdDJzoBidgZ0sjMYVRv+iXEAAAAAAAAAGCYuxgEAAAAAAADImotxAAAAAAAAALLmYhwAAAAAAACArLkYBwAAAAAAACBrLsYBAAAAAAAAyJqLcQAAAAAAAACy5mIcAAAAAAAAgKy5GAcAAAAAAAAgay7GAQAAAAAAAMiai3EAAAAAAAAAsuZiHAAAAAAAAICsuRgHAAAAAAAAIGuTgz5ASn544kR8/GO/FWfPLkSl8pp48At/GG+8+eZBH2vgzj/357GyOBdx/idx5ev/YYxv3r7m/6+8eDzOn3osrnjd+2KisjsiIpbPPhUrL/zvaJ17KSZf+86YvHrv6vNbrVYsn34iXmnMR0TExI63xGT1zZfr5bBBuiimC7RRTBujTRfFdIE2imljtOmimC7QRjFtjDZdFNMF2iimjdGmi2K6uMBPjF/k07/zqbjn45+M7z31TNz7T34vfvuT9wz6SEmYeM2emLrxAxFXbO34f62X/28sL3w/xrZcs+a/j2/ZEVfs/nsxvu2mjq955aWno3XuxbjyjR+JK2/6UCyf+U68cu6lvp2fcnRRTBdoo5g2RpsuiukCbRTTxmjTRTFdoI1i2hhtuiimC7RRTBujTRfFdHGBi/Gfe/755+O73/l2fPgjvxEREe//wAdjfu7ZmJ+bG+zBEjD+y9fG2JW/XPj/zte/EVe89h0RYxNrv2ZzNcY3XRURYx1fs/LjH8ZE9U0xNjYeY5ObYmLbnlh56UQ/jk5Jurg0XYw2bVyaNkaXLi5NF6NNG5emjdGli0vTxWjTxqVpY3Tp4tJ0Mdq0cWnaGF26uDRdXOBi/Oeeq9dj17XXxuTkz367/NjYWFxXuz7q9VMDPlm6lhf+T4xtuirGf2nnhr6u9fJPYuyiv5UyduXWiPM/6fXx6AFdbJwuRoM2Nk4b+dPFxuliNGhj47SRP11snC5GgzY2Thv508XG6WI0aGPjtJE/XWzcKHbR1cX4oUOHol6vx/333x+PP/54PPzww4XPazab0Wg01jxSNja29m89tFqtAZ0kfa80G7Fy9vsxuett3Q24+K3O5G1ebxcRw9WGLtZPF8XsDLTRyc5AF8XsDLTRyc5AF8XsDLTRyc5AF8XsDLTRyc5gVLvo6mJ87969MT8/H/v374+5ubmoVquFzzt48GBUKpXVR61WK3XYfrquVovTzz0Xy8vLEfGzWE4/V49a7foBnyxNrZ/+KFrnl6L5gz+Jc9//o2j99Eycr389ls9+/xd+7diVW6P18oW/OdI6/5PCf9dg2Ky3i4jhaUMXG6OLYnYG2uhkZ6CLYnYG2uhkZ6CLYnYG2uhkZ6CLYnYG2uhkZzCqXXR1MX7y5MlYWVmJI0eOxO7du2NhYaHwebOzs7G4uLj6qNfrpQ7bTzt27Ih9b7k1vvzHX4qIiK8++khcf8PuuGH37sEeLFET226KTW/6WGy65aOx6ZaPxtiWa+KK2nticvstv/Brxyt/I1YWvh+t1ivRWj4XKy+diIltey7DqftrvV1EDE8butgYXRSzM9BGJzsDXRSzM9BGJzsDXRSzM9BGJzsDXRSzM9BGJzuDUe1irHUZf49Ao9GISqUSZ84uxvT09OX6tuv2zNNPxyfuuTtefPFsTG+djgcf+mLcfMsv/gBcbtve+unL+v3OP/d4rCw+G3H+pxGTm2Js/IqYuvk31zyneeKrMbnj1pio7I6IiJUXn47zf/0XESvNiLGJiPHJuPJ174vxLVdHq/VKLJ9+Il5p/OzfdZi4el9MXr33sr2e1srL0fzeg7G4mM7nMOU2hqWLiMvbRm5dRKTXRspdRAxPG3ZGOal1EZF2G8PSRYSdUVZqbaTcRcTwtGFnlJNaFxFptzEsXUTYGWWl1kbKXUQMTxt2RjmpdRGRdhvD0kWEnVFWam2k3EXE8LRhZ5STWhcRabcxLF1E2BlldduGi/EhdLkXSW4sknxpo5zU2tBFb+iinNS6iNBGr2ijnNTa0EVv6KKc1LqI0EavaKOc1NrQRW/oopzUuojQRq9oo5zU2tBFb+iinNS6iNBGr2ijnG7b6OpXqQMAAAAAAADAsHAxDgAAAAAAAEDWXIwDAAAAAAAAkDUX4wAAAAAAAABkzcU4AAAAAAAAAFlzMQ4AAAAAAABA1lyMAwAAAAAAAJA1F+MAAAAAAAAAZM3FOAAAAAAAAABZczEOAAAAAAAAQNZcjAMAAAAAAACQNRfjAAAAAAAAAGTNxTgAAAAAAAAAWXMxDgAAAAAAAEDWXIwDAAAAAAAAkLWuLsYPHToUjUYjHnjggfja174W3/rWtwqf12w2o9ForHlArtbbRYQ2GC12BnSyM6CYnQGd7AwoZmdAJzsDitkZ0MnOYFR1dTG+d+/e2LJlS+zatSuq1WosLS0VPu/gwYNRqVRWH7VardRhIWXr7SJCG4wWOwM62RlQzM6ATnYGFLMzoJOdAcXsDOhkZzCquroYP3nyZBw9ejSOHTsWmzdvjvn5+cLnzc7OxuLi4uqjXq+XOiykbL1dRGiD0WJnQCc7A4rZGdDJzoBidgZ0sjOgmJ0BnewMRtVYq9VqXa5v1mg0olKpxJmzizE9PX25vm12tr3104M+wlBrrbwcze89GIuL6XwOtdEb2igntTZ00Ru6KCe1LiK00SvaKCe1NnTRG7ooJ7UuIrTRK9ooJ7U2dNEbuigntS4itNEr2igntTZ00Ru6KCe1LiK00SvaKKfbNrr6iXEAAAAAAAAAGBYuxgEAAAAAAADImotxAAAAAAAAALLmYhwAAAAAAACArLkYBwAAAAAAACBrLsYBAAAAAAAAyJqLcQAAAAAAAACy5mIcAAAAAAAAgKy5GAcAAAAAAAAgay7GAQAAAAAAAMiai3EAAAAAAAAAsuZiHAAAAAAAAICsuRgHAAAAAAAAIGsuxgEAAAAAAADImotxAAAAAAAAALLmYhwAAAAAAACArE1280WHDh2KO++8M5588sm48cYb4/Tp03HgwIGO5zWbzWg2m6t/XlxcjIiInzQaXR6XiIjWysuDPsJQa79/rVarp3PX20WENvpFG+UMug1d9Icuyhl0FxHa6BdtlDPoNnTRH7ooZ9BdRGijX7RRzqDb0EV/6KKcQXcRoY1+0UY5g25DF/2hi3IG3UWENvpFG+V020ZXF+N79+6NrVu3xvj4eJw7dy6q1Wrh8w4ePBj33Xdfx3/f87paN98Weurs2bNRqVR6Nm+9XURog7QNqg1dkDI7A4rZGdDJzoBidgZ0sjOgmJ0BnewMKLbRNrq6GD958mSMjY3F0tJSbNq0KU6fPl34vNnZ2bj33ntX//zjH/84brjhhjh16lRPAm40GlGr1aJer8f09HTpef2Ymfq8fsxMfd7i4mJcf/31cdVVV5WedbH1dhExfG343KU3rx8zB93GsHXRj5mpz+vHzNTnDbqLiOFrw+cuvXn9mDnoNoati37MTH1eP2amPm/QXUQMXxs+d+nN68fMQbcxbF30Y2bq8/oxM/V5g+4iYvja8LlLb14/Zg66jWHroh8zU5/Xj5mpzxt0FxHD14bPXXrz+jGz2za6uhj/2Mc+FhERMzMz/9/nTU1NxdTUVMd/r1QqPXsjIyKmp6d7Oq8fM1Of14+Zqc8bHx/v2ayI9XcRMbxt+NylN68fMwfVxrB20Y+Zqc/rx8zU59kZg5/Xj5mjNq8fM+2Mwc9MfV4/ZqY+z84Y/Lx+zBy1ef2YaWcMfmbq8/oxM/V5dsbg5/Vj5qjN68dMO2PwM1Of14+Zqc+zMwY/rx8zR21eP2ZutI3elgQAAAAAAAAAiXExDgAAAAAAAEDWLuvF+NTUVHz2s58t/JULKczrx8zU5/Vj5qjN64XUX6PPXXrz+jEztTaG4fWlfkavOb15vZD6a/S5S29eP2am1sYwvL7Uz+g1pzevF1J/jT536c3rx8zU2hiG15f6Gb3m9Ob1Quqv0ecuvXn9mJlaG8Pw+lI/o9ec3rxeSP01+tylN68fM7udN9ZqtVo9OQEAAAAAAAAAJMivUgcAAAAAAAAgay7GAQAAAAAAAMiai3EAAAAAAAAAsjZ5Ob7J0aNH4/Tp03HgwIF44IEH4r3vfW88+uijceDAgbj22mtLz/zmN78Z4+PjcfTo0bj33nu7+ofbDx8+HNddd10cO3Ysbr311jhz5kwcOHCgq7O15+3atSuOHz8etVottm/fHrfffnupebVaLZ555pm48cYbV197GYcPH46dO3fGE088Efv27Ytrrrmm1BkPHToU73//++ORRx7pyXt46NChuPvuu+MrX/lKT97D9sw777wznnzyyZ69j2X0uo3Uu2jPTLmNXncRkX4buXfx6pkptpF6F+2ZdkZebaTeRXtmym3YGfl18eqZKbaRehftmXZGXm2k3kV7Zspt2Bn5dfHqmSm2kXoX7Zl2Rl5tpN5Fe2bKbdgZ+XXx6pkptpF6F+2ZdkZebaTeRXtmym3YGRt/Dy/LT4yfO3cuqtVqRERcffXVcerUqdi/f3+cOHGiJzNvvvnmGB8fj0qlEufPn+9qXrVajcceeyz2798fc3Nzq7O7Va1WY3l5OXbt2hXVajWWlpZKz6vX6zE+Pr7mtZedubKyErfddlvs3Lmz9Bn37t0b8/PzPXsP9+7dG1u2bOnZe9ieuXXr1p6+j2X0uo3Uu2jPTLmNXncRkX4buXfx6pkptpF6F+2ZdkZebaTeRXtmym3YGfl18eqZKbaRehftmXZGXm2k3kV7Zspt2Bn5dfHqmSm2kXoX7Zl2Rl5tpN5Fe2bKbdgZ+XXx6pkptpF6F+2ZdkZebaTeRXtmym3YGRt/D/t+Mf6FL3whPve5z8XCwkLMz8/Ha17zmlhaWoojR47Enj17ejLzoYceih07dsTWrVvjhRde6GrmwsJCzMzMxJEjR2L37t2xsLDQ1ZyL501MTMSxY8di8+bNMT8/X3pe+0O9adOm0ue7+IxjY2Oxbdu20mc8efJkrKys9Ow9PHnyZBw9erRn72F75vHjx3v6Pnar120MQxftmSm30esuItJvI+cuimam2EbqXVx8RjtjMOyMNNuwM/Lqomhmim2k3sXFZ7QzBsPOSLMNOyOvLopmpthG6l1cfEY7YzDsjDTbsDPy6qJoZoptpN7FxWe0MwbDzkizDTtj4+/hWKvVapU6AQAAAAAAAAAk7LL8KnUAAAAAAAAAGBQX4wAAAAAAAABkzcU4AAAAAAAAAFlzMQ4AAAAAAABA1lyMAwAAAAAAAJA1F+MAAAAAAAAAZM3FOAAAAAAAAABZczEOAAAAAAAAQNZcjAMAAAAAAACQNRfjAAAAAAAAAGTt/wHg26i+rdlv+wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 2000x1200 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1200 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "confusion_matrices_train = [eval_metrics['confusion_matrix'] for eval_metrics in train_eval_metrics_list]\n",
    "confusion_matrices_test = [eval_metrics['confusion_matrix'] for eval_metrics in test_eval_metrics_list]\n",
    "\n",
    "n_matrices = len(confusion_matrices_train)\n",
    "n_cols = min(10, n_matrices)  # max 10 columns\n",
    "n_rows = (n_matrices + n_cols - 1) // n_cols  # ceiling division\n",
    "\n",
    "plt.figure(figsize=(20, 4*n_rows))\n",
    "for i, conf_matrix in enumerate(confusion_matrices_train):\n",
    "    plt.subplot(n_rows, n_cols, i+1)\n",
    "    plt.imshow(conf_matrix, cmap='Blues')\n",
    "    plt.title(f'Epoch {i+1}', fontsize=2)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=2)\n",
    "    # Add numbers to the cells\n",
    "    for x in range(conf_matrix.shape[0]):\n",
    "        for y in range(conf_matrix.shape[1]):\n",
    "            plt.text(y, x, str(conf_matrix[x, y]),\n",
    "                    ha='center', va='center', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 4*n_rows))\n",
    "for i, conf_matrix in enumerate(confusion_matrices_test):\n",
    "    plt.subplot(n_rows, n_cols, i+1)\n",
    "    plt.imshow(conf_matrix, cmap='Blues')\n",
    "    plt.title(f'Epoch {i+1}', fontsize=2)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=2)\n",
    "    # Add numbers to the cells\n",
    "    for x in range(conf_matrix.shape[0]):\n",
    "        for y in range(conf_matrix.shape[1]):\n",
    "            plt.text(y, x, str(conf_matrix[x, y]),\n",
    "                    ha='center', va='center', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
