{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "from encoders import *\n",
    "from data import *\n",
    "from models import *\n",
    "from evals import *\n",
    "print(\"using device: \", device)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for debugging\n",
    "# import importlib\n",
    "# import models\n",
    "# importlib.reload(models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ugly Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ugly engineering here\n",
    "df = pd.read_csv('../../data/HR_events.csv')\n",
    "df_y = pd.read_excel('../../data/PAS Challenge Outcome Data.xlsx', engine=\"calamine\")\n",
    "df_y = df_y[['VitalID', 'Died']]\n",
    "df = df.merge(df_y, on='VitalID', how='left')\n",
    "df['label'] = df.index.to_series()\n",
    "df['text'] = df['Died'].apply(lambda x: 'This infant will die in 7 days. ' if x == 1 else 'This infant will survive. ')\n",
    "# df['text'] = df['text'] +' '+ df['event_description'].astype(str)\n",
    "\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, stratify=df['text'])    \n",
    "# df_train['text'] = df_train['text'] +' '+ df_train['event_description'].astype(str)\n",
    "# df_test['text'] = df_test['text'] +' '+ df_test['event_description'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "df_new = df_train\n",
    "df_new['outcome'] = df_new['Died'].apply(lambda x: 'class1' if x == 1 else 'class2')\n",
    "df_new_y = pd.get_dummies(df_new['outcome'])\n",
    "y_true_train = torch.tensor(df_new_y.values)\n",
    "ts_df_train = df_new.loc[:,'1':'300']\n",
    "\n",
    "\n",
    "df_new = df_test\n",
    "df_new['outcome'] = df_new['Died'].apply(lambda x: 'class1' if x == 1 else 'class2')\n",
    "df_new_y = pd.get_dummies(df_new['outcome'])\n",
    "y_true_test = torch.tensor(df_new_y.values)\n",
    "ts_df_test = df_new.loc[:,'1':'300']\n",
    "\n",
    "txt_ls = ['die', 'survive']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All required objects are properly defined and non-empty\n"
     ]
    }
   ],
   "source": [
    "# ---- config that need to be changed ----\n",
    "overwrite = True\n",
    "model_name = 'clip_hr_death_binary'\n",
    "init_lr = 0.01 # initial learning rate\n",
    "loss_type = 'similarity'\n",
    "\n",
    "\n",
    "def check_data_ready():\n",
    "    required_objects = {\n",
    "        'y_true_train': (pd.DataFrame, np.ndarray, torch.Tensor),\n",
    "        'ts_df_train': pd.DataFrame,\n",
    "        'y_true_test': (pd.DataFrame, np.ndarray, torch.Tensor),\n",
    "        'ts_df_test': pd.DataFrame,\n",
    "        'txt_ls': list\n",
    "    }\n",
    "    \n",
    "    for obj_name, expected_type in required_objects.items():\n",
    "        # Check if object exists in globals\n",
    "        if obj_name not in globals():\n",
    "            raise ValueError(f\"Missing required object: {obj_name}\")\n",
    "        \n",
    "        obj = globals()[obj_name]\n",
    "        # Check if object is None\n",
    "        if obj is None:\n",
    "            raise ValueError(f\"Object {obj_name} is None\")\n",
    "            \n",
    "        # Check type\n",
    "        if not isinstance(obj, expected_type):\n",
    "            raise TypeError(f\"{obj_name} should be of type {expected_type}, but got {type(obj)}\")\n",
    "        \n",
    "        # Check if empty\n",
    "        if hasattr(obj, '__len__') and len(obj) == 0:\n",
    "            raise ValueError(f\"{obj_name} is empty\")\n",
    "\n",
    "    print(\"✓ All required objects are properly defined and non-empty\")\n",
    "    return True\n",
    "try:\n",
    "    check_data_ready()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please define all required objects before proceeding\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "CLIPModel                                1\n",
      "├─Sequential: 1-1                        --\n",
      "│    └─Linear: 2-1                       4,224\n",
      "│    └─LeakyReLU: 2-2                    --\n",
      "│    └─Linear: 2-3                       33,024\n",
      "│    └─LeakyReLU: 2-4                    --\n",
      "│    └─Linear: 2-5                       32,896\n",
      "│    └─LeakyReLU: 2-6                    --\n",
      "│    └─Linear: 2-7                       16,512\n",
      "├─Sequential: 1-2                        --\n",
      "│    └─Linear: 2-8                       98,432\n",
      "│    └─LeakyReLU: 2-9                    --\n",
      "│    └─Linear: 2-10                      33,024\n",
      "│    └─LeakyReLU: 2-11                   --\n",
      "│    └─Linear: 2-12                      65,792\n",
      "│    └─LeakyReLU: 2-13                   --\n",
      "│    └─Linear: 2-14                      32,896\n",
      "│    └─LeakyReLU: 2-15                   --\n",
      "│    └─Linear: 2-16                      16,512\n",
      "=================================================================\n",
      "Total params: 333,313\n",
      "Trainable params: 333,313\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- ready dataloader ---- \n",
    "ts_f_train, tx_f_train, labels_train = get_features(df_train,ts_encoder_name,text_encoder_name)\n",
    "train_dataloader = CLIPDataset(ts_f_train, tx_f_train, labels_train).dataloader(batch_size=batch_size)\n",
    "ts_f_test, tx_f_test, labels_test = get_features(df_test, ts_encoder_name,text_encoder_name)\n",
    "test_dataloader = CLIPDataset(ts_f_test, tx_f_test, labels_test).dataloader(batch_size=batch_size)\n",
    "\n",
    "# ---- ready model ---- \n",
    "model_path = './results/'+model_name+'_'+loss_type+'.pth' \n",
    "eval_path = './results/'+model_name+'_'+loss_type+'_evals.pth'\n",
    "# Initialize model\n",
    "model = CLIPModel(\n",
    "        ts_dim=ts_f_train.shape[1],    # 32\n",
    "        text_dim=tx_f_train.shape[1],  # 768\n",
    "        projection_dim=embedded_dim\n",
    "    )\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=init_lr)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',           # Reduce LR when metric stops decreasing\n",
    "    factor=0.9,          # Multiply LR by this factor\n",
    "    patience=patience,          # Number of epochs to wait before reducing LR\n",
    "    min_lr=1e-20         # Don't reduce LR below this value\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "\tTraining Loss: 538.832481\n",
      "\tTesting Loss: 538.653737\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 540.031876\n",
      "\tTesting Loss: 539.848409\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 540.093658\n",
      "\tTesting Loss: 539.565938\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 540.074422\n",
      "\tTesting Loss: 539.671987\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 539.936656\n",
      "\tTesting Loss: 539.429006\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 539.820165\n",
      "\tTesting Loss: 539.565308\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 539.601176\n",
      "\tTesting Loss: 539.388102\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 539.724589\n",
      "\tTesting Loss: 539.413167\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 539.740415\n",
      "\tTesting Loss: 539.443787\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 539.670659\n",
      "\tTesting Loss: 539.400655\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 539.696002\n",
      "\tTesting Loss: 539.436849\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 539.699674\n",
      "\tTesting Loss: 539.382161\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 539.629677\n",
      "\tTesting Loss: 539.397532\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 539.668922\n",
      "\tTesting Loss: 539.473826\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 539.686442\n",
      "\tTesting Loss: 539.371195\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 539.697367\n",
      "\tTesting Loss: 539.416951\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 539.703852\n",
      "\tTesting Loss: 539.497884\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 539.716672\n",
      "\tTesting Loss: 539.488973\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 539.694600\n",
      "\tTesting Loss: 539.459900\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 539.731745\n",
      "\tTesting Loss: 539.423726\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 539.722931\n",
      "\tTesting Loss: 539.522034\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 539.739349\n",
      "\tTesting Loss: 539.459696\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 539.726430\n",
      "\tTesting Loss: 539.439290\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 539.730385\n",
      "\tTesting Loss: 539.535604\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 539.745654\n",
      "\tTesting Loss: 539.479685\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 539.754578\n",
      "\tTesting Loss: 539.511292\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 539.746941\n",
      "\tTesting Loss: 539.535665\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 539.776489\n",
      "\tTesting Loss: 539.502828\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 539.747772\n",
      "\tTesting Loss: 539.582499\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 539.766546\n",
      "\tTesting Loss: 539.500397\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 539.777962\n",
      "\tTesting Loss: 539.570089\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 539.785563\n",
      "\tTesting Loss: 539.496002\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 539.780238\n",
      "\tTesting Loss: 539.608114\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 539.832092\n",
      "\tTesting Loss: 539.535828\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 539.824041\n",
      "\tTesting Loss: 539.566793\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 539.849197\n",
      "\tTesting Loss: 539.526143\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 539.821045\n",
      "\tTesting Loss: 539.617086\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 539.809458\n",
      "\tTesting Loss: 539.621236\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 539.832273\n",
      "\tTesting Loss: 539.619375\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 539.868561\n",
      "\tTesting Loss: 539.587138\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 539.845482\n",
      "\tTesting Loss: 539.549296\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 539.859032\n",
      "\tTesting Loss: 539.555003\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 539.878131\n",
      "\tTesting Loss: 539.558024\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 539.859764\n",
      "\tTesting Loss: 539.599955\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 539.892395\n",
      "\tTesting Loss: 539.559163\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 539.860519\n",
      "\tTesting Loss: 539.578105\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 539.897387\n",
      "\tTesting Loss: 539.628276\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 539.881536\n",
      "\tTesting Loss: 539.587240\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 539.814140\n",
      "\tTesting Loss: 539.640096\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 539.934484\n",
      "\tTesting Loss: 539.671794\n",
      "\tLearning Rate: 0.010000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "\tTraining Loss: 539.882261\n",
      "\tTesting Loss: 539.634867\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 539.955709\n",
      "\tTesting Loss: 539.675863\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 539.915665\n",
      "\tTesting Loss: 539.717834\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 539.970446\n",
      "\tTesting Loss: 539.691976\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 539.953089\n",
      "\tTesting Loss: 539.712646\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 539.976140\n",
      "\tTesting Loss: 539.680440\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 539.963562\n",
      "\tTesting Loss: 539.672729\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 540.005211\n",
      "\tTesting Loss: 539.675171\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 539.962519\n",
      "\tTesting Loss: 539.709788\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 539.991941\n",
      "\tTesting Loss: 539.640442\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 539.973757\n",
      "\tTesting Loss: 539.687174\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 539.999227\n",
      "\tTesting Loss: 539.688222\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 539.987874\n",
      "\tTesting Loss: 539.722168\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 539.987340\n",
      "\tTesting Loss: 539.699280\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 539.995356\n",
      "\tTesting Loss: 539.656464\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 539.989024\n",
      "\tTesting Loss: 539.709839\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 540.002233\n",
      "\tTesting Loss: 539.725179\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 540.006963\n",
      "\tTesting Loss: 539.729757\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 540.006055\n",
      "\tTesting Loss: 539.679382\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 540.001152\n",
      "\tTesting Loss: 539.712189\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 540.012792\n",
      "\tTesting Loss: 539.671885\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 539.988093\n",
      "\tTesting Loss: 539.705526\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 540.014318\n",
      "\tTesting Loss: 539.735982\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 540.019831\n",
      "\tTesting Loss: 539.747253\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 540.026006\n",
      "\tTesting Loss: 539.715597\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 540.001940\n",
      "\tTesting Loss: 539.717611\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 540.027059\n",
      "\tTesting Loss: 539.685852\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 540.038834\n",
      "\tTesting Loss: 539.715495\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 540.034398\n",
      "\tTesting Loss: 539.715302\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 540.053357\n",
      "\tTesting Loss: 539.694071\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 540.026067\n",
      "\tTesting Loss: 539.709819\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 540.034968\n",
      "\tTesting Loss: 539.731222\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 540.038704\n",
      "\tTesting Loss: 539.720093\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 540.033595\n",
      "\tTesting Loss: 539.752177\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 540.044375\n",
      "\tTesting Loss: 539.735118\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 540.063095\n",
      "\tTesting Loss: 539.733887\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 540.048625\n",
      "\tTesting Loss: 539.738953\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 540.051730\n",
      "\tTesting Loss: 539.705027\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 540.056267\n",
      "\tTesting Loss: 539.699382\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 540.045275\n",
      "\tTesting Loss: 539.709798\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 540.070946\n",
      "\tTesting Loss: 539.722880\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 540.063433\n",
      "\tTesting Loss: 539.746226\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 540.064491\n",
      "\tTesting Loss: 539.723938\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 540.074336\n",
      "\tTesting Loss: 539.745056\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 540.052170\n",
      "\tTesting Loss: 539.720917\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 540.077187\n",
      "\tTesting Loss: 539.721415\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 540.070353\n",
      "\tTesting Loss: 539.762573\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 540.084198\n",
      "\tTesting Loss: 539.722117\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 540.075340\n",
      "\tTesting Loss: 539.771210\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 540.078779\n",
      "\tTesting Loss: 539.748759\n",
      "\tLearning Rate: 0.010000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "\tTraining Loss: 540.077449\n",
      "\tTesting Loss: 539.763814\n",
      "\tLearning Rate: 0.010000000\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 540.083206\n",
      "\tTesting Loss: 539.750305\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 540.088994\n",
      "\tTesting Loss: 539.746338\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 540.095487\n",
      "\tTesting Loss: 539.772227\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 540.098770\n",
      "\tTesting Loss: 539.726176\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 540.087331\n",
      "\tTesting Loss: 539.769531\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 540.103622\n",
      "\tTesting Loss: 539.772176\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 540.107666\n",
      "\tTesting Loss: 539.758413\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 540.098686\n",
      "\tTesting Loss: 539.734070\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 540.088702\n",
      "\tTesting Loss: 539.742981\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 540.096868\n",
      "\tTesting Loss: 539.755656\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 540.091456\n",
      "\tTesting Loss: 539.759115\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 540.099564\n",
      "\tTesting Loss: 539.757497\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 540.103689\n",
      "\tTesting Loss: 539.761983\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 540.112928\n",
      "\tTesting Loss: 539.761393\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 540.107162\n",
      "\tTesting Loss: 539.780782\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 540.113485\n",
      "\tTesting Loss: 539.749471\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 540.110041\n",
      "\tTesting Loss: 539.772949\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 540.099803\n",
      "\tTesting Loss: 539.782796\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 540.113614\n",
      "\tTesting Loss: 539.787272\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 540.109721\n",
      "\tTesting Loss: 539.786031\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 540.116102\n",
      "\tTesting Loss: 539.770264\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 540.123823\n",
      "\tTesting Loss: 539.748322\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 540.113678\n",
      "\tTesting Loss: 539.769480\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 540.117538\n",
      "\tTesting Loss: 539.760925\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 540.109787\n",
      "\tTesting Loss: 539.753255\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 540.118963\n",
      "\tTesting Loss: 539.761454\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 540.120361\n",
      "\tTesting Loss: 539.760498\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 540.113825\n",
      "\tTesting Loss: 539.780894\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 540.122213\n",
      "\tTesting Loss: 539.789398\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 540.134715\n",
      "\tTesting Loss: 539.762990\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 540.125254\n",
      "\tTesting Loss: 539.795624\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 540.128006\n",
      "\tTesting Loss: 539.774790\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 540.118927\n",
      "\tTesting Loss: 539.796204\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 540.138601\n",
      "\tTesting Loss: 539.765462\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 540.128520\n",
      "\tTesting Loss: 539.763733\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 540.121623\n",
      "\tTesting Loss: 539.772746\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 540.131851\n",
      "\tTesting Loss: 539.772441\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 540.140127\n",
      "\tTesting Loss: 539.770040\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 540.141675\n",
      "\tTesting Loss: 539.775665\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 540.143148\n",
      "\tTesting Loss: 539.773865\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 540.134951\n",
      "\tTesting Loss: 539.795410\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 540.136096\n",
      "\tTesting Loss: 539.776835\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 540.138578\n",
      "\tTesting Loss: 539.771627\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 540.143374\n",
      "\tTesting Loss: 539.796651\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 540.139206\n",
      "\tTesting Loss: 539.779195\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 540.137477\n",
      "\tTesting Loss: 539.773600\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 540.147232\n",
      "\tTesting Loss: 539.805237\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 540.147212\n",
      "\tTesting Loss: 539.780924\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 540.150716\n",
      "\tTesting Loss: 539.807383\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 540.151672\n",
      "\tTesting Loss: 539.779185\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 540.144877\n",
      "\tTesting Loss: 539.779215\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 540.145999\n",
      "\tTesting Loss: 539.780406\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 540.155350\n",
      "\tTesting Loss: 539.798442\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 540.152613\n",
      "\tTesting Loss: 539.801514\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 540.144875\n",
      "\tTesting Loss: 539.799296\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 540.154226\n",
      "\tTesting Loss: 539.803253\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 540.151443\n",
      "\tTesting Loss: 539.797506\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 540.149610\n",
      "\tTesting Loss: 539.785848\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 540.145109\n",
      "\tTesting Loss: 539.789296\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 540.157494\n",
      "\tTesting Loss: 539.803935\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 540.152631\n",
      "\tTesting Loss: 539.789815\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 540.160517\n",
      "\tTesting Loss: 539.789347\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 540.154839\n",
      "\tTesting Loss: 539.792643\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 540.153646\n",
      "\tTesting Loss: 539.793925\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 540.155337\n",
      "\tTesting Loss: 539.793406\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 540.159571\n",
      "\tTesting Loss: 539.817403\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 540.160029\n",
      "\tTesting Loss: 539.803640\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 540.154472\n",
      "\tTesting Loss: 539.794088\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 540.161649\n",
      "\tTesting Loss: 539.794769\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 540.162865\n",
      "\tTesting Loss: 539.795217\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 540.162422\n",
      "\tTesting Loss: 539.815877\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 540.168365\n",
      "\tTesting Loss: 539.796529\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 540.167791\n",
      "\tTesting Loss: 539.816223\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 540.168170\n",
      "\tTesting Loss: 539.798360\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 540.161903\n",
      "\tTesting Loss: 539.802572\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 540.163152\n",
      "\tTesting Loss: 539.812764\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 540.179385\n",
      "\tTesting Loss: 539.802775\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 540.188705\n",
      "\tTesting Loss: 539.797099\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 540.176949\n",
      "\tTesting Loss: 539.818390\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 540.162043\n",
      "\tTesting Loss: 539.823853\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 540.189237\n",
      "\tTesting Loss: 539.794942\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 540.166552\n",
      "\tTesting Loss: 539.813314\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 540.167437\n",
      "\tTesting Loss: 539.801676\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 540.172244\n",
      "\tTesting Loss: 539.813171\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 540.171789\n",
      "\tTesting Loss: 539.815104\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 540.171931\n",
      "\tTesting Loss: 539.807190\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 540.172602\n",
      "\tTesting Loss: 539.822642\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 540.174784\n",
      "\tTesting Loss: 539.817851\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 540.172068\n",
      "\tTesting Loss: 539.806112\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 540.173798\n",
      "\tTesting Loss: 539.807617\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 540.172226\n",
      "\tTesting Loss: 539.819580\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 540.174873\n",
      "\tTesting Loss: 539.809855\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 540.173910\n",
      "\tTesting Loss: 539.809947\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 540.172203\n",
      "\tTesting Loss: 539.819458\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 540.174171\n",
      "\tTesting Loss: 539.821859\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 540.180257\n",
      "\tTesting Loss: 539.812866\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 540.177775\n",
      "\tTesting Loss: 539.811707\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 540.175799\n",
      "\tTesting Loss: 539.828044\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 540.180316\n",
      "\tTesting Loss: 539.827535\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 540.178279\n",
      "\tTesting Loss: 539.814189\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 540.180003\n",
      "\tTesting Loss: 539.815267\n",
      "\tLearning Rate: 0.009000000\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 540.181320\n",
      "\tTesting Loss: 539.829834\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 540.184301\n",
      "\tTesting Loss: 539.808024\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 540.178218\n",
      "\tTesting Loss: 539.810577\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 540.180672\n",
      "\tTesting Loss: 539.818827\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 540.186717\n",
      "\tTesting Loss: 539.812541\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 540.180143\n",
      "\tTesting Loss: 539.813202\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 540.182139\n",
      "\tTesting Loss: 539.826009\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 540.175522\n",
      "\tTesting Loss: 539.819784\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 540.181259\n",
      "\tTesting Loss: 539.832682\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 540.181447\n",
      "\tTesting Loss: 539.833232\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 540.182454\n",
      "\tTesting Loss: 539.812805\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 540.186790\n",
      "\tTesting Loss: 539.802887\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 540.200150\n",
      "\tTesting Loss: 539.801086\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 540.205093\n",
      "\tTesting Loss: 539.808645\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 540.218099\n",
      "\tTesting Loss: 539.839498\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 540.188627\n",
      "\tTesting Loss: 539.841410\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 540.195953\n",
      "\tTesting Loss: 539.823995\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 540.195580\n",
      "\tTesting Loss: 539.814830\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 540.182348\n",
      "\tTesting Loss: 539.828451\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 540.181964\n",
      "\tTesting Loss: 539.836975\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 540.189519\n",
      "\tTesting Loss: 539.820719\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 540.191368\n",
      "\tTesting Loss: 539.829234\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 540.184652\n",
      "\tTesting Loss: 539.822571\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 540.184949\n",
      "\tTesting Loss: 539.818827\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 540.189102\n",
      "\tTesting Loss: 539.818970\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 540.184647\n",
      "\tTesting Loss: 539.817546\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 540.184461\n",
      "\tTesting Loss: 539.832102\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 540.185921\n",
      "\tTesting Loss: 539.832784\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 540.187701\n",
      "\tTesting Loss: 539.834025\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 540.186386\n",
      "\tTesting Loss: 539.833150\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 540.191663\n",
      "\tTesting Loss: 539.821635\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 540.185534\n",
      "\tTesting Loss: 539.821940\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 540.188965\n",
      "\tTesting Loss: 539.824402\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 540.185331\n",
      "\tTesting Loss: 539.824239\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 540.195999\n",
      "\tTesting Loss: 539.821879\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 540.185404\n",
      "\tTesting Loss: 539.829386\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 540.193245\n",
      "\tTesting Loss: 539.830160\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 540.187836\n",
      "\tTesting Loss: 539.831563\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 540.182899\n",
      "\tTesting Loss: 539.845713\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 540.185193\n",
      "\tTesting Loss: 539.838175\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 540.176407\n",
      "\tTesting Loss: 539.842631\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 540.179128\n",
      "\tTesting Loss: 539.849996\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 540.218913\n",
      "\tTesting Loss: 539.806895\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 540.237478\n",
      "\tTesting Loss: 539.803212\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 540.222796\n",
      "\tTesting Loss: 539.824666\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 540.192505\n",
      "\tTesting Loss: 539.837382\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 540.201576\n",
      "\tTesting Loss: 539.818634\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 540.196251\n",
      "\tTesting Loss: 539.828247\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 540.196396\n",
      "\tTesting Loss: 539.833232\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 540.196017\n",
      "\tTesting Loss: 539.838287\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 540.191544\n",
      "\tTesting Loss: 539.838135\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 540.190239\n",
      "\tTesting Loss: 539.829559\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 540.193377\n",
      "\tTesting Loss: 539.832377\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 540.189613\n",
      "\tTesting Loss: 539.837179\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 540.189891\n",
      "\tTesting Loss: 539.835490\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 540.189377\n",
      "\tTesting Loss: 539.844401\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 540.191264\n",
      "\tTesting Loss: 539.836609\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 540.187538\n",
      "\tTesting Loss: 539.838867\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 540.186328\n",
      "\tTesting Loss: 539.839986\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 540.189112\n",
      "\tTesting Loss: 539.843404\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 540.200829\n",
      "\tTesting Loss: 539.823201\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 540.214544\n",
      "\tTesting Loss: 539.796773\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 540.232396\n",
      "\tTesting Loss: 539.789429\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 540.238243\n",
      "\tTesting Loss: 539.842367\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 540.212051\n",
      "\tTesting Loss: 539.841970\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 540.197314\n",
      "\tTesting Loss: 539.835022\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 540.199046\n",
      "\tTesting Loss: 539.839986\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 540.198308\n",
      "\tTesting Loss: 539.832540\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 540.197917\n",
      "\tTesting Loss: 539.836222\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 540.198273\n",
      "\tTesting Loss: 539.837036\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 540.196409\n",
      "\tTesting Loss: 539.844442\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 540.196322\n",
      "\tTesting Loss: 539.848633\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 540.191625\n",
      "\tTesting Loss: 539.851959\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 540.191930\n",
      "\tTesting Loss: 539.849711\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 540.189568\n",
      "\tTesting Loss: 539.845744\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 540.185582\n",
      "\tTesting Loss: 539.848979\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 540.185496\n",
      "\tTesting Loss: 539.854146\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 540.202527\n",
      "\tTesting Loss: 539.838470\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 540.227509\n",
      "\tTesting Loss: 539.800649\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 540.231130\n",
      "\tTesting Loss: 539.808085\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 540.223595\n",
      "\tTesting Loss: 539.811646\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 540.225474\n",
      "\tTesting Loss: 539.816060\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 540.214223\n",
      "\tTesting Loss: 539.824219\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 540.214091\n",
      "\tTesting Loss: 539.835144\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 540.209544\n",
      "\tTesting Loss: 539.848714\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 540.195862\n",
      "\tTesting Loss: 539.860494\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 540.197482\n",
      "\tTesting Loss: 539.855489\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 540.201312\n",
      "\tTesting Loss: 539.837799\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 540.200559\n",
      "\tTesting Loss: 539.835510\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 540.193881\n",
      "\tTesting Loss: 539.837748\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 540.194875\n",
      "\tTesting Loss: 539.841848\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 540.191139\n",
      "\tTesting Loss: 539.842082\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 540.191310\n",
      "\tTesting Loss: 539.843781\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 540.188950\n",
      "\tTesting Loss: 539.840820\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 540.191907\n",
      "\tTesting Loss: 539.841970\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 540.192134\n",
      "\tTesting Loss: 539.835775\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 540.195414\n",
      "\tTesting Loss: 539.835897\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 540.193863\n",
      "\tTesting Loss: 539.840739\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 540.208405\n",
      "\tTesting Loss: 539.833659\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 540.179962\n",
      "\tTesting Loss: 539.855225\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 540.231008\n",
      "\tTesting Loss: 539.855520\n",
      "\tLearning Rate: 0.008100000\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 540.243576\n",
      "\tTesting Loss: 539.833496\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 540.205490\n",
      "\tTesting Loss: 539.830912\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 540.204425\n",
      "\tTesting Loss: 539.858378\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 540.199905\n",
      "\tTesting Loss: 539.872772\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 540.223251\n",
      "\tTesting Loss: 539.829061\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 540.204102\n",
      "\tTesting Loss: 539.844035\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 540.181826\n",
      "\tTesting Loss: 539.855438\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 540.182398\n",
      "\tTesting Loss: 539.854787\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 540.218183\n",
      "\tTesting Loss: 539.831248\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 540.202265\n",
      "\tTesting Loss: 539.830241\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 540.179352\n",
      "\tTesting Loss: 539.855082\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 540.183222\n",
      "\tTesting Loss: 539.855632\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 540.186569\n",
      "\tTesting Loss: 539.858693\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 540.189766\n",
      "\tTesting Loss: 539.859548\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 540.215851\n",
      "\tTesting Loss: 539.820221\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 540.206355\n",
      "\tTesting Loss: 539.818583\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 540.179303\n",
      "\tTesting Loss: 539.841248\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 540.184779\n",
      "\tTesting Loss: 539.849314\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 540.176371\n",
      "\tTesting Loss: 539.845540\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 540.187429\n",
      "\tTesting Loss: 539.819316\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 540.180662\n",
      "\tTesting Loss: 539.858622\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 540.224281\n",
      "\tTesting Loss: 539.860474\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 540.204783\n",
      "\tTesting Loss: 539.847076\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 540.249311\n",
      "\tTesting Loss: 539.812734\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 540.176137\n",
      "\tTesting Loss: 539.839274\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 540.186719\n",
      "\tTesting Loss: 539.837423\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 540.198781\n",
      "\tTesting Loss: 539.819631\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 540.211080\n",
      "\tTesting Loss: 539.804545\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 540.197901\n",
      "\tTesting Loss: 539.822306\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 540.194232\n",
      "\tTesting Loss: 539.876241\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 540.280792\n",
      "\tTesting Loss: 539.806620\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 540.207105\n",
      "\tTesting Loss: 539.870260\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 540.191956\n",
      "\tTesting Loss: 539.870351\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 540.197055\n",
      "\tTesting Loss: 539.864482\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 540.198504\n",
      "\tTesting Loss: 539.851939\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 540.198367\n",
      "\tTesting Loss: 539.850505\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 540.182149\n",
      "\tTesting Loss: 539.856008\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 540.173836\n",
      "\tTesting Loss: 539.868347\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 540.183861\n",
      "\tTesting Loss: 539.859741\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 540.202016\n",
      "\tTesting Loss: 539.838755\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 540.183777\n",
      "\tTesting Loss: 539.853577\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 540.178172\n",
      "\tTesting Loss: 539.835378\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 540.209096\n",
      "\tTesting Loss: 539.852437\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 540.192719\n",
      "\tTesting Loss: 539.864034\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 540.178571\n",
      "\tTesting Loss: 539.872803\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 540.167987\n",
      "\tTesting Loss: 539.880727\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 540.304789\n",
      "\tTesting Loss: 539.835225\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 540.188204\n",
      "\tTesting Loss: 539.860636\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 540.173932\n",
      "\tTesting Loss: 539.859192\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 540.168973\n",
      "\tTesting Loss: 539.858500\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 540.239100\n",
      "\tTesting Loss: 539.794210\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 540.184931\n",
      "\tTesting Loss: 539.817017\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 540.203743\n",
      "\tTesting Loss: 539.886698\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 540.342130\n",
      "\tTesting Loss: 539.883870\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 540.280195\n",
      "\tTesting Loss: 539.809977\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 540.227992\n",
      "\tTesting Loss: 539.851278\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 540.255763\n",
      "\tTesting Loss: 539.831136\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 540.217926\n",
      "\tTesting Loss: 539.845723\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 540.225288\n",
      "\tTesting Loss: 539.834035\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 540.204147\n",
      "\tTesting Loss: 539.843038\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 540.205030\n",
      "\tTesting Loss: 539.839935\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 540.214437\n",
      "\tTesting Loss: 539.834757\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 540.203677\n",
      "\tTesting Loss: 539.842306\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 540.201090\n",
      "\tTesting Loss: 539.851247\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 540.191821\n",
      "\tTesting Loss: 539.859029\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 540.192515\n",
      "\tTesting Loss: 539.854645\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 540.193932\n",
      "\tTesting Loss: 539.855326\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 540.193441\n",
      "\tTesting Loss: 539.847829\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 540.183075\n",
      "\tTesting Loss: 539.861155\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 540.177711\n",
      "\tTesting Loss: 539.866882\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 540.176127\n",
      "\tTesting Loss: 539.865967\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 540.173983\n",
      "\tTesting Loss: 539.868256\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 540.226824\n",
      "\tTesting Loss: 539.805705\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 540.203644\n",
      "\tTesting Loss: 539.814270\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 540.221914\n",
      "\tTesting Loss: 539.903280\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 540.361382\n",
      "\tTesting Loss: 539.900004\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 540.367182\n",
      "\tTesting Loss: 539.897909\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 540.357455\n",
      "\tTesting Loss: 539.890849\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 540.306300\n",
      "\tTesting Loss: 539.834351\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 540.261243\n",
      "\tTesting Loss: 539.861003\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 540.248482\n",
      "\tTesting Loss: 539.840403\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 540.215861\n",
      "\tTesting Loss: 539.849915\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 540.234769\n",
      "\tTesting Loss: 539.843547\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 540.230990\n",
      "\tTesting Loss: 539.849121\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 540.233854\n",
      "\tTesting Loss: 539.846415\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 540.227432\n",
      "\tTesting Loss: 539.849589\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 540.230151\n",
      "\tTesting Loss: 539.844248\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 540.228376\n",
      "\tTesting Loss: 539.844503\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 540.229670\n",
      "\tTesting Loss: 539.845266\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 540.228175\n",
      "\tTesting Loss: 539.847982\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 540.228282\n",
      "\tTesting Loss: 539.846578\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 540.228124\n",
      "\tTesting Loss: 539.845815\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 540.227056\n",
      "\tTesting Loss: 539.844798\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 540.226606\n",
      "\tTesting Loss: 539.844655\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 540.226227\n",
      "\tTesting Loss: 539.844604\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 540.226781\n",
      "\tTesting Loss: 539.843282\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 540.224495\n",
      "\tTesting Loss: 539.848063\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 540.227203\n",
      "\tTesting Loss: 539.846395\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 540.225423\n",
      "\tTesting Loss: 539.846822\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 540.226992\n",
      "\tTesting Loss: 539.845601\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 540.225253\n",
      "\tTesting Loss: 539.844076\n",
      "\tLearning Rate: 0.007290000\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 540.224691\n",
      "\tTesting Loss: 539.844615\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 540.224838\n",
      "\tTesting Loss: 539.844747\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 540.221232\n",
      "\tTesting Loss: 539.844971\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 540.221260\n",
      "\tTesting Loss: 539.843953\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 540.222443\n",
      "\tTesting Loss: 539.841583\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 540.222481\n",
      "\tTesting Loss: 539.845896\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 540.219922\n",
      "\tTesting Loss: 539.846720\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 540.219447\n",
      "\tTesting Loss: 539.844727\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 540.218597\n",
      "\tTesting Loss: 539.845662\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 540.215149\n",
      "\tTesting Loss: 539.842061\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 540.215686\n",
      "\tTesting Loss: 539.842773\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 540.212204\n",
      "\tTesting Loss: 539.844889\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 540.209061\n",
      "\tTesting Loss: 539.846202\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 540.206551\n",
      "\tTesting Loss: 539.849548\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 540.204025\n",
      "\tTesting Loss: 539.854024\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 540.200531\n",
      "\tTesting Loss: 539.857035\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 540.200254\n",
      "\tTesting Loss: 539.864380\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 540.198318\n",
      "\tTesting Loss: 539.864176\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 540.196882\n",
      "\tTesting Loss: 539.863241\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 540.196813\n",
      "\tTesting Loss: 539.860799\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 540.206482\n",
      "\tTesting Loss: 539.841939\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 540.210864\n",
      "\tTesting Loss: 539.833659\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 540.239446\n",
      "\tTesting Loss: 539.895976\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 540.347198\n",
      "\tTesting Loss: 539.897746\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 540.347341\n",
      "\tTesting Loss: 539.885722\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 540.292885\n",
      "\tTesting Loss: 539.847870\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 540.242844\n",
      "\tTesting Loss: 539.842061\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 540.268982\n",
      "\tTesting Loss: 539.858175\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 540.260478\n",
      "\tTesting Loss: 539.842651\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 540.248316\n",
      "\tTesting Loss: 539.849325\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 540.247472\n",
      "\tTesting Loss: 539.843404\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 540.239594\n",
      "\tTesting Loss: 539.847432\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 540.237470\n",
      "\tTesting Loss: 539.844889\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 540.231873\n",
      "\tTesting Loss: 539.847982\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 540.225024\n",
      "\tTesting Loss: 539.840719\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 540.222687\n",
      "\tTesting Loss: 539.848806\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 540.219948\n",
      "\tTesting Loss: 539.839579\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 540.220238\n",
      "\tTesting Loss: 539.851664\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 540.220749\n",
      "\tTesting Loss: 539.842346\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 540.219899\n",
      "\tTesting Loss: 539.848979\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 540.218043\n",
      "\tTesting Loss: 539.839945\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 540.217504\n",
      "\tTesting Loss: 539.847839\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 540.214236\n",
      "\tTesting Loss: 539.843058\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 540.214940\n",
      "\tTesting Loss: 539.848368\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 540.211110\n",
      "\tTesting Loss: 539.843160\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 540.210882\n",
      "\tTesting Loss: 539.848958\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 540.206146\n",
      "\tTesting Loss: 539.847005\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 540.205276\n",
      "\tTesting Loss: 539.856893\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 540.197980\n",
      "\tTesting Loss: 539.853861\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 540.199478\n",
      "\tTesting Loss: 539.857595\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 540.197881\n",
      "\tTesting Loss: 539.855021\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 540.194939\n",
      "\tTesting Loss: 539.865204\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 540.187475\n",
      "\tTesting Loss: 539.867930\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 540.189946\n",
      "\tTesting Loss: 539.874400\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 540.188583\n",
      "\tTesting Loss: 539.865865\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 540.196035\n",
      "\tTesting Loss: 539.868490\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 540.189433\n",
      "\tTesting Loss: 539.870321\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 540.192103\n",
      "\tTesting Loss: 539.875804\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 540.211639\n",
      "\tTesting Loss: 539.843577\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 540.194433\n",
      "\tTesting Loss: 539.874441\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 540.216507\n",
      "\tTesting Loss: 539.903422\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 540.361196\n",
      "\tTesting Loss: 539.903076\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 540.373220\n",
      "\tTesting Loss: 539.907461\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 540.360214\n",
      "\tTesting Loss: 539.904633\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 540.369225\n",
      "\tTesting Loss: 539.906494\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 540.352737\n",
      "\tTesting Loss: 539.900757\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 540.346395\n",
      "\tTesting Loss: 539.888326\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 540.281825\n",
      "\tTesting Loss: 539.840495\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 540.232157\n",
      "\tTesting Loss: 539.843689\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 540.255397\n",
      "\tTesting Loss: 539.855357\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 540.234779\n",
      "\tTesting Loss: 539.835775\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 540.227305\n",
      "\tTesting Loss: 539.850098\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 540.228785\n",
      "\tTesting Loss: 539.835144\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 540.224091\n",
      "\tTesting Loss: 539.847310\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 540.227076\n",
      "\tTesting Loss: 539.837321\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 540.225220\n",
      "\tTesting Loss: 539.848022\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 540.226212\n",
      "\tTesting Loss: 539.835571\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 540.224955\n",
      "\tTesting Loss: 539.846608\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 540.224731\n",
      "\tTesting Loss: 539.836792\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 540.224620\n",
      "\tTesting Loss: 539.847514\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 540.224042\n",
      "\tTesting Loss: 539.835164\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 540.222750\n",
      "\tTesting Loss: 539.844910\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 540.223737\n",
      "\tTesting Loss: 539.838053\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 540.221746\n",
      "\tTesting Loss: 539.844910\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 540.220678\n",
      "\tTesting Loss: 539.835378\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 540.219523\n",
      "\tTesting Loss: 539.844055\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 540.218737\n",
      "\tTesting Loss: 539.834941\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 540.217626\n",
      "\tTesting Loss: 539.845388\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 540.215683\n",
      "\tTesting Loss: 539.838206\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 540.215581\n",
      "\tTesting Loss: 539.844879\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 540.211029\n",
      "\tTesting Loss: 539.836131\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 540.209610\n",
      "\tTesting Loss: 539.846252\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 540.204671\n",
      "\tTesting Loss: 539.841309\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 540.202863\n",
      "\tTesting Loss: 539.851542\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 540.195577\n",
      "\tTesting Loss: 539.848165\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 540.191808\n",
      "\tTesting Loss: 539.863047\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 540.185137\n",
      "\tTesting Loss: 539.866028\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 540.186821\n",
      "\tTesting Loss: 539.873596\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 540.183624\n",
      "\tTesting Loss: 539.875234\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 540.183019\n",
      "\tTesting Loss: 539.885295\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 540.193932\n",
      "\tTesting Loss: 539.864054\n",
      "\tLearning Rate: 0.006561000\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 540.242193\n",
      "\tTesting Loss: 539.816589\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 540.161560\n",
      "\tTesting Loss: 539.904012\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 540.345632\n",
      "\tTesting Loss: 539.904582\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 540.363497\n",
      "\tTesting Loss: 539.904134\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 540.326078\n",
      "\tTesting Loss: 539.887594\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 540.266917\n",
      "\tTesting Loss: 539.806824\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 540.218699\n",
      "\tTesting Loss: 539.849772\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 540.257263\n",
      "\tTesting Loss: 539.836873\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 540.226674\n",
      "\tTesting Loss: 539.837646\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 540.217328\n",
      "\tTesting Loss: 539.822673\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 540.200033\n",
      "\tTesting Loss: 539.829549\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 540.188441\n",
      "\tTesting Loss: 539.815043\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 540.179972\n",
      "\tTesting Loss: 539.833852\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 540.163246\n",
      "\tTesting Loss: 539.835276\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 540.159134\n",
      "\tTesting Loss: 539.850952\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 540.155291\n",
      "\tTesting Loss: 539.914408\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 540.348892\n",
      "\tTesting Loss: 539.910990\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 540.390053\n",
      "\tTesting Loss: 539.919718\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 540.354273\n",
      "\tTesting Loss: 539.909465\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 540.385897\n",
      "\tTesting Loss: 539.915934\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 540.351006\n",
      "\tTesting Loss: 539.907918\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 540.378665\n",
      "\tTesting Loss: 539.913951\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 540.339798\n",
      "\tTesting Loss: 539.903727\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 540.342715\n",
      "\tTesting Loss: 539.879567\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 540.260984\n",
      "\tTesting Loss: 539.847351\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 540.239634\n",
      "\tTesting Loss: 539.838277\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 540.250626\n",
      "\tTesting Loss: 539.859253\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 540.244461\n",
      "\tTesting Loss: 539.827108\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 540.228739\n",
      "\tTesting Loss: 539.850647\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 540.233322\n",
      "\tTesting Loss: 539.828186\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 540.216817\n",
      "\tTesting Loss: 539.844615\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 540.214330\n",
      "\tTesting Loss: 539.822693\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 540.208636\n",
      "\tTesting Loss: 539.839457\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 540.212639\n",
      "\tTesting Loss: 539.824107\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 540.199203\n",
      "\tTesting Loss: 539.834615\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 540.205310\n",
      "\tTesting Loss: 539.819499\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 540.196930\n",
      "\tTesting Loss: 539.833750\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 540.189550\n",
      "\tTesting Loss: 539.821971\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 540.175331\n",
      "\tTesting Loss: 539.841654\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 540.175252\n",
      "\tTesting Loss: 539.851186\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 540.312602\n",
      "\tTesting Loss: 539.876607\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 540.238627\n",
      "\tTesting Loss: 539.816315\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 540.222005\n",
      "\tTesting Loss: 539.831787\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 540.222771\n",
      "\tTesting Loss: 539.830739\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 540.205739\n",
      "\tTesting Loss: 539.841492\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 540.186539\n",
      "\tTesting Loss: 539.873800\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 540.308538\n",
      "\tTesting Loss: 539.913584\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 540.341769\n",
      "\tTesting Loss: 539.903503\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 540.345927\n",
      "\tTesting Loss: 539.881673\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 540.258682\n",
      "\tTesting Loss: 539.826467\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 540.194870\n",
      "\tTesting Loss: 539.816111\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 540.235107\n",
      "\tTesting Loss: 539.845723\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 540.204951\n",
      "\tTesting Loss: 539.803691\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 540.199687\n",
      "\tTesting Loss: 539.842448\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 540.161893\n",
      "\tTesting Loss: 539.843140\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 540.151347\n",
      "\tTesting Loss: 539.876343\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 540.217473\n",
      "\tTesting Loss: 539.922607\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 540.357084\n",
      "\tTesting Loss: 539.909892\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 540.390645\n",
      "\tTesting Loss: 539.919535\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 540.357142\n",
      "\tTesting Loss: 539.910563\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 540.387288\n",
      "\tTesting Loss: 539.917013\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 540.357376\n",
      "\tTesting Loss: 539.907654\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 540.380267\n",
      "\tTesting Loss: 539.915548\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 540.354632\n",
      "\tTesting Loss: 539.907532\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 540.366852\n",
      "\tTesting Loss: 539.906881\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 540.336533\n",
      "\tTesting Loss: 539.894430\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 540.300741\n",
      "\tTesting Loss: 539.850800\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 540.233607\n",
      "\tTesting Loss: 539.839783\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 540.243703\n",
      "\tTesting Loss: 539.844574\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 540.237630\n",
      "\tTesting Loss: 539.844859\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 540.228030\n",
      "\tTesting Loss: 539.835999\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 540.224894\n",
      "\tTesting Loss: 539.843597\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 540.226036\n",
      "\tTesting Loss: 539.835195\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 540.222242\n",
      "\tTesting Loss: 539.841634\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 540.224698\n",
      "\tTesting Loss: 539.835103\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 540.221830\n",
      "\tTesting Loss: 539.840566\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 540.223292\n",
      "\tTesting Loss: 539.835500\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 540.220640\n",
      "\tTesting Loss: 539.840739\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 540.221171\n",
      "\tTesting Loss: 539.835754\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 540.218808\n",
      "\tTesting Loss: 539.839742\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 540.218852\n",
      "\tTesting Loss: 539.835551\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 540.218997\n",
      "\tTesting Loss: 539.837535\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 540.217135\n",
      "\tTesting Loss: 539.833293\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 540.216840\n",
      "\tTesting Loss: 539.838582\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 540.214676\n",
      "\tTesting Loss: 539.832316\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 540.212463\n",
      "\tTesting Loss: 539.834646\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 540.211395\n",
      "\tTesting Loss: 539.831136\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 540.209592\n",
      "\tTesting Loss: 539.834320\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 540.208191\n",
      "\tTesting Loss: 539.829508\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 540.204918\n",
      "\tTesting Loss: 539.832560\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 540.201899\n",
      "\tTesting Loss: 539.829753\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 540.198758\n",
      "\tTesting Loss: 539.832143\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 540.191750\n",
      "\tTesting Loss: 539.832703\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 540.184448\n",
      "\tTesting Loss: 539.836233\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 540.174950\n",
      "\tTesting Loss: 539.844320\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 540.160210\n",
      "\tTesting Loss: 539.866150\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 540.159620\n",
      "\tTesting Loss: 539.877950\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 540.205009\n",
      "\tTesting Loss: 539.897329\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 540.288727\n",
      "\tTesting Loss: 539.817342\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 540.185224\n",
      "\tTesting Loss: 539.867940\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 527.104258\n",
      "\tTesting Loss: 432.432190\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 387.726974\n",
      "\tTesting Loss: 320.001516\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 311.153381\n",
      "\tTesting Loss: 283.600835\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 296.258611\n",
      "\tTesting Loss: 293.898265\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 294.336675\n",
      "\tTesting Loss: 306.187337\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 308.570137\n",
      "\tTesting Loss: 293.637024\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 302.692619\n",
      "\tTesting Loss: 347.809031\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 318.026530\n",
      "\tTesting Loss: 348.510610\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 313.422067\n",
      "\tTesting Loss: 300.518178\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 326.105750\n",
      "\tTesting Loss: 280.032257\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 319.157033\n",
      "\tTesting Loss: 348.236186\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 301.114855\n",
      "\tTesting Loss: 354.361633\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 299.328128\n",
      "\tTesting Loss: 302.962748\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 305.590474\n",
      "\tTesting Loss: 351.010254\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 311.170049\n",
      "\tTesting Loss: 253.500254\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 289.541938\n",
      "\tTesting Loss: 334.562795\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 320.488603\n",
      "\tTesting Loss: 316.662425\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 306.034144\n",
      "\tTesting Loss: 346.230428\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 324.395234\n",
      "\tTesting Loss: 283.659032\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 305.310092\n",
      "\tTesting Loss: 302.232920\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 307.067802\n",
      "\tTesting Loss: 266.350566\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 308.611946\n",
      "\tTesting Loss: 308.026235\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 320.135686\n",
      "\tTesting Loss: 358.801676\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 325.580269\n",
      "\tTesting Loss: 292.278687\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 315.217559\n",
      "\tTesting Loss: 319.020854\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 316.051936\n",
      "\tTesting Loss: 358.051758\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 312.948289\n",
      "\tTesting Loss: 334.643168\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 322.578303\n",
      "\tTesting Loss: 366.142629\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 320.607897\n",
      "\tTesting Loss: 303.265767\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 342.841623\n",
      "\tTesting Loss: 279.389582\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 314.087097\n",
      "\tTesting Loss: 313.272339\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 329.375305\n",
      "\tTesting Loss: 304.590210\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 336.034480\n",
      "\tTesting Loss: 345.955922\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 340.849585\n",
      "\tTesting Loss: 330.574178\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 338.480728\n",
      "\tTesting Loss: 352.577749\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 352.026426\n",
      "\tTesting Loss: 399.836487\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 349.322667\n",
      "\tTesting Loss: 355.484751\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 359.802458\n",
      "\tTesting Loss: 378.291748\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 373.162209\n",
      "\tTesting Loss: 368.384949\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 381.200511\n",
      "\tTesting Loss: 401.445017\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 387.546628\n",
      "\tTesting Loss: 411.618225\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 384.230741\n",
      "\tTesting Loss: 419.544861\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 402.493690\n",
      "\tTesting Loss: 412.918925\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 409.621634\n",
      "\tTesting Loss: 428.996236\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 408.935018\n",
      "\tTesting Loss: 409.837514\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 413.949646\n",
      "\tTesting Loss: 458.383626\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 431.610011\n",
      "\tTesting Loss: 445.307444\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 429.252996\n",
      "\tTesting Loss: 440.216868\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 432.899251\n",
      "\tTesting Loss: 453.130046\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 436.826914\n",
      "\tTesting Loss: 459.570811\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 437.774104\n",
      "\tTesting Loss: 458.967824\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 440.203542\n",
      "\tTesting Loss: 451.410522\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 444.261018\n",
      "\tTesting Loss: 472.648936\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 448.912277\n",
      "\tTesting Loss: 465.909973\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 452.715576\n",
      "\tTesting Loss: 460.145711\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 458.908826\n",
      "\tTesting Loss: 473.460795\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 468.045312\n",
      "\tTesting Loss: 478.727610\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 466.145370\n",
      "\tTesting Loss: 477.989522\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 473.301147\n",
      "\tTesting Loss: 485.613322\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 476.039715\n",
      "\tTesting Loss: 484.343597\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 481.951463\n",
      "\tTesting Loss: 487.718933\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 486.544121\n",
      "\tTesting Loss: 487.680979\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 489.703348\n",
      "\tTesting Loss: 490.098409\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 492.411758\n",
      "\tTesting Loss: 493.626933\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 496.593796\n",
      "\tTesting Loss: 495.765208\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 498.482869\n",
      "\tTesting Loss: 495.806580\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 500.115481\n",
      "\tTesting Loss: 497.483724\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 501.454226\n",
      "\tTesting Loss: 498.139435\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 501.400274\n",
      "\tTesting Loss: 498.073527\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 501.018229\n",
      "\tTesting Loss: 498.203695\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 499.365608\n",
      "\tTesting Loss: 500.058248\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 498.704158\n",
      "\tTesting Loss: 500.842448\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 499.405512\n",
      "\tTesting Loss: 500.086914\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 500.613052\n",
      "\tTesting Loss: 495.500549\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 508.442042\n",
      "\tTesting Loss: 527.785136\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 539.673360\n",
      "\tTesting Loss: 540.860758\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 540.694550\n",
      "\tTesting Loss: 539.958405\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 540.502197\n",
      "\tTesting Loss: 539.988342\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 540.369692\n",
      "\tTesting Loss: 539.942403\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 540.436073\n",
      "\tTesting Loss: 539.941101\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 540.365840\n",
      "\tTesting Loss: 539.924255\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 540.435900\n",
      "\tTesting Loss: 539.912048\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 540.373817\n",
      "\tTesting Loss: 539.915649\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 540.365336\n",
      "\tTesting Loss: 539.911845\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 540.371226\n",
      "\tTesting Loss: 539.913625\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 540.363617\n",
      "\tTesting Loss: 539.911397\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 540.364510\n",
      "\tTesting Loss: 539.911153\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 540.358109\n",
      "\tTesting Loss: 539.908691\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 540.355988\n",
      "\tTesting Loss: 539.907410\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 540.347285\n",
      "\tTesting Loss: 539.902344\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 540.338303\n",
      "\tTesting Loss: 539.894063\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 540.321121\n",
      "\tTesting Loss: 539.880168\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 540.300463\n",
      "\tTesting Loss: 539.861186\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 540.275019\n",
      "\tTesting Loss: 539.842346\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 540.255658\n",
      "\tTesting Loss: 539.829386\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 540.245595\n",
      "\tTesting Loss: 539.826650\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 540.243164\n",
      "\tTesting Loss: 539.824992\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 540.241681\n",
      "\tTesting Loss: 539.825165\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 540.240021\n",
      "\tTesting Loss: 539.823425\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 540.237129\n",
      "\tTesting Loss: 539.821991\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 540.234032\n",
      "\tTesting Loss: 539.820414\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 540.231056\n",
      "\tTesting Loss: 539.818461\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 540.228569\n",
      "\tTesting Loss: 539.817668\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 540.226212\n",
      "\tTesting Loss: 539.816386\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 540.224035\n",
      "\tTesting Loss: 539.814880\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 540.221837\n",
      "\tTesting Loss: 539.813985\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 540.219737\n",
      "\tTesting Loss: 539.813009\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 540.217514\n",
      "\tTesting Loss: 539.811747\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 540.215535\n",
      "\tTesting Loss: 539.810567\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 540.214012\n",
      "\tTesting Loss: 539.809326\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 540.211721\n",
      "\tTesting Loss: 539.809041\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 540.210241\n",
      "\tTesting Loss: 539.808146\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 540.208331\n",
      "\tTesting Loss: 539.807292\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 540.206721\n",
      "\tTesting Loss: 539.806091\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 540.204839\n",
      "\tTesting Loss: 539.806274\n",
      "\tLearning Rate: 0.005904900\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 540.203486\n",
      "\tTesting Loss: 539.804728\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 540.203267\n",
      "\tTesting Loss: 539.803080\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 540.201322\n",
      "\tTesting Loss: 539.803070\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 540.200246\n",
      "\tTesting Loss: 539.802226\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 540.198934\n",
      "\tTesting Loss: 539.800863\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 540.197784\n",
      "\tTesting Loss: 539.800771\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 540.196561\n",
      "\tTesting Loss: 539.799805\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 540.195653\n",
      "\tTesting Loss: 539.799194\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 540.194425\n",
      "\tTesting Loss: 539.797872\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 540.193141\n",
      "\tTesting Loss: 539.796977\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 540.192332\n",
      "\tTesting Loss: 539.796936\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 540.191416\n",
      "\tTesting Loss: 539.795614\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 540.189995\n",
      "\tTesting Loss: 539.794922\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 540.188899\n",
      "\tTesting Loss: 539.793701\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 540.187826\n",
      "\tTesting Loss: 539.793233\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 540.187040\n",
      "\tTesting Loss: 539.792613\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 540.185941\n",
      "\tTesting Loss: 539.791758\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 540.184832\n",
      "\tTesting Loss: 539.790710\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 540.183619\n",
      "\tTesting Loss: 539.789683\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 540.182653\n",
      "\tTesting Loss: 539.789307\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 540.181760\n",
      "\tTesting Loss: 539.788157\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 540.180667\n",
      "\tTesting Loss: 539.787170\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 540.179560\n",
      "\tTesting Loss: 539.786112\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 540.178487\n",
      "\tTesting Loss: 539.785136\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 540.176628\n",
      "\tTesting Loss: 539.784526\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 540.176483\n",
      "\tTesting Loss: 539.783610\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 540.175613\n",
      "\tTesting Loss: 539.782410\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 540.174375\n",
      "\tTesting Loss: 539.781982\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 540.173017\n",
      "\tTesting Loss: 539.780050\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 540.171694\n",
      "\tTesting Loss: 539.779012\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 540.170746\n",
      "\tTesting Loss: 539.778239\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 540.169512\n",
      "\tTesting Loss: 539.776978\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 540.168457\n",
      "\tTesting Loss: 539.776174\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 540.167277\n",
      "\tTesting Loss: 539.774536\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 540.165934\n",
      "\tTesting Loss: 539.773214\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 540.164429\n",
      "\tTesting Loss: 539.771484\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 540.162626\n",
      "\tTesting Loss: 539.770365\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 540.161702\n",
      "\tTesting Loss: 539.768901\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 540.160347\n",
      "\tTesting Loss: 539.767141\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 540.158152\n",
      "\tTesting Loss: 539.765615\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 540.156982\n",
      "\tTesting Loss: 539.764648\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 540.155767\n",
      "\tTesting Loss: 539.762980\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 540.153956\n",
      "\tTesting Loss: 539.761169\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 540.152288\n",
      "\tTesting Loss: 539.760152\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 540.150686\n",
      "\tTesting Loss: 539.757904\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 540.148705\n",
      "\tTesting Loss: 539.756846\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 540.146873\n",
      "\tTesting Loss: 539.754395\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 540.144595\n",
      "\tTesting Loss: 539.752055\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 540.143026\n",
      "\tTesting Loss: 539.750437\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 540.141179\n",
      "\tTesting Loss: 539.748556\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 540.138738\n",
      "\tTesting Loss: 539.746094\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 540.136414\n",
      "\tTesting Loss: 539.743510\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 540.134031\n",
      "\tTesting Loss: 539.741435\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 540.131434\n",
      "\tTesting Loss: 539.738495\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 540.129028\n",
      "\tTesting Loss: 539.736430\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 540.126699\n",
      "\tTesting Loss: 539.733449\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 540.123456\n",
      "\tTesting Loss: 539.730143\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 540.120720\n",
      "\tTesting Loss: 539.727976\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 540.117767\n",
      "\tTesting Loss: 539.724813\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 540.114619\n",
      "\tTesting Loss: 539.721049\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 540.110967\n",
      "\tTesting Loss: 539.717428\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 540.107567\n",
      "\tTesting Loss: 539.714233\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 540.103948\n",
      "\tTesting Loss: 539.710327\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 540.099487\n",
      "\tTesting Loss: 539.705912\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 540.095820\n",
      "\tTesting Loss: 539.701986\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 540.091649\n",
      "\tTesting Loss: 539.697266\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 540.086787\n",
      "\tTesting Loss: 539.692200\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 540.081492\n",
      "\tTesting Loss: 539.687113\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 540.075864\n",
      "\tTesting Loss: 539.681590\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 540.072164\n",
      "\tTesting Loss: 539.677338\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 540.066353\n",
      "\tTesting Loss: 539.670207\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 540.059504\n",
      "\tTesting Loss: 539.663127\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 540.051674\n",
      "\tTesting Loss: 539.655579\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 540.044902\n",
      "\tTesting Loss: 539.648733\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 540.036532\n",
      "\tTesting Loss: 539.639893\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 540.026749\n",
      "\tTesting Loss: 539.630890\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 540.017715\n",
      "\tTesting Loss: 539.620341\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 540.006734\n",
      "\tTesting Loss: 539.610311\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 539.998230\n",
      "\tTesting Loss: 539.600321\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 539.986521\n",
      "\tTesting Loss: 539.587290\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 539.973028\n",
      "\tTesting Loss: 539.573995\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 539.958420\n",
      "\tTesting Loss: 539.557699\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 539.941444\n",
      "\tTesting Loss: 539.542318\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 539.928233\n",
      "\tTesting Loss: 539.526703\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 539.906225\n",
      "\tTesting Loss: 539.503723\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 539.887655\n",
      "\tTesting Loss: 539.485107\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 539.860662\n",
      "\tTesting Loss: 539.456472\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 539.839991\n",
      "\tTesting Loss: 539.436106\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 539.804642\n",
      "\tTesting Loss: 539.395345\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 539.775810\n",
      "\tTesting Loss: 539.375488\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 539.738251\n",
      "\tTesting Loss: 539.321350\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 539.687307\n",
      "\tTesting Loss: 539.286102\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 539.656550\n",
      "\tTesting Loss: 539.235189\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 539.564758\n",
      "\tTesting Loss: 539.177531\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 539.571541\n",
      "\tTesting Loss: 539.145589\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 539.414678\n",
      "\tTesting Loss: 539.005422\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 539.391289\n",
      "\tTesting Loss: 539.002055\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 539.324692\n",
      "\tTesting Loss: 538.899516\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 539.150782\n",
      "\tTesting Loss: 538.766764\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 539.085828\n",
      "\tTesting Loss: 538.781921\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 539.104579\n",
      "\tTesting Loss: 538.719096\n",
      "\tLearning Rate: 0.005314410\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 539.022324\n",
      "\tTesting Loss: 538.615448\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 538.876592\n",
      "\tTesting Loss: 538.443146\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 538.707809\n",
      "\tTesting Loss: 538.296804\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 538.541995\n",
      "\tTesting Loss: 538.084005\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 538.358589\n",
      "\tTesting Loss: 538.512482\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 538.891085\n",
      "\tTesting Loss: 538.549744\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 538.914205\n",
      "\tTesting Loss: 538.483134\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 538.925921\n",
      "\tTesting Loss: 538.479451\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 538.770271\n",
      "\tTesting Loss: 538.282298\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 538.555929\n",
      "\tTesting Loss: 538.126149\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 538.337959\n",
      "\tTesting Loss: 537.861959\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 538.155912\n",
      "\tTesting Loss: 537.910350\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 538.120819\n",
      "\tTesting Loss: 538.640462\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 539.128634\n",
      "\tTesting Loss: 538.527883\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 538.921443\n",
      "\tTesting Loss: 538.850800\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 539.278562\n",
      "\tTesting Loss: 538.708842\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 538.995303\n",
      "\tTesting Loss: 538.659983\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 539.211655\n",
      "\tTesting Loss: 538.747742\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 538.866613\n",
      "\tTesting Loss: 538.321269\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 538.888535\n",
      "\tTesting Loss: 538.547038\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 538.854335\n",
      "\tTesting Loss: 538.275452\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 538.585424\n",
      "\tTesting Loss: 538.170451\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 538.387650\n",
      "\tTesting Loss: 537.926392\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 538.142390\n",
      "\tTesting Loss: 537.670186\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 537.989250\n",
      "\tTesting Loss: 537.967367\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 538.026372\n",
      "\tTesting Loss: 538.653473\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 539.133169\n",
      "\tTesting Loss: 538.598969\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 538.922831\n",
      "\tTesting Loss: 538.720886\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 539.114283\n",
      "\tTesting Loss: 538.624807\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 538.982585\n",
      "\tTesting Loss: 538.532979\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 538.829117\n",
      "\tTesting Loss: 538.321452\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 538.714177\n",
      "\tTesting Loss: 538.213582\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 538.509082\n",
      "\tTesting Loss: 537.977448\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 538.210223\n",
      "\tTesting Loss: 537.720418\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 537.967369\n",
      "\tTesting Loss: 537.494446\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 537.722041\n",
      "\tTesting Loss: 537.353678\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 537.757604\n",
      "\tTesting Loss: 537.613342\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 538.063024\n",
      "\tTesting Loss: 538.125651\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 538.668027\n",
      "\tTesting Loss: 538.142965\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 538.462784\n",
      "\tTesting Loss: 538.110250\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 538.728592\n",
      "\tTesting Loss: 538.227844\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 538.309756\n",
      "\tTesting Loss: 537.785299\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 538.228831\n",
      "\tTesting Loss: 537.789225\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 537.878298\n",
      "\tTesting Loss: 537.480560\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 537.876902\n",
      "\tTesting Loss: 537.618205\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 537.877090\n",
      "\tTesting Loss: 538.030060\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 538.647733\n",
      "\tTesting Loss: 538.227661\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 538.590034\n",
      "\tTesting Loss: 538.220581\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 538.758962\n",
      "\tTesting Loss: 538.259074\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 538.424266\n",
      "\tTesting Loss: 537.887482\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 538.337334\n",
      "\tTesting Loss: 537.872640\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 538.027700\n",
      "\tTesting Loss: 537.609294\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 537.882319\n",
      "\tTesting Loss: 537.520243\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 537.745616\n",
      "\tTesting Loss: 537.639933\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 537.945170\n",
      "\tTesting Loss: 538.269826\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 538.769379\n",
      "\tTesting Loss: 538.161682\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 538.598531\n",
      "\tTesting Loss: 538.338125\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 538.897479\n",
      "\tTesting Loss: 538.369690\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 538.495702\n",
      "\tTesting Loss: 537.976847\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 538.520208\n",
      "\tTesting Loss: 538.048014\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 538.244080\n",
      "\tTesting Loss: 537.739716\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 538.073441\n",
      "\tTesting Loss: 537.698242\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 537.624245\n",
      "\tTesting Loss: 537.409047\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 537.487885\n",
      "\tTesting Loss: 536.990550\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 537.356122\n",
      "\tTesting Loss: 538.186493\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 538.371005\n",
      "\tTesting Loss: 538.095174\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 538.809682\n",
      "\tTesting Loss: 538.431458\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 538.605525\n",
      "\tTesting Loss: 537.995158\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 538.493184\n",
      "\tTesting Loss: 538.028748\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 538.232760\n",
      "\tTesting Loss: 537.710856\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 537.909574\n",
      "\tTesting Loss: 537.596985\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 537.522611\n",
      "\tTesting Loss: 537.349060\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 537.228902\n",
      "\tTesting Loss: 536.996175\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 536.904350\n",
      "\tTesting Loss: 536.453288\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 536.983220\n",
      "\tTesting Loss: 537.694580\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 537.537514\n",
      "\tTesting Loss: 537.966654\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 538.557274\n",
      "\tTesting Loss: 537.929118\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 538.095835\n",
      "\tTesting Loss: 537.681844\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 538.269470\n",
      "\tTesting Loss: 537.790548\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 537.890244\n",
      "\tTesting Loss: 537.408122\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 537.710963\n",
      "\tTesting Loss: 537.340413\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 537.164879\n",
      "\tTesting Loss: 537.041677\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 536.957588\n",
      "\tTesting Loss: 536.411418\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 537.001241\n",
      "\tTesting Loss: 537.443817\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 537.215991\n",
      "\tTesting Loss: 537.826335\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 538.505941\n",
      "\tTesting Loss: 537.851919\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 537.958694\n",
      "\tTesting Loss: 537.606303\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 538.126195\n",
      "\tTesting Loss: 537.551249\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 537.568751\n",
      "\tTesting Loss: 537.022064\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 537.472750\n",
      "\tTesting Loss: 537.083445\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 537.147372\n",
      "\tTesting Loss: 536.923899\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 537.340591\n",
      "\tTesting Loss: 537.153931\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 537.634636\n",
      "\tTesting Loss: 537.494039\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 537.996282\n",
      "\tTesting Loss: 537.512472\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 537.758942\n",
      "\tTesting Loss: 537.268758\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 537.522619\n",
      "\tTesting Loss: 537.122874\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 537.127213\n",
      "\tTesting Loss: 536.978678\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 537.134967\n",
      "\tTesting Loss: 537.096802\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 537.467496\n",
      "\tTesting Loss: 537.815389\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 538.238719\n",
      "\tTesting Loss: 537.555176\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 537.947230\n",
      "\tTesting Loss: 537.691732\n",
      "\tLearning Rate: 0.004782969\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 537.876900\n",
      "\tTesting Loss: 537.360494\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 537.610209\n",
      "\tTesting Loss: 537.124481\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 537.354744\n",
      "\tTesting Loss: 537.012777\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 537.124771\n",
      "\tTesting Loss: 536.880107\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 537.209244\n",
      "\tTesting Loss: 537.122721\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 537.668691\n",
      "\tTesting Loss: 537.431742\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 537.794942\n",
      "\tTesting Loss: 537.206767\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 537.498945\n",
      "\tTesting Loss: 537.097127\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 537.242691\n",
      "\tTesting Loss: 537.013428\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 537.181544\n",
      "\tTesting Loss: 537.066996\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 537.459033\n",
      "\tTesting Loss: 537.383830\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 537.873174\n",
      "\tTesting Loss: 537.452209\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 537.713331\n",
      "\tTesting Loss: 537.228963\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 537.432187\n",
      "\tTesting Loss: 537.109100\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 537.062920\n",
      "\tTesting Loss: 536.926809\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 536.877431\n",
      "\tTesting Loss: 537.073568\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 537.171280\n",
      "\tTesting Loss: 537.444071\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 537.827558\n",
      "\tTesting Loss: 537.342896\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 537.653328\n",
      "\tTesting Loss: 537.390045\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 537.298767\n",
      "\tTesting Loss: 537.186493\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 536.944021\n",
      "\tTesting Loss: 537.122538\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 537.430644\n",
      "\tTesting Loss: 537.316589\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 537.557594\n",
      "\tTesting Loss: 537.278849\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 537.480576\n",
      "\tTesting Loss: 537.332601\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 537.239881\n",
      "\tTesting Loss: 537.258606\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 537.181292\n",
      "\tTesting Loss: 537.099772\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 537.458277\n",
      "\tTesting Loss: 537.320536\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 537.508731\n",
      "\tTesting Loss: 537.269318\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 537.448987\n",
      "\tTesting Loss: 537.284770\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 537.269768\n",
      "\tTesting Loss: 537.268921\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 536.998934\n",
      "\tTesting Loss: 536.942240\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 537.210757\n",
      "\tTesting Loss: 537.189148\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 537.385447\n",
      "\tTesting Loss: 537.210347\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 537.442169\n",
      "\tTesting Loss: 537.302277\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 537.300611\n",
      "\tTesting Loss: 537.311503\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 537.077644\n",
      "\tTesting Loss: 537.026733\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 537.198583\n",
      "\tTesting Loss: 537.111410\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 537.287684\n",
      "\tTesting Loss: 537.152690\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 537.337443\n",
      "\tTesting Loss: 537.256734\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 537.286949\n",
      "\tTesting Loss: 537.271128\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 537.072469\n",
      "\tTesting Loss: 537.025716\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 537.071457\n",
      "\tTesting Loss: 537.003754\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 537.182276\n",
      "\tTesting Loss: 537.127523\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 537.335589\n",
      "\tTesting Loss: 537.241648\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 537.293376\n",
      "\tTesting Loss: 537.237701\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 536.965914\n",
      "\tTesting Loss: 537.077026\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 537.157644\n",
      "\tTesting Loss: 537.093486\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 537.250610\n",
      "\tTesting Loss: 537.290558\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 537.327896\n",
      "\tTesting Loss: 537.353455\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 537.270510\n",
      "\tTesting Loss: 537.258301\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 537.248006\n",
      "\tTesting Loss: 537.205190\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 537.294200\n",
      "\tTesting Loss: 537.319010\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 537.406606\n",
      "\tTesting Loss: 537.369395\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 537.451426\n",
      "\tTesting Loss: 537.381419\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 537.396983\n",
      "\tTesting Loss: 537.409465\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 537.304637\n",
      "\tTesting Loss: 537.243225\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 537.203339\n",
      "\tTesting Loss: 537.253103\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 537.203476\n",
      "\tTesting Loss: 537.292562\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 537.304881\n",
      "\tTesting Loss: 537.268138\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 537.332909\n",
      "\tTesting Loss: 537.392995\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 537.320402\n",
      "\tTesting Loss: 537.355774\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 537.239553\n",
      "\tTesting Loss: 537.291687\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 537.247335\n",
      "\tTesting Loss: 537.254700\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 537.259694\n",
      "\tTesting Loss: 537.385518\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 537.262487\n",
      "\tTesting Loss: 537.397542\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 537.166214\n",
      "\tTesting Loss: 537.232666\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 537.143265\n",
      "\tTesting Loss: 537.218099\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 537.099983\n",
      "\tTesting Loss: 537.475749\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 537.365972\n",
      "\tTesting Loss: 537.376272\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 537.229319\n",
      "\tTesting Loss: 537.530507\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 537.145943\n",
      "\tTesting Loss: 537.279602\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 537.275513\n",
      "\tTesting Loss: 537.424744\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 537.119141\n",
      "\tTesting Loss: 537.611928\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 537.531484\n",
      "\tTesting Loss: 537.368785\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 537.114003\n",
      "\tTesting Loss: 537.754924\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 536.476369\n",
      "\tTesting Loss: 537.325012\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 538.076055\n",
      "\tTesting Loss: 537.913676\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 537.028188\n",
      "\tTesting Loss: 537.574422\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 536.726301\n",
      "\tTesting Loss: 537.598948\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 526.316879\n",
      "\tTesting Loss: 502.129903\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 509.199954\n",
      "\tTesting Loss: 493.221802\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 505.539825\n",
      "\tTesting Loss: 493.617605\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 505.467257\n",
      "\tTesting Loss: 491.357320\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 504.594348\n",
      "\tTesting Loss: 492.049042\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 506.327054\n",
      "\tTesting Loss: 493.973521\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 505.202988\n",
      "\tTesting Loss: 487.224731\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 496.480855\n",
      "\tTesting Loss: 492.152303\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 506.498306\n",
      "\tTesting Loss: 498.489115\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 514.407870\n",
      "\tTesting Loss: 495.918233\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 509.648532\n",
      "\tTesting Loss: 499.847941\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 502.518773\n",
      "\tTesting Loss: 485.869527\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 504.982473\n",
      "\tTesting Loss: 507.954549\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 519.634707\n",
      "\tTesting Loss: 500.239146\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 519.605225\n",
      "\tTesting Loss: 504.235657\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 515.794978\n",
      "\tTesting Loss: 489.031667\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 503.983513\n",
      "\tTesting Loss: 503.082367\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 504.865008\n",
      "\tTesting Loss: 506.277140\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 510.558599\n",
      "\tTesting Loss: 505.408651\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 510.291512\n",
      "\tTesting Loss: 491.245097\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 503.284111\n",
      "\tTesting Loss: 500.702352\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 504.454259\n",
      "\tTesting Loss: 510.208069\n",
      "\tLearning Rate: 0.004304672\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 513.564715\n",
      "\tTesting Loss: 508.462463\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 513.300611\n",
      "\tTesting Loss: 498.776937\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 507.281352\n",
      "\tTesting Loss: 498.614746\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 504.263875\n",
      "\tTesting Loss: 501.006775\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 505.455490\n",
      "\tTesting Loss: 498.948181\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 505.230962\n",
      "\tTesting Loss: 496.459757\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 503.651357\n",
      "\tTesting Loss: 499.150208\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 504.255117\n",
      "\tTesting Loss: 497.804535\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 504.382607\n",
      "\tTesting Loss: 497.079020\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 503.602170\n",
      "\tTesting Loss: 498.041280\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 503.816307\n",
      "\tTesting Loss: 497.302958\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 503.923963\n",
      "\tTesting Loss: 497.165324\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 503.929469\n",
      "\tTesting Loss: 497.742798\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 504.317464\n",
      "\tTesting Loss: 498.004293\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 504.847315\n",
      "\tTesting Loss: 498.270610\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 505.372879\n",
      "\tTesting Loss: 498.785319\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 505.927231\n",
      "\tTesting Loss: 498.992696\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 506.403300\n",
      "\tTesting Loss: 499.331736\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 506.945763\n",
      "\tTesting Loss: 499.655416\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 507.490700\n",
      "\tTesting Loss: 499.966888\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 507.980942\n",
      "\tTesting Loss: 500.361674\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 508.471013\n",
      "\tTesting Loss: 500.677205\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 508.903580\n",
      "\tTesting Loss: 500.843384\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 509.168879\n",
      "\tTesting Loss: 501.077525\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 509.468638\n",
      "\tTesting Loss: 501.164113\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 509.750275\n",
      "\tTesting Loss: 501.380971\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 510.022588\n",
      "\tTesting Loss: 501.637594\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 510.351120\n",
      "\tTesting Loss: 501.833354\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 510.643168\n",
      "\tTesting Loss: 502.068054\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 510.937480\n",
      "\tTesting Loss: 502.332316\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 511.273506\n",
      "\tTesting Loss: 502.591319\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 511.578059\n",
      "\tTesting Loss: 502.908223\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 511.846133\n",
      "\tTesting Loss: 503.188232\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 512.158399\n",
      "\tTesting Loss: 503.430623\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 512.449928\n",
      "\tTesting Loss: 503.671224\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 512.745550\n",
      "\tTesting Loss: 503.993195\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 513.078557\n",
      "\tTesting Loss: 504.288289\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 513.426539\n",
      "\tTesting Loss: 504.521281\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 513.780563\n",
      "\tTesting Loss: 504.781169\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 514.121498\n",
      "\tTesting Loss: 504.994812\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 514.461726\n",
      "\tTesting Loss: 505.230286\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 514.774770\n",
      "\tTesting Loss: 505.376506\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 515.064929\n",
      "\tTesting Loss: 505.500610\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 515.361109\n",
      "\tTesting Loss: 505.627686\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 515.672577\n",
      "\tTesting Loss: 505.768311\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 515.979459\n",
      "\tTesting Loss: 505.881124\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 516.287033\n",
      "\tTesting Loss: 506.054942\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 516.587407\n",
      "\tTesting Loss: 506.211263\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 516.886932\n",
      "\tTesting Loss: 506.366760\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 517.206403\n",
      "\tTesting Loss: 506.623596\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 517.558614\n",
      "\tTesting Loss: 506.895243\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 517.906054\n",
      "\tTesting Loss: 507.213481\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 518.295054\n",
      "\tTesting Loss: 507.548777\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 518.624669\n",
      "\tTesting Loss: 507.858144\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 519.000783\n",
      "\tTesting Loss: 508.271647\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 519.422241\n",
      "\tTesting Loss: 508.675995\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 519.795049\n",
      "\tTesting Loss: 509.090881\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 520.199463\n",
      "\tTesting Loss: 509.501261\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 520.654806\n",
      "\tTesting Loss: 509.936890\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 521.097107\n",
      "\tTesting Loss: 510.352559\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 521.543007\n",
      "\tTesting Loss: 510.670603\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 521.986013\n",
      "\tTesting Loss: 511.009928\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 522.416768\n",
      "\tTesting Loss: 511.460541\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 522.855362\n",
      "\tTesting Loss: 511.903687\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 523.350627\n",
      "\tTesting Loss: 512.372609\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 523.922656\n",
      "\tTesting Loss: 512.797648\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 524.397629\n",
      "\tTesting Loss: 513.121847\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 524.890381\n",
      "\tTesting Loss: 513.569071\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 525.502619\n",
      "\tTesting Loss: 514.161499\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 526.175217\n",
      "\tTesting Loss: 514.814779\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 526.890920\n",
      "\tTesting Loss: 515.555878\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 527.646886\n",
      "\tTesting Loss: 516.377441\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 528.395373\n",
      "\tTesting Loss: 517.091878\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 529.097438\n",
      "\tTesting Loss: 517.672689\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 529.828110\n",
      "\tTesting Loss: 518.168111\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 530.478343\n",
      "\tTesting Loss: 518.723562\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 531.204081\n",
      "\tTesting Loss: 519.363241\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 531.952103\n",
      "\tTesting Loss: 520.050120\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 532.750351\n",
      "\tTesting Loss: 520.785746\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 533.566147\n",
      "\tTesting Loss: 521.448242\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 534.437236\n",
      "\tTesting Loss: 521.997945\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 535.263814\n",
      "\tTesting Loss: 522.483521\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 536.082408\n",
      "\tTesting Loss: 523.196289\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 536.949104\n",
      "\tTesting Loss: 524.131978\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 537.889188\n",
      "\tTesting Loss: 525.440043\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 539.062836\n",
      "\tTesting Loss: 526.869263\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 540.242025\n",
      "\tTesting Loss: 527.970306\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 541.332527\n",
      "\tTesting Loss: 528.555237\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 542.358627\n",
      "\tTesting Loss: 528.649923\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 543.046127\n",
      "\tTesting Loss: 528.552144\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 543.620829\n",
      "\tTesting Loss: 528.845449\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 544.405080\n",
      "\tTesting Loss: 530.219076\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 545.480474\n",
      "\tTesting Loss: 532.447734\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 546.880157\n",
      "\tTesting Loss: 534.624451\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 548.407977\n",
      "\tTesting Loss: 535.865763\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 549.634349\n",
      "\tTesting Loss: 536.399190\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 550.912580\n",
      "\tTesting Loss: 536.621358\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 551.939519\n",
      "\tTesting Loss: 536.019277\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 552.646261\n",
      "\tTesting Loss: 537.345072\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 553.904490\n",
      "\tTesting Loss: 541.690165\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 555.713882\n",
      "\tTesting Loss: 545.265299\n",
      "\tLearning Rate: 0.003874205\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 557.780121\n",
      "\tTesting Loss: 544.735331\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 560.138031\n",
      "\tTesting Loss: 545.673889\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 561.386058\n",
      "\tTesting Loss: 550.026123\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 564.654409\n",
      "\tTesting Loss: 553.423045\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 567.106941\n",
      "\tTesting Loss: 558.184469\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 570.019384\n",
      "\tTesting Loss: 562.316732\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 573.660446\n",
      "\tTesting Loss: 566.937317\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 577.407135\n",
      "\tTesting Loss: 571.444906\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 570.235008\n",
      "\tTesting Loss: 522.695089\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 529.081980\n",
      "\tTesting Loss: 538.817342\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 540.261058\n",
      "\tTesting Loss: 539.870036\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 540.300509\n",
      "\tTesting Loss: 539.856649\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 540.306114\n",
      "\tTesting Loss: 539.850118\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 540.285950\n",
      "\tTesting Loss: 539.859863\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 540.278290\n",
      "\tTesting Loss: 539.841777\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 540.249095\n",
      "\tTesting Loss: 539.841756\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 540.232925\n",
      "\tTesting Loss: 539.814992\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 540.204755\n",
      "\tTesting Loss: 539.813049\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 540.188230\n",
      "\tTesting Loss: 539.782369\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 540.160690\n",
      "\tTesting Loss: 539.773326\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 540.143260\n",
      "\tTesting Loss: 539.748566\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 540.118792\n",
      "\tTesting Loss: 539.734090\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 540.097959\n",
      "\tTesting Loss: 539.708089\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 540.065605\n",
      "\tTesting Loss: 539.676310\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 540.029716\n",
      "\tTesting Loss: 539.643270\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 539.984365\n",
      "\tTesting Loss: 539.591624\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 539.922132\n",
      "\tTesting Loss: 539.530294\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 539.836110\n",
      "\tTesting Loss: 539.437317\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 539.705981\n",
      "\tTesting Loss: 539.290100\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 539.453692\n",
      "\tTesting Loss: 538.971029\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 538.853864\n",
      "\tTesting Loss: 538.002930\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 536.109825\n",
      "\tTesting Loss: 530.583150\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 518.409426\n",
      "\tTesting Loss: 509.503621\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 532.441320\n",
      "\tTesting Loss: 529.902466\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 549.220861\n",
      "\tTesting Loss: 540.318014\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 560.067800\n",
      "\tTesting Loss: 544.575195\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 563.733716\n",
      "\tTesting Loss: 556.668355\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 568.522522\n",
      "\tTesting Loss: 558.895610\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 572.044429\n",
      "\tTesting Loss: 552.995575\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 575.653956\n",
      "\tTesting Loss: 553.322266\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 577.043671\n",
      "\tTesting Loss: 562.901967\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 578.749873\n",
      "\tTesting Loss: 563.251424\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 580.079984\n",
      "\tTesting Loss: 560.940592\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 580.758817\n",
      "\tTesting Loss: 565.653097\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 581.226725\n",
      "\tTesting Loss: 564.624980\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 581.594551\n",
      "\tTesting Loss: 563.825867\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 581.843066\n",
      "\tTesting Loss: 563.523183\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 582.026978\n",
      "\tTesting Loss: 564.039225\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 582.178833\n",
      "\tTesting Loss: 563.899943\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 582.335965\n",
      "\tTesting Loss: 564.180054\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 582.537542\n",
      "\tTesting Loss: 565.708425\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 569.795512\n",
      "\tTesting Loss: 518.169271\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 532.239459\n",
      "\tTesting Loss: 538.631297\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 540.741247\n",
      "\tTesting Loss: 540.167725\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 540.373838\n",
      "\tTesting Loss: 539.917308\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 540.394572\n",
      "\tTesting Loss: 539.906799\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 540.385053\n",
      "\tTesting Loss: 540.010061\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 540.456579\n",
      "\tTesting Loss: 539.931925\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 540.307086\n",
      "\tTesting Loss: 539.889771\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 540.306132\n",
      "\tTesting Loss: 539.891530\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 540.314186\n",
      "\tTesting Loss: 539.883260\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 540.290082\n",
      "\tTesting Loss: 539.882741\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 540.286395\n",
      "\tTesting Loss: 539.874512\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 540.269948\n",
      "\tTesting Loss: 539.876434\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 540.268489\n",
      "\tTesting Loss: 539.866608\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 540.255600\n",
      "\tTesting Loss: 539.869934\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 540.255735\n",
      "\tTesting Loss: 539.861959\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 540.244781\n",
      "\tTesting Loss: 539.860626\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 540.243006\n",
      "\tTesting Loss: 539.857045\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 540.238263\n",
      "\tTesting Loss: 539.856018\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 540.235235\n",
      "\tTesting Loss: 539.852407\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 540.229655\n",
      "\tTesting Loss: 539.849386\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 540.224520\n",
      "\tTesting Loss: 539.844808\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 540.219462\n",
      "\tTesting Loss: 539.843079\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 540.214930\n",
      "\tTesting Loss: 539.837056\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 540.207545\n",
      "\tTesting Loss: 539.832937\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 540.201370\n",
      "\tTesting Loss: 539.827576\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 540.194290\n",
      "\tTesting Loss: 539.822449\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 540.187119\n",
      "\tTesting Loss: 539.816335\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 540.180822\n",
      "\tTesting Loss: 539.812459\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 540.174273\n",
      "\tTesting Loss: 539.804331\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 540.162318\n",
      "\tTesting Loss: 539.795166\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 540.149724\n",
      "\tTesting Loss: 539.784821\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 540.136998\n",
      "\tTesting Loss: 539.774719\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 540.123131\n",
      "\tTesting Loss: 539.761617\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 540.106143\n",
      "\tTesting Loss: 539.745717\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 540.085503\n",
      "\tTesting Loss: 539.726746\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 540.060226\n",
      "\tTesting Loss: 539.702738\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 540.028671\n",
      "\tTesting Loss: 539.671336\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 539.992752\n",
      "\tTesting Loss: 539.640299\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 539.938726\n",
      "\tTesting Loss: 539.580688\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 539.853022\n",
      "\tTesting Loss: 539.496104\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 539.733999\n",
      "\tTesting Loss: 539.359416\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 539.521436\n",
      "\tTesting Loss: 539.079803\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 539.029739\n",
      "\tTesting Loss: 538.300303\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 536.993111\n",
      "\tTesting Loss: 533.244436\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 530.887952\n",
      "\tTesting Loss: 529.754598\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 543.934580\n",
      "\tTesting Loss: 557.694031\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 568.140521\n",
      "\tTesting Loss: 555.550517\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 574.219854\n",
      "\tTesting Loss: 570.142415\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 569.250773\n",
      "\tTesting Loss: 541.708944\n",
      "\tLearning Rate: 0.003486784\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 574.160154\n",
      "\tTesting Loss: 522.053406\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 536.872894\n",
      "\tTesting Loss: 540.530599\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 540.751650\n",
      "\tTesting Loss: 539.903992\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 540.383138\n",
      "\tTesting Loss: 539.916443\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 540.416941\n",
      "\tTesting Loss: 539.940511\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 540.445374\n",
      "\tTesting Loss: 539.909922\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 540.348544\n",
      "\tTesting Loss: 539.903849\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 540.360619\n",
      "\tTesting Loss: 539.907735\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 540.336901\n",
      "\tTesting Loss: 539.900757\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 540.345726\n",
      "\tTesting Loss: 539.901978\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 540.331390\n",
      "\tTesting Loss: 539.897705\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 540.332415\n",
      "\tTesting Loss: 539.896932\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 540.325750\n",
      "\tTesting Loss: 539.895040\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 540.321264\n",
      "\tTesting Loss: 539.893107\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 540.316208\n",
      "\tTesting Loss: 539.891235\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 540.309992\n",
      "\tTesting Loss: 539.888733\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 540.303230\n",
      "\tTesting Loss: 539.885803\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 540.295842\n",
      "\tTesting Loss: 539.882924\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 540.288099\n",
      "\tTesting Loss: 539.879659\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 540.279828\n",
      "\tTesting Loss: 539.876546\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 540.272069\n",
      "\tTesting Loss: 539.873352\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 540.263662\n",
      "\tTesting Loss: 539.869670\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 540.255870\n",
      "\tTesting Loss: 539.866486\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 540.249237\n",
      "\tTesting Loss: 539.864034\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 540.243973\n",
      "\tTesting Loss: 539.861430\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 540.238653\n",
      "\tTesting Loss: 539.858724\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 540.234428\n",
      "\tTesting Loss: 539.856384\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 540.230840\n",
      "\tTesting Loss: 539.853943\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 540.227331\n",
      "\tTesting Loss: 539.851135\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 540.223816\n",
      "\tTesting Loss: 539.848501\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 540.219981\n",
      "\tTesting Loss: 539.845479\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 540.216593\n",
      "\tTesting Loss: 539.842529\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 540.212949\n",
      "\tTesting Loss: 539.838928\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 540.208654\n",
      "\tTesting Loss: 539.836161\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 540.204826\n",
      "\tTesting Loss: 539.833008\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 540.200724\n",
      "\tTesting Loss: 539.831278\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 540.197688\n",
      "\tTesting Loss: 539.829041\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 540.194664\n",
      "\tTesting Loss: 539.824951\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 540.190252\n",
      "\tTesting Loss: 539.823100\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 540.186409\n",
      "\tTesting Loss: 539.818258\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 540.180394\n",
      "\tTesting Loss: 539.813049\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 540.174736\n",
      "\tTesting Loss: 539.809224\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 540.169413\n",
      "\tTesting Loss: 539.801656\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 540.161514\n",
      "\tTesting Loss: 539.798574\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 540.155599\n",
      "\tTesting Loss: 539.788177\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 540.146306\n",
      "\tTesting Loss: 539.781321\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 540.137624\n",
      "\tTesting Loss: 539.772583\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 540.126328\n",
      "\tTesting Loss: 539.759298\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 540.112900\n",
      "\tTesting Loss: 539.750468\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 540.099454\n",
      "\tTesting Loss: 539.729024\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 540.081512\n",
      "\tTesting Loss: 539.719279\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 540.059514\n",
      "\tTesting Loss: 539.693726\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 540.034342\n",
      "\tTesting Loss: 539.659688\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 540.000857\n",
      "\tTesting Loss: 539.628337\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 539.960531\n",
      "\tTesting Loss: 539.577881\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 539.912549\n",
      "\tTesting Loss: 539.515910\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 539.848587\n",
      "\tTesting Loss: 539.431091\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 539.758120\n",
      "\tTesting Loss: 539.308838\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 539.632797\n",
      "\tTesting Loss: 539.114909\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 539.455154\n",
      "\tTesting Loss: 538.862467\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 539.200742\n",
      "\tTesting Loss: 538.553141\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 538.898636\n",
      "\tTesting Loss: 538.382345\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 538.790675\n",
      "\tTesting Loss: 538.301361\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 538.984993\n",
      "\tTesting Loss: 538.732259\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 539.307070\n",
      "\tTesting Loss: 539.097005\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 539.505379\n",
      "\tTesting Loss: 538.985046\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 539.158414\n",
      "\tTesting Loss: 538.510539\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 538.835037\n",
      "\tTesting Loss: 538.112590\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 538.372365\n",
      "\tTesting Loss: 537.495992\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 538.205488\n",
      "\tTesting Loss: 537.738688\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 538.494863\n",
      "\tTesting Loss: 538.157308\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 538.743212\n",
      "\tTesting Loss: 538.121867\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 538.349762\n",
      "\tTesting Loss: 537.293742\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 537.459653\n",
      "\tTesting Loss: 536.469686\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 537.413104\n",
      "\tTesting Loss: 537.307638\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 538.192848\n",
      "\tTesting Loss: 538.247762\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 538.818039\n",
      "\tTesting Loss: 538.112091\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 538.190648\n",
      "\tTesting Loss: 537.064596\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 537.129097\n",
      "\tTesting Loss: 536.401550\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 537.410940\n",
      "\tTesting Loss: 537.632467\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 538.380104\n",
      "\tTesting Loss: 538.040365\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 538.356150\n",
      "\tTesting Loss: 537.264709\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 536.837924\n",
      "\tTesting Loss: 535.481323\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 536.761075\n",
      "\tTesting Loss: 537.256897\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 538.437798\n",
      "\tTesting Loss: 538.315999\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 538.672134\n",
      "\tTesting Loss: 537.684306\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 537.085251\n",
      "\tTesting Loss: 535.280853\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 536.251144\n",
      "\tTesting Loss: 535.670858\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 538.092532\n",
      "\tTesting Loss: 538.318563\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 538.996646\n",
      "\tTesting Loss: 538.217000\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 538.481354\n",
      "\tTesting Loss: 538.087219\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 537.960571\n",
      "\tTesting Loss: 522.026367\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 539.746536\n",
      "\tTesting Loss: 542.684957\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 560.870707\n",
      "\tTesting Loss: 540.857859\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 558.106277\n",
      "\tTesting Loss: 544.715739\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 565.509186\n",
      "\tTesting Loss: 548.233988\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 567.150940\n",
      "\tTesting Loss: 554.679779\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 570.199727\n",
      "\tTesting Loss: 547.648966\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 570.717730\n",
      "\tTesting Loss: 562.515462\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 571.841235\n",
      "\tTesting Loss: 550.817424\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 577.337280\n",
      "\tTesting Loss: 550.892354\n",
      "\tLearning Rate: 0.003138106\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 579.625577\n",
      "\tTesting Loss: 553.038879\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 580.867071\n",
      "\tTesting Loss: 563.480998\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 576.523631\n",
      "\tTesting Loss: 554.459381\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 574.127660\n",
      "\tTesting Loss: 552.760040\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 578.952382\n",
      "\tTesting Loss: 548.413615\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 578.109141\n",
      "\tTesting Loss: 553.953735\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 581.840062\n",
      "\tTesting Loss: 539.258362\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 537.804418\n",
      "\tTesting Loss: 525.857544\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 535.049975\n",
      "\tTesting Loss: 538.063721\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 539.797290\n",
      "\tTesting Loss: 539.614777\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 540.144185\n",
      "\tTesting Loss: 539.725179\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 540.052561\n",
      "\tTesting Loss: 539.737376\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 540.135239\n",
      "\tTesting Loss: 539.663981\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 540.074855\n",
      "\tTesting Loss: 539.678243\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 540.031713\n",
      "\tTesting Loss: 539.616231\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 540.009567\n",
      "\tTesting Loss: 539.602610\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 540.000656\n",
      "\tTesting Loss: 539.586466\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 540.020767\n",
      "\tTesting Loss: 539.610942\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 540.179385\n",
      "\tTesting Loss: 539.698568\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 540.346947\n",
      "\tTesting Loss: 539.998301\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 540.454597\n",
      "\tTesting Loss: 539.710836\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 540.757688\n",
      "\tTesting Loss: 542.563741\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 558.705571\n",
      "\tTesting Loss: 550.138621\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 565.197825\n",
      "\tTesting Loss: 551.776143\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 567.931483\n",
      "\tTesting Loss: 542.863668\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 567.293732\n",
      "\tTesting Loss: 538.151652\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 568.433202\n",
      "\tTesting Loss: 544.196147\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 576.497147\n",
      "\tTesting Loss: 539.539307\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 573.154292\n",
      "\tTesting Loss: 549.593018\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 579.326721\n",
      "\tTesting Loss: 528.087657\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 532.121892\n",
      "\tTesting Loss: 533.110891\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 538.665929\n",
      "\tTesting Loss: 540.133403\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 540.549995\n",
      "\tTesting Loss: 539.898092\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 540.306554\n",
      "\tTesting Loss: 539.899638\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 540.353157\n",
      "\tTesting Loss: 539.926137\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 540.367020\n",
      "\tTesting Loss: 539.870931\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 540.275861\n",
      "\tTesting Loss: 539.862345\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 540.251684\n",
      "\tTesting Loss: 539.838257\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 540.220365\n",
      "\tTesting Loss: 539.835409\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 540.210429\n",
      "\tTesting Loss: 539.813456\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 540.184855\n",
      "\tTesting Loss: 539.809733\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 540.177516\n",
      "\tTesting Loss: 539.798787\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 540.162104\n",
      "\tTesting Loss: 539.790609\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 540.155037\n",
      "\tTesting Loss: 539.787425\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 540.148137\n",
      "\tTesting Loss: 539.779856\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 540.141841\n",
      "\tTesting Loss: 539.775960\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 540.134646\n",
      "\tTesting Loss: 539.766052\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 540.127446\n",
      "\tTesting Loss: 539.760844\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 540.120989\n",
      "\tTesting Loss: 539.754110\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 540.112628\n",
      "\tTesting Loss: 539.745127\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 540.104073\n",
      "\tTesting Loss: 539.737671\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 540.095540\n",
      "\tTesting Loss: 539.728760\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 540.087255\n",
      "\tTesting Loss: 539.721141\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 540.075569\n",
      "\tTesting Loss: 539.706757\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 540.060232\n",
      "\tTesting Loss: 539.695506\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 540.045703\n",
      "\tTesting Loss: 539.676249\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 540.029602\n",
      "\tTesting Loss: 539.658915\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 540.008906\n",
      "\tTesting Loss: 539.636525\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 539.983083\n",
      "\tTesting Loss: 539.598531\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 539.950770\n",
      "\tTesting Loss: 539.563253\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 539.908956\n",
      "\tTesting Loss: 539.504588\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 539.853465\n",
      "\tTesting Loss: 539.438416\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 539.779017\n",
      "\tTesting Loss: 539.328308\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 539.661082\n",
      "\tTesting Loss: 539.133494\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 539.444466\n",
      "\tTesting Loss: 538.744222\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 538.930870\n",
      "\tTesting Loss: 537.527344\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 535.968587\n",
      "\tTesting Loss: 527.952433\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 528.344091\n",
      "\tTesting Loss: 529.839101\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 556.110537\n",
      "\tTesting Loss: 539.910990\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 563.917819\n",
      "\tTesting Loss: 545.135915\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 568.121312\n",
      "\tTesting Loss: 544.905019\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 569.465942\n",
      "\tTesting Loss: 541.995616\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 570.566930\n",
      "\tTesting Loss: 543.279755\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 573.451912\n",
      "\tTesting Loss: 539.732605\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 572.468246\n",
      "\tTesting Loss: 542.568481\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 576.069616\n",
      "\tTesting Loss: 529.266357\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 531.037804\n",
      "\tTesting Loss: 516.020915\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 528.930087\n",
      "\tTesting Loss: 533.775167\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 539.082110\n",
      "\tTesting Loss: 539.673411\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 540.100138\n",
      "\tTesting Loss: 539.708150\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 540.051697\n",
      "\tTesting Loss: 539.719421\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 539.989741\n",
      "\tTesting Loss: 539.627431\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 539.964638\n",
      "\tTesting Loss: 539.574422\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 539.863103\n",
      "\tTesting Loss: 539.474497\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 539.766627\n",
      "\tTesting Loss: 539.344747\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 539.602348\n",
      "\tTesting Loss: 539.142751\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 539.394155\n",
      "\tTesting Loss: 538.813772\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 539.075872\n",
      "\tTesting Loss: 538.365855\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 538.751968\n",
      "\tTesting Loss: 537.774251\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 538.613408\n",
      "\tTesting Loss: 537.351929\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 539.606148\n",
      "\tTesting Loss: 537.406718\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 538.862447\n",
      "\tTesting Loss: 538.241089\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 538.537618\n",
      "\tTesting Loss: 536.316193\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 537.390874\n",
      "\tTesting Loss: 534.104747\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 537.639496\n",
      "\tTesting Loss: 530.171183\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 534.546839\n",
      "\tTesting Loss: 532.536601\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 536.292542\n",
      "\tTesting Loss: 531.901754\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 534.952072\n",
      "\tTesting Loss: 527.048157\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 540.742803\n",
      "\tTesting Loss: 531.028544\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 541.548953\n",
      "\tTesting Loss: 526.265350\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 542.055598\n",
      "\tTesting Loss: 529.600718\n",
      "\tLearning Rate: 0.002824295\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 541.139860\n",
      "\tTesting Loss: 533.004110\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 536.650246\n",
      "\tTesting Loss: 530.042114\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 545.947512\n",
      "\tTesting Loss: 535.364339\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 555.611618\n",
      "\tTesting Loss: 542.612610\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 550.366104\n",
      "\tTesting Loss: 538.692098\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 543.464900\n",
      "\tTesting Loss: 534.544515\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 551.743650\n",
      "\tTesting Loss: 539.568949\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 559.170085\n",
      "\tTesting Loss: 547.734650\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 557.636185\n",
      "\tTesting Loss: 544.598328\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 553.393234\n",
      "\tTesting Loss: 544.065409\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 556.487361\n",
      "\tTesting Loss: 545.793142\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 561.400960\n",
      "\tTesting Loss: 550.896423\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 562.759623\n",
      "\tTesting Loss: 550.669006\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 562.153498\n",
      "\tTesting Loss: 551.366231\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 563.404058\n",
      "\tTesting Loss: 553.318288\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 565.415677\n",
      "\tTesting Loss: 555.227987\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 566.879148\n",
      "\tTesting Loss: 556.394663\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 567.955724\n",
      "\tTesting Loss: 557.062337\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 568.920731\n",
      "\tTesting Loss: 557.125163\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 568.730731\n",
      "\tTesting Loss: 553.582743\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 568.201696\n",
      "\tTesting Loss: 547.520691\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 569.091415\n",
      "\tTesting Loss: 548.054270\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 569.918104\n",
      "\tTesting Loss: 549.980103\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 572.281751\n",
      "\tTesting Loss: 552.790588\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 574.747711\n",
      "\tTesting Loss: 555.710856\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 576.708730\n",
      "\tTesting Loss: 557.834106\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 578.508886\n",
      "\tTesting Loss: 557.656047\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 579.945656\n",
      "\tTesting Loss: 556.533203\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 580.624858\n",
      "\tTesting Loss: 557.238464\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 581.505269\n",
      "\tTesting Loss: 558.843699\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 582.055135\n",
      "\tTesting Loss: 559.030050\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 582.196350\n",
      "\tTesting Loss: 559.883403\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 582.271886\n",
      "\tTesting Loss: 561.032969\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 582.209524\n",
      "\tTesting Loss: 559.674021\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 582.344523\n",
      "\tTesting Loss: 565.515869\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 582.526372\n",
      "\tTesting Loss: 565.452647\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 582.551849\n",
      "\tTesting Loss: 551.073995\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 581.824178\n",
      "\tTesting Loss: 566.777710\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 582.238373\n",
      "\tTesting Loss: 560.135722\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 582.067134\n",
      "\tTesting Loss: 555.678446\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 550.295868\n",
      "\tTesting Loss: 526.954814\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 532.418340\n",
      "\tTesting Loss: 530.289856\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 537.315842\n",
      "\tTesting Loss: 537.542643\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 540.575274\n",
      "\tTesting Loss: 539.743011\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 540.383596\n",
      "\tTesting Loss: 539.834147\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 540.250046\n",
      "\tTesting Loss: 539.801717\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 540.282422\n",
      "\tTesting Loss: 539.823201\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 540.282242\n",
      "\tTesting Loss: 539.806132\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 540.297890\n",
      "\tTesting Loss: 539.811483\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 540.311813\n",
      "\tTesting Loss: 539.811462\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 540.387489\n",
      "\tTesting Loss: 539.843872\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 540.511945\n",
      "\tTesting Loss: 539.937236\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 540.882413\n",
      "\tTesting Loss: 540.186574\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 542.304939\n",
      "\tTesting Loss: 543.438171\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 570.057444\n",
      "\tTesting Loss: 566.855896\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 576.220512\n",
      "\tTesting Loss: 564.422302\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 579.735250\n",
      "\tTesting Loss: 565.680420\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 580.280106\n",
      "\tTesting Loss: 569.053833\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 579.875748\n",
      "\tTesting Loss: 563.897827\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 579.540919\n",
      "\tTesting Loss: 548.460042\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 543.990229\n",
      "\tTesting Loss: 511.000020\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 525.067924\n",
      "\tTesting Loss: 506.740682\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 520.935877\n",
      "\tTesting Loss: 505.756429\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 520.850143\n",
      "\tTesting Loss: 507.699829\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 523.568156\n",
      "\tTesting Loss: 510.462565\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 527.358617\n",
      "\tTesting Loss: 514.796488\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 531.789317\n",
      "\tTesting Loss: 519.812663\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 534.586820\n",
      "\tTesting Loss: 526.481333\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 538.549255\n",
      "\tTesting Loss: 537.647786\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 540.754776\n",
      "\tTesting Loss: 539.835307\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 540.260966\n",
      "\tTesting Loss: 539.754985\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 540.325857\n",
      "\tTesting Loss: 539.764720\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 540.535922\n",
      "\tTesting Loss: 539.852142\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 541.101771\n",
      "\tTesting Loss: 539.842122\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 555.673604\n",
      "\tTesting Loss: 569.877543\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 581.002950\n",
      "\tTesting Loss: 566.306030\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 581.734772\n",
      "\tTesting Loss: 554.336945\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 581.005262\n",
      "\tTesting Loss: 551.608175\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 582.810750\n",
      "\tTesting Loss: 543.511363\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 556.310135\n",
      "\tTesting Loss: 531.079742\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 559.052185\n",
      "\tTesting Loss: 548.729777\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 571.037890\n",
      "\tTesting Loss: 555.166575\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 583.527743\n",
      "\tTesting Loss: 568.186676\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 583.771383\n",
      "\tTesting Loss: 581.947184\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 574.859222\n",
      "\tTesting Loss: 578.599202\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 569.193014\n",
      "\tTesting Loss: 529.242920\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 551.426013\n",
      "\tTesting Loss: 548.889852\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 581.269274\n",
      "\tTesting Loss: 560.069234\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 583.891685\n",
      "\tTesting Loss: 574.247793\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 584.260269\n",
      "\tTesting Loss: 572.829020\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 584.318593\n",
      "\tTesting Loss: 570.144491\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 584.271337\n",
      "\tTesting Loss: 579.271464\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 584.315440\n",
      "\tTesting Loss: 576.881622\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 584.402639\n",
      "\tTesting Loss: 559.290202\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 584.373357\n",
      "\tTesting Loss: 580.706197\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 584.410446\n",
      "\tTesting Loss: 554.918660\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 584.384967\n",
      "\tTesting Loss: 578.402283\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 584.423965\n",
      "\tTesting Loss: 566.697428\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 584.388245\n",
      "\tTesting Loss: 583.035563\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 567.503743\n",
      "\tTesting Loss: 531.127228\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 577.665563\n",
      "\tTesting Loss: 525.687480\n",
      "\tLearning Rate: 0.002541866\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 522.311737\n",
      "\tTesting Loss: 526.624125\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 535.171491\n",
      "\tTesting Loss: 539.040446\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 540.756734\n",
      "\tTesting Loss: 540.039775\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 540.378159\n",
      "\tTesting Loss: 539.929443\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 540.403173\n",
      "\tTesting Loss: 539.905396\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 540.370422\n",
      "\tTesting Loss: 539.904348\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 540.359573\n",
      "\tTesting Loss: 539.901082\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 540.341415\n",
      "\tTesting Loss: 539.899556\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 540.336988\n",
      "\tTesting Loss: 539.896739\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 540.335630\n",
      "\tTesting Loss: 539.895589\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 540.330963\n",
      "\tTesting Loss: 539.893290\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 540.326907\n",
      "\tTesting Loss: 539.890808\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 540.322876\n",
      "\tTesting Loss: 539.887950\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 540.317937\n",
      "\tTesting Loss: 539.884989\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 540.312447\n",
      "\tTesting Loss: 539.881510\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 540.306341\n",
      "\tTesting Loss: 539.877441\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 540.299487\n",
      "\tTesting Loss: 539.873342\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 540.292216\n",
      "\tTesting Loss: 539.868632\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 540.283366\n",
      "\tTesting Loss: 539.861898\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 540.273750\n",
      "\tTesting Loss: 539.855672\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 540.262395\n",
      "\tTesting Loss: 539.846985\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 540.251717\n",
      "\tTesting Loss: 539.840800\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 540.242098\n",
      "\tTesting Loss: 539.834167\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 540.231929\n",
      "\tTesting Loss: 539.826508\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 540.222186\n",
      "\tTesting Loss: 539.820740\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 540.214348\n",
      "\tTesting Loss: 539.815348\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 540.207514\n",
      "\tTesting Loss: 539.811778\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 540.202377\n",
      "\tTesting Loss: 539.808421\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 540.197881\n",
      "\tTesting Loss: 539.805990\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 540.194850\n",
      "\tTesting Loss: 539.804464\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 540.192144\n",
      "\tTesting Loss: 539.802002\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 540.189199\n",
      "\tTesting Loss: 539.800385\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 540.187220\n",
      "\tTesting Loss: 539.799764\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 540.185389\n",
      "\tTesting Loss: 539.797526\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 540.182706\n",
      "\tTesting Loss: 539.795431\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 540.179845\n",
      "\tTesting Loss: 539.793518\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 540.176888\n",
      "\tTesting Loss: 539.790151\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 540.173185\n",
      "\tTesting Loss: 539.787008\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 540.169871\n",
      "\tTesting Loss: 539.785543\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 540.167117\n",
      "\tTesting Loss: 539.781840\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 540.162898\n",
      "\tTesting Loss: 539.778371\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 540.158824\n",
      "\tTesting Loss: 539.774923\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 540.154869\n",
      "\tTesting Loss: 539.771128\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 540.151489\n",
      "\tTesting Loss: 539.767863\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 540.147522\n",
      "\tTesting Loss: 539.764679\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 540.143778\n",
      "\tTesting Loss: 539.761251\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 540.140612\n",
      "\tTesting Loss: 539.758138\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 540.135958\n",
      "\tTesting Loss: 539.752869\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 540.129819\n",
      "\tTesting Loss: 539.747142\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 540.125191\n",
      "\tTesting Loss: 539.743988\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 540.121155\n",
      "\tTesting Loss: 539.739156\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 540.114761\n",
      "\tTesting Loss: 539.732727\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 540.109339\n",
      "\tTesting Loss: 539.726888\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 540.101046\n",
      "\tTesting Loss: 539.717468\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 540.093389\n",
      "\tTesting Loss: 539.709656\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 540.084544\n",
      "\tTesting Loss: 539.700755\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 540.070104\n",
      "\tTesting Loss: 539.681132\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 540.057381\n",
      "\tTesting Loss: 539.673665\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 540.046341\n",
      "\tTesting Loss: 539.652384\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 540.025818\n",
      "\tTesting Loss: 539.629842\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 540.006121\n",
      "\tTesting Loss: 539.601542\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 539.972656\n",
      "\tTesting Loss: 539.561646\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 539.915207\n",
      "\tTesting Loss: 539.479797\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 539.854533\n",
      "\tTesting Loss: 539.421661\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 539.784831\n",
      "\tTesting Loss: 539.311523\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 539.687663\n",
      "\tTesting Loss: 539.159963\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 539.558141\n",
      "\tTesting Loss: 538.918396\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 539.248184\n",
      "\tTesting Loss: 538.457255\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 539.098531\n",
      "\tTesting Loss: 538.353231\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 539.071493\n",
      "\tTesting Loss: 538.697601\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 539.542872\n",
      "\tTesting Loss: 539.214071\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 539.762980\n",
      "\tTesting Loss: 538.981547\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 539.486046\n",
      "\tTesting Loss: 538.546895\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 539.108348\n",
      "\tTesting Loss: 538.611226\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 539.741776\n",
      "\tTesting Loss: 538.859324\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 539.830902\n",
      "\tTesting Loss: 538.890605\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 539.688543\n",
      "\tTesting Loss: 538.712382\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 539.870989\n",
      "\tTesting Loss: 538.932495\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 540.075063\n",
      "\tTesting Loss: 539.000203\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 540.070188\n",
      "\tTesting Loss: 538.918467\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 540.166395\n",
      "\tTesting Loss: 539.147247\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 540.475972\n",
      "\tTesting Loss: 539.249552\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 540.692347\n",
      "\tTesting Loss: 539.323120\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 541.017123\n",
      "\tTesting Loss: 539.570475\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 541.411006\n",
      "\tTesting Loss: 539.415833\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 541.571747\n",
      "\tTesting Loss: 539.441243\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 541.488068\n",
      "\tTesting Loss: 539.583649\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 541.828412\n",
      "\tTesting Loss: 539.283488\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 541.654821\n",
      "\tTesting Loss: 539.543620\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 542.088491\n",
      "\tTesting Loss: 539.287730\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 542.048233\n",
      "\tTesting Loss: 539.397298\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 542.627492\n",
      "\tTesting Loss: 539.270854\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 542.473612\n",
      "\tTesting Loss: 539.294800\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 542.406039\n",
      "\tTesting Loss: 539.419556\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 542.981107\n",
      "\tTesting Loss: 539.722961\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 542.838722\n",
      "\tTesting Loss: 539.431722\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 544.250804\n",
      "\tTesting Loss: 539.362752\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 543.508443\n",
      "\tTesting Loss: 538.700887\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 543.282934\n",
      "\tTesting Loss: 539.179270\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 543.737325\n",
      "\tTesting Loss: 538.873484\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 544.860733\n",
      "\tTesting Loss: 538.817128\n",
      "\tLearning Rate: 0.002287679\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 543.466840\n",
      "\tTesting Loss: 539.565186\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 542.009125\n",
      "\tTesting Loss: 539.274526\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 544.589424\n",
      "\tTesting Loss: 539.518840\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 544.450236\n",
      "\tTesting Loss: 538.244558\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 542.214478\n",
      "\tTesting Loss: 539.898275\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 541.094930\n",
      "\tTesting Loss: 539.341634\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 539.981873\n",
      "\tTesting Loss: 539.437785\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 539.917882\n",
      "\tTesting Loss: 539.340190\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 539.856936\n",
      "\tTesting Loss: 539.158285\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 539.807994\n",
      "\tTesting Loss: 538.798737\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 539.809708\n",
      "\tTesting Loss: 538.261759\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 539.850339\n",
      "\tTesting Loss: 537.629985\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 540.765849\n",
      "\tTesting Loss: 538.169698\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 542.616348\n",
      "\tTesting Loss: 538.903788\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 544.655457\n",
      "\tTesting Loss: 538.409322\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 544.791517\n",
      "\tTesting Loss: 538.261322\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 545.632757\n",
      "\tTesting Loss: 538.360372\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 545.408735\n",
      "\tTesting Loss: 538.272583\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 545.636770\n",
      "\tTesting Loss: 538.346120\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 545.849258\n",
      "\tTesting Loss: 538.177104\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 545.873632\n",
      "\tTesting Loss: 537.938660\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 545.939702\n",
      "\tTesting Loss: 538.159678\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 546.388809\n",
      "\tTesting Loss: 537.984070\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 546.686134\n",
      "\tTesting Loss: 537.791606\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 546.952662\n",
      "\tTesting Loss: 538.057739\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.264473\n",
      "\tTesting Loss: 537.769114\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.494499\n",
      "\tTesting Loss: 538.320679\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.660716\n",
      "\tTesting Loss: 537.884542\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.735433\n",
      "\tTesting Loss: 537.814412\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.742785\n",
      "\tTesting Loss: 537.759338\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.909790\n",
      "\tTesting Loss: 537.908915\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.892255\n",
      "\tTesting Loss: 538.029317\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 548.018931\n",
      "\tTesting Loss: 538.560811\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 548.096029\n",
      "\tTesting Loss: 539.228160\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 548.146434\n",
      "\tTesting Loss: 538.616557\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 548.237030\n",
      "\tTesting Loss: 538.615306\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 548.241473\n",
      "\tTesting Loss: 538.413818\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 548.449956\n",
      "\tTesting Loss: 539.299947\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 548.539945\n",
      "\tTesting Loss: 540.699137\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 548.963966\n",
      "\tTesting Loss: 543.804586\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 548.419228\n",
      "\tTesting Loss: 535.826762\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 541.420481\n",
      "\tTesting Loss: 538.517822\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 539.610362\n",
      "\tTesting Loss: 538.781118\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 540.147339\n",
      "\tTesting Loss: 539.021444\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 541.498667\n",
      "\tTesting Loss: 539.378967\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 544.769778\n",
      "\tTesting Loss: 538.007406\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 544.452423\n",
      "\tTesting Loss: 546.359589\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 543.852361\n",
      "\tTesting Loss: 552.945007\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 546.123334\n",
      "\tTesting Loss: 538.045166\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 540.360013\n",
      "\tTesting Loss: 538.901815\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 540.786494\n",
      "\tTesting Loss: 539.461639\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 542.915207\n",
      "\tTesting Loss: 539.660522\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 545.403374\n",
      "\tTesting Loss: 540.371908\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 548.001984\n",
      "\tTesting Loss: 557.327596\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 543.071604\n",
      "\tTesting Loss: 538.874247\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 540.008667\n",
      "\tTesting Loss: 539.471863\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 540.061493\n",
      "\tTesting Loss: 539.589966\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 540.045929\n",
      "\tTesting Loss: 539.481588\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 540.101575\n",
      "\tTesting Loss: 539.427490\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 540.767835\n",
      "\tTesting Loss: 539.355723\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 542.070002\n",
      "\tTesting Loss: 540.177083\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 545.220683\n",
      "\tTesting Loss: 553.896606\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 545.693192\n",
      "\tTesting Loss: 559.666402\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 544.465998\n",
      "\tTesting Loss: 539.306407\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 540.067805\n",
      "\tTesting Loss: 539.633860\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 540.133998\n",
      "\tTesting Loss: 539.740438\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 540.167989\n",
      "\tTesting Loss: 539.724508\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 540.121241\n",
      "\tTesting Loss: 539.665202\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 540.048869\n",
      "\tTesting Loss: 539.549398\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 540.011378\n",
      "\tTesting Loss: 539.462830\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 540.080564\n",
      "\tTesting Loss: 539.454000\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 540.449397\n",
      "\tTesting Loss: 539.424255\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 541.848104\n",
      "\tTesting Loss: 539.892476\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 541.674169\n",
      "\tTesting Loss: 537.642995\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 545.568736\n",
      "\tTesting Loss: 534.035421\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 541.900065\n",
      "\tTesting Loss: 538.726440\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 540.051303\n",
      "\tTesting Loss: 539.390198\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 540.014704\n",
      "\tTesting Loss: 539.434998\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 540.576146\n",
      "\tTesting Loss: 540.110657\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 543.234823\n",
      "\tTesting Loss: 539.912333\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 546.457349\n",
      "\tTesting Loss: 544.755310\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 549.276159\n",
      "\tTesting Loss: 555.308187\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 552.736282\n",
      "\tTesting Loss: 549.541199\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 556.782593\n",
      "\tTesting Loss: 547.177714\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 555.476672\n",
      "\tTesting Loss: 550.200704\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 559.238373\n",
      "\tTesting Loss: 554.605164\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 562.506439\n",
      "\tTesting Loss: 551.735474\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 559.828072\n",
      "\tTesting Loss: 556.429321\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 564.189377\n",
      "\tTesting Loss: 558.716125\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 566.070974\n",
      "\tTesting Loss: 554.705505\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 564.183929\n",
      "\tTesting Loss: 560.490082\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 566.089025\n",
      "\tTesting Loss: 560.486247\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 567.670041\n",
      "\tTesting Loss: 559.344950\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 568.118375\n",
      "\tTesting Loss: 558.489217\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 567.836619\n",
      "\tTesting Loss: 558.624593\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 567.677073\n",
      "\tTesting Loss: 559.459371\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 567.931091\n",
      "\tTesting Loss: 561.694478\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 569.127907\n",
      "\tTesting Loss: 563.410726\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 570.506185\n",
      "\tTesting Loss: 565.411011\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 570.503306\n",
      "\tTesting Loss: 564.929118\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 573.360097\n",
      "\tTesting Loss: 565.527222\n",
      "\tLearning Rate: 0.002058911\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 573.644958\n",
      "\tTesting Loss: 566.494690\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 573.264460\n",
      "\tTesting Loss: 567.595357\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 573.519089\n",
      "\tTesting Loss: 565.709066\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 576.291346\n",
      "\tTesting Loss: 566.560211\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 577.322955\n",
      "\tTesting Loss: 567.807271\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 577.766866\n",
      "\tTesting Loss: 567.629150\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 577.263466\n",
      "\tTesting Loss: 568.714783\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 577.900091\n",
      "\tTesting Loss: 569.439992\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 578.687225\n",
      "\tTesting Loss: 569.119130\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 579.225365\n",
      "\tTesting Loss: 569.643616\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 579.972893\n",
      "\tTesting Loss: 567.875478\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 580.638194\n",
      "\tTesting Loss: 566.935221\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 579.983678\n",
      "\tTesting Loss: 563.527873\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 558.216207\n",
      "\tTesting Loss: 553.318522\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 573.927704\n",
      "\tTesting Loss: 553.575033\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 575.349594\n",
      "\tTesting Loss: 555.339498\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 575.263784\n",
      "\tTesting Loss: 556.357646\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 576.656250\n",
      "\tTesting Loss: 556.011810\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 573.077670\n",
      "\tTesting Loss: 545.989705\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 558.760292\n",
      "\tTesting Loss: 562.215454\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 579.371572\n",
      "\tTesting Loss: 567.185954\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 580.700643\n",
      "\tTesting Loss: 573.597972\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 581.749865\n",
      "\tTesting Loss: 570.759705\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 577.836533\n",
      "\tTesting Loss: 576.214722\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 577.716143\n",
      "\tTesting Loss: 564.000509\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 580.905894\n",
      "\tTesting Loss: 565.327026\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 558.071630\n",
      "\tTesting Loss: 544.296875\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 568.448603\n",
      "\tTesting Loss: 569.537537\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 574.631444\n",
      "\tTesting Loss: 557.777222\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 580.746152\n",
      "\tTesting Loss: 561.345642\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 548.543828\n",
      "\tTesting Loss: 535.203593\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 546.429738\n",
      "\tTesting Loss: 548.042521\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 580.220622\n",
      "\tTesting Loss: 558.284912\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 554.099116\n",
      "\tTesting Loss: 548.382243\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 581.226873\n",
      "\tTesting Loss: 558.094421\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 546.220146\n",
      "\tTesting Loss: 529.920329\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 538.238469\n",
      "\tTesting Loss: 534.826965\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 561.854731\n",
      "\tTesting Loss: 572.908722\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 570.409017\n",
      "\tTesting Loss: 544.944244\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 576.310575\n",
      "\tTesting Loss: 557.622691\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 545.074814\n",
      "\tTesting Loss: 528.716471\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 533.412710\n",
      "\tTesting Loss: 531.430461\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 539.414256\n",
      "\tTesting Loss: 538.350393\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 540.919497\n",
      "\tTesting Loss: 539.956767\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 541.799886\n",
      "\tTesting Loss: 539.787191\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 540.610046\n",
      "\tTesting Loss: 539.687113\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 540.356837\n",
      "\tTesting Loss: 539.614827\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 540.412676\n",
      "\tTesting Loss: 539.805125\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 541.793976\n",
      "\tTesting Loss: 540.159729\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 545.227300\n",
      "\tTesting Loss: 539.307566\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 545.977631\n",
      "\tTesting Loss: 540.549174\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 545.300913\n",
      "\tTesting Loss: 540.544657\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.464671\n",
      "\tTesting Loss: 539.296071\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 544.366165\n",
      "\tTesting Loss: 540.916463\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 546.649134\n",
      "\tTesting Loss: 544.404205\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 552.273626\n",
      "\tTesting Loss: 553.030874\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 546.658435\n",
      "\tTesting Loss: 539.941305\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.547185\n",
      "\tTesting Loss: 548.388947\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.184199\n",
      "\tTesting Loss: 548.574483\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 548.738749\n",
      "\tTesting Loss: 539.444621\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 546.014488\n",
      "\tTesting Loss: 540.826884\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.104253\n",
      "\tTesting Loss: 539.762594\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 553.589020\n",
      "\tTesting Loss: 543.896790\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 548.614848\n",
      "\tTesting Loss: 548.947632\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 549.710429\n",
      "\tTesting Loss: 557.230265\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.285390\n",
      "\tTesting Loss: 538.871826\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 546.179214\n",
      "\tTesting Loss: 545.154622\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.053518\n",
      "\tTesting Loss: 540.606466\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 560.401059\n",
      "\tTesting Loss: 549.978333\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 546.842995\n",
      "\tTesting Loss: 536.826538\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.897438\n",
      "\tTesting Loss: 557.698720\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 557.050438\n",
      "\tTesting Loss: 551.964671\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.162763\n",
      "\tTesting Loss: 543.463521\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.193265\n",
      "\tTesting Loss: 550.991272\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.916809\n",
      "\tTesting Loss: 541.891225\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 552.559107\n",
      "\tTesting Loss: 541.792582\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 549.753774\n",
      "\tTesting Loss: 549.270060\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 550.043900\n",
      "\tTesting Loss: 557.009735\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 546.554156\n",
      "\tTesting Loss: 537.418620\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 546.560850\n",
      "\tTesting Loss: 550.673808\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 548.121506\n",
      "\tTesting Loss: 544.327769\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 551.089785\n",
      "\tTesting Loss: 553.174723\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 551.224864\n",
      "\tTesting Loss: 555.814819\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.128675\n",
      "\tTesting Loss: 547.399577\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 549.529638\n",
      "\tTesting Loss: 555.587667\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 551.247396\n",
      "\tTesting Loss: 556.089030\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 553.502396\n",
      "\tTesting Loss: 560.984273\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 549.328044\n",
      "\tTesting Loss: 551.957743\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 555.376155\n",
      "\tTesting Loss: 556.221598\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 564.932533\n",
      "\tTesting Loss: 560.325846\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 569.627528\n",
      "\tTesting Loss: 562.969615\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 572.665990\n",
      "\tTesting Loss: 565.981130\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 574.905457\n",
      "\tTesting Loss: 568.551463\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 576.960134\n",
      "\tTesting Loss: 570.679159\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 578.480077\n",
      "\tTesting Loss: 572.735423\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 579.667577\n",
      "\tTesting Loss: 574.039520\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 580.435048\n",
      "\tTesting Loss: 575.014801\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 580.941518\n",
      "\tTesting Loss: 575.685852\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 581.402654\n",
      "\tTesting Loss: 574.718445\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 579.828842\n",
      "\tTesting Loss: 578.107727\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 579.390539\n",
      "\tTesting Loss: 571.559611\n",
      "\tLearning Rate: 0.001853020\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 579.942075\n",
      "\tTesting Loss: 568.692902\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 579.156377\n",
      "\tTesting Loss: 575.020996\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 582.005600\n",
      "\tTesting Loss: 578.012817\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 582.001531\n",
      "\tTesting Loss: 569.169159\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 582.588593\n",
      "\tTesting Loss: 575.827911\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 582.515272\n",
      "\tTesting Loss: 573.754110\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 582.606552\n",
      "\tTesting Loss: 581.140991\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 581.240326\n",
      "\tTesting Loss: 565.345256\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 583.015869\n",
      "\tTesting Loss: 567.687236\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 582.924566\n",
      "\tTesting Loss: 563.775594\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 582.907776\n",
      "\tTesting Loss: 581.290487\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 581.687795\n",
      "\tTesting Loss: 565.839355\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 582.949280\n",
      "\tTesting Loss: 571.834859\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 582.693120\n",
      "\tTesting Loss: 573.759430\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 582.817993\n",
      "\tTesting Loss: 579.654480\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 582.153407\n",
      "\tTesting Loss: 565.049764\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 563.971303\n",
      "\tTesting Loss: 550.429606\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 581.321976\n",
      "\tTesting Loss: 559.417735\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 550.450226\n",
      "\tTesting Loss: 532.685893\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 536.640396\n",
      "\tTesting Loss: 547.851807\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 581.233785\n",
      "\tTesting Loss: 566.361938\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 579.886408\n",
      "\tTesting Loss: 578.480306\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 545.335737\n",
      "\tTesting Loss: 529.717397\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 536.678675\n",
      "\tTesting Loss: 537.401143\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 539.729337\n",
      "\tTesting Loss: 539.582520\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 540.350573\n",
      "\tTesting Loss: 539.837341\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 540.265228\n",
      "\tTesting Loss: 539.788696\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 540.247554\n",
      "\tTesting Loss: 539.805298\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 540.254779\n",
      "\tTesting Loss: 539.787191\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 540.225266\n",
      "\tTesting Loss: 539.757467\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 540.189931\n",
      "\tTesting Loss: 539.724111\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 540.156278\n",
      "\tTesting Loss: 539.697205\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 540.128113\n",
      "\tTesting Loss: 539.670603\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 540.096588\n",
      "\tTesting Loss: 539.641184\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 540.058347\n",
      "\tTesting Loss: 539.608643\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 540.016078\n",
      "\tTesting Loss: 539.571920\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 539.972855\n",
      "\tTesting Loss: 539.532532\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 539.926394\n",
      "\tTesting Loss: 539.488851\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 539.898870\n",
      "\tTesting Loss: 539.468282\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 539.879110\n",
      "\tTesting Loss: 539.443095\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 539.857478\n",
      "\tTesting Loss: 539.413574\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 539.838710\n",
      "\tTesting Loss: 539.388652\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 539.831884\n",
      "\tTesting Loss: 539.368846\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 539.814489\n",
      "\tTesting Loss: 539.332255\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 539.843218\n",
      "\tTesting Loss: 539.355265\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 539.998009\n",
      "\tTesting Loss: 539.518331\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 540.984317\n",
      "\tTesting Loss: 539.779968\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 545.969584\n",
      "\tTesting Loss: 556.392660\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 562.596568\n",
      "\tTesting Loss: 549.154846\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 564.461912\n",
      "\tTesting Loss: 552.923442\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 565.307648\n",
      "\tTesting Loss: 556.205363\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 567.815715\n",
      "\tTesting Loss: 558.317424\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 571.679675\n",
      "\tTesting Loss: 560.549123\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 573.805949\n",
      "\tTesting Loss: 561.691996\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 575.490143\n",
      "\tTesting Loss: 562.692118\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 576.578964\n",
      "\tTesting Loss: 563.118184\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 577.617783\n",
      "\tTesting Loss: 563.095469\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 578.409434\n",
      "\tTesting Loss: 562.602926\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 579.157445\n",
      "\tTesting Loss: 562.221456\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 579.846021\n",
      "\tTesting Loss: 561.927572\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 580.467717\n",
      "\tTesting Loss: 561.926941\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 580.948736\n",
      "\tTesting Loss: 561.916351\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 581.328094\n",
      "\tTesting Loss: 562.014201\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 581.595065\n",
      "\tTesting Loss: 561.822123\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 581.815908\n",
      "\tTesting Loss: 561.632568\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 581.971720\n",
      "\tTesting Loss: 561.292603\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 582.110229\n",
      "\tTesting Loss: 560.998311\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 582.208318\n",
      "\tTesting Loss: 560.662313\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 582.279109\n",
      "\tTesting Loss: 560.330363\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 582.342133\n",
      "\tTesting Loss: 560.020711\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 582.381088\n",
      "\tTesting Loss: 559.717387\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 582.435923\n",
      "\tTesting Loss: 559.486155\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 582.416239\n",
      "\tTesting Loss: 559.346720\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 582.574516\n",
      "\tTesting Loss: 559.251597\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 582.160863\n",
      "\tTesting Loss: 559.950633\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 582.785772\n",
      "\tTesting Loss: 563.327454\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 582.655619\n",
      "\tTesting Loss: 557.831095\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 582.220846\n",
      "\tTesting Loss: 559.185150\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 582.973643\n",
      "\tTesting Loss: 562.979594\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 582.590286\n",
      "\tTesting Loss: 557.243988\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 583.157786\n",
      "\tTesting Loss: 559.458069\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 583.170687\n",
      "\tTesting Loss: 557.883972\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 582.969823\n",
      "\tTesting Loss: 555.129425\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 582.895676\n",
      "\tTesting Loss: 556.097270\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 582.575961\n",
      "\tTesting Loss: 556.380066\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 583.198148\n",
      "\tTesting Loss: 559.392171\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 583.223684\n",
      "\tTesting Loss: 558.325012\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 583.174614\n",
      "\tTesting Loss: 556.159475\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 582.475947\n",
      "\tTesting Loss: 556.318095\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 583.231661\n",
      "\tTesting Loss: 560.261800\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 583.272893\n",
      "\tTesting Loss: 558.182709\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 583.305298\n",
      "\tTesting Loss: 556.418894\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 583.105520\n",
      "\tTesting Loss: 553.482931\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 583.018013\n",
      "\tTesting Loss: 559.065613\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 583.339167\n",
      "\tTesting Loss: 558.670837\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 583.403814\n",
      "\tTesting Loss: 557.789683\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 583.401825\n",
      "\tTesting Loss: 556.606242\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 583.199468\n",
      "\tTesting Loss: 554.257548\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 583.156667\n",
      "\tTesting Loss: 556.242940\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 583.286344\n",
      "\tTesting Loss: 555.079875\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 582.731791\n",
      "\tTesting Loss: 554.933726\n",
      "\tLearning Rate: 0.001667718\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 583.363215\n",
      "\tTesting Loss: 558.729675\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 583.382688\n",
      "\tTesting Loss: 558.092346\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 583.403539\n",
      "\tTesting Loss: 557.232686\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 583.395370\n",
      "\tTesting Loss: 556.430094\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 583.303624\n",
      "\tTesting Loss: 554.787740\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 582.823608\n",
      "\tTesting Loss: 553.686879\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 583.203974\n",
      "\tTesting Loss: 555.305359\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 582.952825\n",
      "\tTesting Loss: 552.871989\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 583.081080\n",
      "\tTesting Loss: 557.790253\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 583.290924\n",
      "\tTesting Loss: 557.790690\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 583.187792\n",
      "\tTesting Loss: 555.417450\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 582.617844\n",
      "\tTesting Loss: 554.996663\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 583.168482\n",
      "\tTesting Loss: 558.400380\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 583.156988\n",
      "\tTesting Loss: 556.942495\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 582.700480\n",
      "\tTesting Loss: 555.293345\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 582.954585\n",
      "\tTesting Loss: 557.276530\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 582.697357\n",
      "\tTesting Loss: 555.674784\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 582.873576\n",
      "\tTesting Loss: 557.966410\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 582.737333\n",
      "\tTesting Loss: 556.127452\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 582.782547\n",
      "\tTesting Loss: 558.342834\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 582.702390\n",
      "\tTesting Loss: 556.763285\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 582.694865\n",
      "\tTesting Loss: 557.948385\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 582.596553\n",
      "\tTesting Loss: 557.276062\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 582.666807\n",
      "\tTesting Loss: 557.846954\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 582.578517\n",
      "\tTesting Loss: 557.583344\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 582.642942\n",
      "\tTesting Loss: 557.903951\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 582.577077\n",
      "\tTesting Loss: 557.788635\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 582.624959\n",
      "\tTesting Loss: 557.995524\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 582.579069\n",
      "\tTesting Loss: 557.945109\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 582.615618\n",
      "\tTesting Loss: 558.100210\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 582.583504\n",
      "\tTesting Loss: 558.079163\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 582.609573\n",
      "\tTesting Loss: 558.187785\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 582.590866\n",
      "\tTesting Loss: 558.189372\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 582.609314\n",
      "\tTesting Loss: 558.253581\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 582.599548\n",
      "\tTesting Loss: 558.259888\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 582.614779\n",
      "\tTesting Loss: 558.310516\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 582.611715\n",
      "\tTesting Loss: 558.317688\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 582.623934\n",
      "\tTesting Loss: 558.349915\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 582.625097\n",
      "\tTesting Loss: 558.349945\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 582.636126\n",
      "\tTesting Loss: 558.370900\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 582.639384\n",
      "\tTesting Loss: 558.374756\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 582.648127\n",
      "\tTesting Loss: 558.390828\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 582.653175\n",
      "\tTesting Loss: 558.403086\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 582.659215\n",
      "\tTesting Loss: 558.411499\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 582.669724\n",
      "\tTesting Loss: 558.418457\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 582.674825\n",
      "\tTesting Loss: 558.422170\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 582.682421\n",
      "\tTesting Loss: 558.421214\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 582.692273\n",
      "\tTesting Loss: 558.423014\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 582.698481\n",
      "\tTesting Loss: 558.418457\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 582.704742\n",
      "\tTesting Loss: 558.412760\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 582.715950\n",
      "\tTesting Loss: 558.404460\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 582.721232\n",
      "\tTesting Loss: 558.407501\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 582.733147\n",
      "\tTesting Loss: 558.391317\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 582.734062\n",
      "\tTesting Loss: 558.388031\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 582.750692\n",
      "\tTesting Loss: 558.379700\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 582.750519\n",
      "\tTesting Loss: 558.372172\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 582.762662\n",
      "\tTesting Loss: 558.360209\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 582.767329\n",
      "\tTesting Loss: 558.357544\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 582.777542\n",
      "\tTesting Loss: 558.347392\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 582.779622\n",
      "\tTesting Loss: 558.342794\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 582.794284\n",
      "\tTesting Loss: 558.332987\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 582.795588\n",
      "\tTesting Loss: 558.334412\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 582.804769\n",
      "\tTesting Loss: 558.331868\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 582.810623\n",
      "\tTesting Loss: 558.333486\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 582.816012\n",
      "\tTesting Loss: 558.329661\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 582.825010\n",
      "\tTesting Loss: 558.322021\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 582.828812\n",
      "\tTesting Loss: 558.324087\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 582.835678\n",
      "\tTesting Loss: 558.317179\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 582.843953\n",
      "\tTesting Loss: 558.312805\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 582.847176\n",
      "\tTesting Loss: 558.313517\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 582.859601\n",
      "\tTesting Loss: 558.301493\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 582.858576\n",
      "\tTesting Loss: 558.300130\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 582.873299\n",
      "\tTesting Loss: 558.296427\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 582.871562\n",
      "\tTesting Loss: 558.302429\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 582.883967\n",
      "\tTesting Loss: 558.290263\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 582.886434\n",
      "\tTesting Loss: 558.295369\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 582.894526\n",
      "\tTesting Loss: 558.283936\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 582.899938\n",
      "\tTesting Loss: 558.287771\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 582.907328\n",
      "\tTesting Loss: 558.279846\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 582.910456\n",
      "\tTesting Loss: 558.280721\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 582.922073\n",
      "\tTesting Loss: 558.271444\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 582.920906\n",
      "\tTesting Loss: 558.277852\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 582.935776\n",
      "\tTesting Loss: 558.263346\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 582.932475\n",
      "\tTesting Loss: 558.273020\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 582.946086\n",
      "\tTesting Loss: 558.256185\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 582.945740\n",
      "\tTesting Loss: 558.265605\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 582.957260\n",
      "\tTesting Loss: 558.255229\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 582.954608\n",
      "\tTesting Loss: 558.266693\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 582.970497\n",
      "\tTesting Loss: 558.249898\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 582.965927\n",
      "\tTesting Loss: 558.262146\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 582.979350\n",
      "\tTesting Loss: 558.242411\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 582.979093\n",
      "\tTesting Loss: 558.250875\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 582.988475\n",
      "\tTesting Loss: 558.233175\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 582.993067\n",
      "\tTesting Loss: 558.236226\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 582.999964\n",
      "\tTesting Loss: 558.228739\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 583.004112\n",
      "\tTesting Loss: 558.235036\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 583.008916\n",
      "\tTesting Loss: 558.230754\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 583.016355\n",
      "\tTesting Loss: 558.219971\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 583.019206\n",
      "\tTesting Loss: 558.223185\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 583.024971\n",
      "\tTesting Loss: 558.217906\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 583.031565\n",
      "\tTesting Loss: 558.214417\n",
      "\tLearning Rate: 0.001500946\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 583.032984\n",
      "\tTesting Loss: 558.222361\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 583.043711\n",
      "\tTesting Loss: 558.200684\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 583.043391\n",
      "\tTesting Loss: 558.207214\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 583.050730\n",
      "\tTesting Loss: 558.199972\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 583.054438\n",
      "\tTesting Loss: 558.202677\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 583.059748\n",
      "\tTesting Loss: 558.200073\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 583.064245\n",
      "\tTesting Loss: 558.196960\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 583.067719\n",
      "\tTesting Loss: 558.195170\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 583.072652\n",
      "\tTesting Loss: 558.190582\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 583.078690\n",
      "\tTesting Loss: 558.186646\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 583.081579\n",
      "\tTesting Loss: 558.185557\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 583.088203\n",
      "\tTesting Loss: 558.180033\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 583.089541\n",
      "\tTesting Loss: 558.185689\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 583.099531\n",
      "\tTesting Loss: 558.169779\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 583.098470\n",
      "\tTesting Loss: 558.180054\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 583.106817\n",
      "\tTesting Loss: 558.158569\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 583.109507\n",
      "\tTesting Loss: 558.167582\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 583.116201\n",
      "\tTesting Loss: 558.149200\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 583.119812\n",
      "\tTesting Loss: 558.152964\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 583.125966\n",
      "\tTesting Loss: 558.143799\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 583.129349\n",
      "\tTesting Loss: 558.143148\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 583.133886\n",
      "\tTesting Loss: 558.136047\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 583.140737\n",
      "\tTesting Loss: 558.125641\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 583.143239\n",
      "\tTesting Loss: 558.124674\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 583.146637\n",
      "\tTesting Loss: 558.126363\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 583.155396\n",
      "\tTesting Loss: 558.113627\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 583.156372\n",
      "\tTesting Loss: 558.107788\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 583.161433\n",
      "\tTesting Loss: 558.099894\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 583.169942\n",
      "\tTesting Loss: 558.083425\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 583.172366\n",
      "\tTesting Loss: 558.075907\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 583.181854\n",
      "\tTesting Loss: 558.055176\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 583.184168\n",
      "\tTesting Loss: 558.058065\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 583.191282\n",
      "\tTesting Loss: 558.036886\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 583.196688\n",
      "\tTesting Loss: 558.030375\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 583.202682\n",
      "\tTesting Loss: 558.015564\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 583.206039\n",
      "\tTesting Loss: 558.014221\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 583.214228\n",
      "\tTesting Loss: 557.997701\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 583.216591\n",
      "\tTesting Loss: 557.992208\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 583.226046\n",
      "\tTesting Loss: 557.969381\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 583.227509\n",
      "\tTesting Loss: 557.974162\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 583.235448\n",
      "\tTesting Loss: 557.947408\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 583.240036\n",
      "\tTesting Loss: 557.945231\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 583.245224\n",
      "\tTesting Loss: 557.935354\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 583.252179\n",
      "\tTesting Loss: 557.926208\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 583.255356\n",
      "\tTesting Loss: 557.918640\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 583.263626\n",
      "\tTesting Loss: 557.898966\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 583.265386\n",
      "\tTesting Loss: 557.906372\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 583.273478\n",
      "\tTesting Loss: 557.880839\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 583.276891\n",
      "\tTesting Loss: 557.879944\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 583.282501\n",
      "\tTesting Loss: 557.865295\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 583.288231\n",
      "\tTesting Loss: 557.855581\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 583.292010\n",
      "\tTesting Loss: 557.849569\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 583.298365\n",
      "\tTesting Loss: 557.841838\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 583.301221\n",
      "\tTesting Loss: 557.838826\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 583.307129\n",
      "\tTesting Loss: 557.825785\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 583.308517\n",
      "\tTesting Loss: 557.838959\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 583.313548\n",
      "\tTesting Loss: 557.825256\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 583.316345\n",
      "\tTesting Loss: 557.835042\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 583.320727\n",
      "\tTesting Loss: 557.821533\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 583.324305\n",
      "\tTesting Loss: 557.822632\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 583.327728\n",
      "\tTesting Loss: 557.819112\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 583.333252\n",
      "\tTesting Loss: 557.811452\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 583.334096\n",
      "\tTesting Loss: 557.828898\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 583.339999\n",
      "\tTesting Loss: 557.810445\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 583.343145\n",
      "\tTesting Loss: 557.815999\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 583.346453\n",
      "\tTesting Loss: 557.813049\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 583.351690\n",
      "\tTesting Loss: 557.809977\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 583.352859\n",
      "\tTesting Loss: 557.817413\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 583.359965\n",
      "\tTesting Loss: 557.806030\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 583.360382\n",
      "\tTesting Loss: 557.813354\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 583.365206\n",
      "\tTesting Loss: 557.814250\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 583.370280\n",
      "\tTesting Loss: 557.807048\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 583.372121\n",
      "\tTesting Loss: 557.804240\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 583.378029\n",
      "\tTesting Loss: 557.797709\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 583.380274\n",
      "\tTesting Loss: 557.805349\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 583.385132\n",
      "\tTesting Loss: 557.788879\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 583.388412\n",
      "\tTesting Loss: 557.796529\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 583.392021\n",
      "\tTesting Loss: 557.788432\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 583.396909\n",
      "\tTesting Loss: 557.781921\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 583.399490\n",
      "\tTesting Loss: 557.782593\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 583.406108\n",
      "\tTesting Loss: 557.765320\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 583.406138\n",
      "\tTesting Loss: 557.786987\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 583.412196\n",
      "\tTesting Loss: 557.766469\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 583.415517\n",
      "\tTesting Loss: 557.770081\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 583.419189\n",
      "\tTesting Loss: 557.759440\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 583.424779\n",
      "\tTesting Loss: 557.745270\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 583.426715\n",
      "\tTesting Loss: 557.751455\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 583.431150\n",
      "\tTesting Loss: 557.741760\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 583.435150\n",
      "\tTesting Loss: 557.730540\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 583.439128\n",
      "\tTesting Loss: 557.726624\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 583.443263\n",
      "\tTesting Loss: 557.722555\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 583.447477\n",
      "\tTesting Loss: 557.709676\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 583.450221\n",
      "\tTesting Loss: 557.703583\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 583.453763\n",
      "\tTesting Loss: 557.707865\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 583.459508\n",
      "\tTesting Loss: 557.684367\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 583.461116\n",
      "\tTesting Loss: 557.692912\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 583.466344\n",
      "\tTesting Loss: 557.673442\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 583.469971\n",
      "\tTesting Loss: 557.671366\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 583.475062\n",
      "\tTesting Loss: 557.647156\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 583.475825\n",
      "\tTesting Loss: 557.665771\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 583.484019\n",
      "\tTesting Loss: 557.618418\n",
      "\tLearning Rate: 0.001350852\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 583.483882\n",
      "\tTesting Loss: 557.645793\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 583.493693\n",
      "\tTesting Loss: 557.593486\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 583.490100\n",
      "\tTesting Loss: 557.622253\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 583.500242\n",
      "\tTesting Loss: 557.581045\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 583.498454\n",
      "\tTesting Loss: 557.588379\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 583.504949\n",
      "\tTesting Loss: 557.571879\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 583.506808\n",
      "\tTesting Loss: 557.572327\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 583.511681\n",
      "\tTesting Loss: 557.552439\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 583.514516\n",
      "\tTesting Loss: 557.547770\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 583.517741\n",
      "\tTesting Loss: 557.539215\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 583.521990\n",
      "\tTesting Loss: 557.520426\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 583.526324\n",
      "\tTesting Loss: 557.503784\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 583.528753\n",
      "\tTesting Loss: 557.501628\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 583.532461\n",
      "\tTesting Loss: 557.483683\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 583.538193\n",
      "\tTesting Loss: 557.455516\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 583.537974\n",
      "\tTesting Loss: 557.479045\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 583.545547\n",
      "\tTesting Loss: 557.436991\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 583.546285\n",
      "\tTesting Loss: 557.432810\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 583.551610\n",
      "\tTesting Loss: 557.408763\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 583.555107\n",
      "\tTesting Loss: 557.403524\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 583.559436\n",
      "\tTesting Loss: 557.379903\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 583.561752\n",
      "\tTesting Loss: 557.366760\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 583.567571\n",
      "\tTesting Loss: 557.345978\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 583.569199\n",
      "\tTesting Loss: 557.339579\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 583.573761\n",
      "\tTesting Loss: 557.313477\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 583.576200\n",
      "\tTesting Loss: 557.315308\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 583.579872\n",
      "\tTesting Loss: 557.291545\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 583.584783\n",
      "\tTesting Loss: 557.266612\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 583.587901\n",
      "\tTesting Loss: 557.247335\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 583.591746\n",
      "\tTesting Loss: 557.224080\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 583.596143\n",
      "\tTesting Loss: 557.203267\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 583.597951\n",
      "\tTesting Loss: 557.196045\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 583.603592\n",
      "\tTesting Loss: 557.173787\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 583.605133\n",
      "\tTesting Loss: 557.165263\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 583.609932\n",
      "\tTesting Loss: 557.130310\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 583.613881\n",
      "\tTesting Loss: 557.112162\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 583.616135\n",
      "\tTesting Loss: 557.105438\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 583.622821\n",
      "\tTesting Loss: 557.060465\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 583.623713\n",
      "\tTesting Loss: 557.050293\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 583.628571\n",
      "\tTesting Loss: 557.026021\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 583.630943\n",
      "\tTesting Loss: 557.020345\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 583.634893\n",
      "\tTesting Loss: 556.986532\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 583.640760\n",
      "\tTesting Loss: 556.942078\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 583.641795\n",
      "\tTesting Loss: 556.947266\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 583.648666\n",
      "\tTesting Loss: 556.888641\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 583.648677\n",
      "\tTesting Loss: 556.896932\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 583.654605\n",
      "\tTesting Loss: 556.868245\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 583.656606\n",
      "\tTesting Loss: 556.850647\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 583.661425\n",
      "\tTesting Loss: 556.811330\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 583.665807\n",
      "\tTesting Loss: 556.783803\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 583.668531\n",
      "\tTesting Loss: 556.776550\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 583.672557\n",
      "\tTesting Loss: 556.738586\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 583.675713\n",
      "\tTesting Loss: 556.723551\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 583.678909\n",
      "\tTesting Loss: 556.702535\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 583.683334\n",
      "\tTesting Loss: 556.672984\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 583.686193\n",
      "\tTesting Loss: 556.653992\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 583.690069\n",
      "\tTesting Loss: 556.618998\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 583.693934\n",
      "\tTesting Loss: 556.602458\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 583.697057\n",
      "\tTesting Loss: 556.574259\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 583.700989\n",
      "\tTesting Loss: 556.548014\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 583.703344\n",
      "\tTesting Loss: 556.542074\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 583.707265\n",
      "\tTesting Loss: 556.508484\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 583.710203\n",
      "\tTesting Loss: 556.490957\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 583.713409\n",
      "\tTesting Loss: 556.465942\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 583.718486\n",
      "\tTesting Loss: 556.416321\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 583.720398\n",
      "\tTesting Loss: 556.414205\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 583.725408\n",
      "\tTesting Loss: 556.371602\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 583.727748\n",
      "\tTesting Loss: 556.352010\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 583.731201\n",
      "\tTesting Loss: 556.327637\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 583.734937\n",
      "\tTesting Loss: 556.304016\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 583.736933\n",
      "\tTesting Loss: 556.298503\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 583.741681\n",
      "\tTesting Loss: 556.243215\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 583.745346\n",
      "\tTesting Loss: 556.217824\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 583.748377\n",
      "\tTesting Loss: 556.192993\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 583.751976\n",
      "\tTesting Loss: 556.171346\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 583.754885\n",
      "\tTesting Loss: 556.156891\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 583.758331\n",
      "\tTesting Loss: 556.132812\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 583.761541\n",
      "\tTesting Loss: 556.117126\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 583.764605\n",
      "\tTesting Loss: 556.088338\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 583.767446\n",
      "\tTesting Loss: 556.079081\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 583.772364\n",
      "\tTesting Loss: 556.038025\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 583.772868\n",
      "\tTesting Loss: 556.058044\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 583.779271\n",
      "\tTesting Loss: 555.985647\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 583.780365\n",
      "\tTesting Loss: 555.984375\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 583.784459\n",
      "\tTesting Loss: 555.967794\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 583.786601\n",
      "\tTesting Loss: 555.964803\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 583.789724\n",
      "\tTesting Loss: 555.941589\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 583.792984\n",
      "\tTesting Loss: 555.918538\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 583.797267\n",
      "\tTesting Loss: 555.883443\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 583.800140\n",
      "\tTesting Loss: 555.862101\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 583.802312\n",
      "\tTesting Loss: 555.869537\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 583.806094\n",
      "\tTesting Loss: 555.828115\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 583.809362\n",
      "\tTesting Loss: 555.798147\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 583.812770\n",
      "\tTesting Loss: 555.781677\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 583.815150\n",
      "\tTesting Loss: 555.772858\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 583.817884\n",
      "\tTesting Loss: 555.763173\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 583.820541\n",
      "\tTesting Loss: 555.751373\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 583.824104\n",
      "\tTesting Loss: 555.713806\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 583.827682\n",
      "\tTesting Loss: 555.686635\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 583.832202\n",
      "\tTesting Loss: 555.640717\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 583.833672\n",
      "\tTesting Loss: 555.655284\n",
      "\tLearning Rate: 0.001215767\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 583.837265\n",
      "\tTesting Loss: 555.631327\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 583.840144\n",
      "\tTesting Loss: 555.607483\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 583.842234\n",
      "\tTesting Loss: 555.610606\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 583.845327\n",
      "\tTesting Loss: 555.582275\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 583.847908\n",
      "\tTesting Loss: 555.573446\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 583.850794\n",
      "\tTesting Loss: 555.552958\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 583.853732\n",
      "\tTesting Loss: 555.535014\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 583.856801\n",
      "\tTesting Loss: 555.514567\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 583.857697\n",
      "\tTesting Loss: 555.535807\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 583.862292\n",
      "\tTesting Loss: 555.492961\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 583.862340\n",
      "\tTesting Loss: 555.516520\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 583.868057\n",
      "\tTesting Loss: 555.469391\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 583.867505\n",
      "\tTesting Loss: 555.495870\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 583.874664\n",
      "\tTesting Loss: 555.434052\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 583.872192\n",
      "\tTesting Loss: 555.477132\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 583.880730\n",
      "\tTesting Loss: 555.416056\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 583.876132\n",
      "\tTesting Loss: 555.478770\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 583.887802\n",
      "\tTesting Loss: 555.394704\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 583.880493\n",
      "\tTesting Loss: 555.475789\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 583.894173\n",
      "\tTesting Loss: 555.396749\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 583.883942\n",
      "\tTesting Loss: 555.482686\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 583.900337\n",
      "\tTesting Loss: 555.407369\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 583.886871\n",
      "\tTesting Loss: 555.512339\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 583.907115\n",
      "\tTesting Loss: 555.451192\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 583.889910\n",
      "\tTesting Loss: 555.539937\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 583.912498\n",
      "\tTesting Loss: 555.522135\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 583.892731\n",
      "\tTesting Loss: 555.546051\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 583.915253\n",
      "\tTesting Loss: 555.580302\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 583.896164\n",
      "\tTesting Loss: 555.542989\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 583.917053\n",
      "\tTesting Loss: 555.593547\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 583.902486\n",
      "\tTesting Loss: 555.468292\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 583.921341\n",
      "\tTesting Loss: 555.472717\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 583.911532\n",
      "\tTesting Loss: 555.382446\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 583.928090\n",
      "\tTesting Loss: 555.360596\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 583.916911\n",
      "\tTesting Loss: 555.377625\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 583.933497\n",
      "\tTesting Loss: 555.354899\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 583.922587\n",
      "\tTesting Loss: 555.314606\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 583.939738\n",
      "\tTesting Loss: 555.296163\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 583.928095\n",
      "\tTesting Loss: 555.289988\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 583.944967\n",
      "\tTesting Loss: 555.272522\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 583.932139\n",
      "\tTesting Loss: 555.280680\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 583.949025\n",
      "\tTesting Loss: 555.267375\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 583.937597\n",
      "\tTesting Loss: 555.221863\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 583.954381\n",
      "\tTesting Loss: 555.209615\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 583.942657\n",
      "\tTesting Loss: 555.210225\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 583.958476\n",
      "\tTesting Loss: 555.201711\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 583.946818\n",
      "\tTesting Loss: 555.198995\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 583.963412\n",
      "\tTesting Loss: 555.194417\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 583.952354\n",
      "\tTesting Loss: 555.150085\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 583.968852\n",
      "\tTesting Loss: 555.146871\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 583.958232\n",
      "\tTesting Loss: 555.130249\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 583.973602\n",
      "\tTesting Loss: 555.123688\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 583.962435\n",
      "\tTesting Loss: 555.131673\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 583.977994\n",
      "\tTesting Loss: 555.126200\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 583.967756\n",
      "\tTesting Loss: 555.098999\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 583.983175\n",
      "\tTesting Loss: 555.090556\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 583.973503\n",
      "\tTesting Loss: 555.075124\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 583.988419\n",
      "\tTesting Loss: 555.071574\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 583.977687\n",
      "\tTesting Loss: 555.080811\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 583.992645\n",
      "\tTesting Loss: 555.064962\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 583.982915\n",
      "\tTesting Loss: 555.054565\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 583.998212\n",
      "\tTesting Loss: 555.048767\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 583.987874\n",
      "\tTesting Loss: 555.040151\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 584.002838\n",
      "\tTesting Loss: 555.037638\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 583.991186\n",
      "\tTesting Loss: 555.062073\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 584.006302\n",
      "\tTesting Loss: 555.061401\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 583.995143\n",
      "\tTesting Loss: 555.039530\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 584.011134\n",
      "\tTesting Loss: 555.047363\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 584.000331\n",
      "\tTesting Loss: 555.045603\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 584.015058\n",
      "\tTesting Loss: 555.057617\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 584.004608\n",
      "\tTesting Loss: 555.033264\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 584.019577\n",
      "\tTesting Loss: 555.040304\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 584.009992\n",
      "\tTesting Loss: 555.036896\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 584.023842\n",
      "\tTesting Loss: 555.046509\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 584.013713\n",
      "\tTesting Loss: 555.035105\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 584.028224\n",
      "\tTesting Loss: 555.042928\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 584.019135\n",
      "\tTesting Loss: 555.024150\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 584.033076\n",
      "\tTesting Loss: 555.022827\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 584.024925\n",
      "\tTesting Loss: 555.010356\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 584.037816\n",
      "\tTesting Loss: 555.006714\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 584.029444\n",
      "\tTesting Loss: 555.011698\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 584.042160\n",
      "\tTesting Loss: 555.006368\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 584.033758\n",
      "\tTesting Loss: 555.020203\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 584.046761\n",
      "\tTesting Loss: 555.012705\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 584.036916\n",
      "\tTesting Loss: 555.039754\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 584.049657\n",
      "\tTesting Loss: 555.040405\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 584.039851\n",
      "\tTesting Loss: 555.043132\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 584.053482\n",
      "\tTesting Loss: 555.050944\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 584.045232\n",
      "\tTesting Loss: 555.042582\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 584.057889\n",
      "\tTesting Loss: 555.051483\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 584.050817\n",
      "\tTesting Loss: 555.052795\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 584.062431\n",
      "\tTesting Loss: 555.058695\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 584.055982\n",
      "\tTesting Loss: 555.073059\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 584.067441\n",
      "\tTesting Loss: 555.078044\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 584.061539\n",
      "\tTesting Loss: 555.093831\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 584.072522\n",
      "\tTesting Loss: 555.093974\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 584.066147\n",
      "\tTesting Loss: 555.113363\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 584.076843\n",
      "\tTesting Loss: 555.115021\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 584.070381\n",
      "\tTesting Loss: 555.136983\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 584.081276\n",
      "\tTesting Loss: 555.137614\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 584.074552\n",
      "\tTesting Loss: 555.155009\n",
      "\tLearning Rate: 0.001094190\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 584.085749\n",
      "\tTesting Loss: 555.163127\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 584.079249\n",
      "\tTesting Loss: 555.172801\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 584.089317\n",
      "\tTesting Loss: 555.188375\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 584.083430\n",
      "\tTesting Loss: 555.188833\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 584.091967\n",
      "\tTesting Loss: 555.204549\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 584.088130\n",
      "\tTesting Loss: 555.211222\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 584.096316\n",
      "\tTesting Loss: 555.219350\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 584.092911\n",
      "\tTesting Loss: 555.228882\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 584.100301\n",
      "\tTesting Loss: 555.237356\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 584.096776\n",
      "\tTesting Loss: 555.255493\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 584.104253\n",
      "\tTesting Loss: 555.264242\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 584.100316\n",
      "\tTesting Loss: 555.281057\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 584.107773\n",
      "\tTesting Loss: 555.287791\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 584.103882\n",
      "\tTesting Loss: 555.305328\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 584.111532\n",
      "\tTesting Loss: 555.309906\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 584.107040\n",
      "\tTesting Loss: 555.328471\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 584.114772\n",
      "\tTesting Loss: 555.336060\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 584.110296\n",
      "\tTesting Loss: 555.354716\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 584.118952\n",
      "\tTesting Loss: 555.362732\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 584.113401\n",
      "\tTesting Loss: 555.377360\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 584.122035\n",
      "\tTesting Loss: 555.381449\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 584.115931\n",
      "\tTesting Loss: 555.397349\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 584.124387\n",
      "\tTesting Loss: 555.406921\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 584.119359\n",
      "\tTesting Loss: 555.430216\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 584.127955\n",
      "\tTesting Loss: 555.443146\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 584.123489\n",
      "\tTesting Loss: 555.461070\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 584.131653\n",
      "\tTesting Loss: 555.471476\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 584.128176\n",
      "\tTesting Loss: 555.489868\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 584.135910\n",
      "\tTesting Loss: 555.497131\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 584.131561\n",
      "\tTesting Loss: 555.512716\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 584.139328\n",
      "\tTesting Loss: 555.520162\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 584.135020\n",
      "\tTesting Loss: 555.539388\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 584.142715\n",
      "\tTesting Loss: 555.547587\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 584.138753\n",
      "\tTesting Loss: 555.565755\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 584.147324\n",
      "\tTesting Loss: 555.570231\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 584.142741\n",
      "\tTesting Loss: 555.588501\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 584.150523\n",
      "\tTesting Loss: 555.591359\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 584.144989\n",
      "\tTesting Loss: 555.608521\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 584.153809\n",
      "\tTesting Loss: 555.623108\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 584.148928\n",
      "\tTesting Loss: 555.643412\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 584.158231\n",
      "\tTesting Loss: 555.657776\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 584.152430\n",
      "\tTesting Loss: 555.670675\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 584.161229\n",
      "\tTesting Loss: 555.682170\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 584.155396\n",
      "\tTesting Loss: 555.697367\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 584.164393\n",
      "\tTesting Loss: 555.710195\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 584.159332\n",
      "\tTesting Loss: 555.723694\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 584.168292\n",
      "\tTesting Loss: 555.737386\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 584.163976\n",
      "\tTesting Loss: 555.755249\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 584.173630\n",
      "\tTesting Loss: 555.766134\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 584.168208\n",
      "\tTesting Loss: 555.778463\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 584.176364\n",
      "\tTesting Loss: 555.788391\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 584.171143\n",
      "\tTesting Loss: 555.797567\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 584.180684\n",
      "\tTesting Loss: 555.814514\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 584.176351\n",
      "\tTesting Loss: 555.825226\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 584.184446\n",
      "\tTesting Loss: 555.826609\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 584.180763\n",
      "\tTesting Loss: 555.833374\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 584.188937\n",
      "\tTesting Loss: 555.810170\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 584.184596\n",
      "\tTesting Loss: 555.832052\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 584.193194\n",
      "\tTesting Loss: 555.790833\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 584.188197\n",
      "\tTesting Loss: 555.870138\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 584.196002\n",
      "\tTesting Loss: 555.819275\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 584.189061\n",
      "\tTesting Loss: 555.933085\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 584.197431\n",
      "\tTesting Loss: 555.889638\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 584.188794\n",
      "\tTesting Loss: 556.002584\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 584.197423\n",
      "\tTesting Loss: 556.032043\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 584.188258\n",
      "\tTesting Loss: 556.008179\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 584.198481\n",
      "\tTesting Loss: 556.228149\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 584.190742\n",
      "\tTesting Loss: 555.989441\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 584.201060\n",
      "\tTesting Loss: 556.308187\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 584.194260\n",
      "\tTesting Loss: 555.969381\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 584.204285\n",
      "\tTesting Loss: 556.259583\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 584.199137\n",
      "\tTesting Loss: 555.961629\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 584.209183\n",
      "\tTesting Loss: 556.257111\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 584.203364\n",
      "\tTesting Loss: 555.940043\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 584.213308\n",
      "\tTesting Loss: 556.171855\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 584.208349\n",
      "\tTesting Loss: 555.939718\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 584.217712\n",
      "\tTesting Loss: 556.177531\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 584.211904\n",
      "\tTesting Loss: 555.944824\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 584.221573\n",
      "\tTesting Loss: 556.153605\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 584.215769\n",
      "\tTesting Loss: 555.996572\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 584.224553\n",
      "\tTesting Loss: 556.219381\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 584.218091\n",
      "\tTesting Loss: 556.102295\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 584.226171\n",
      "\tTesting Loss: 556.489115\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 584.219091\n",
      "\tTesting Loss: 556.215820\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 584.226501\n",
      "\tTesting Loss: 556.323568\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 584.216619\n",
      "\tTesting Loss: 567.692891\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 584.197250\n",
      "\tTesting Loss: 560.122599\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 584.201162\n",
      "\tTesting Loss: 557.094381\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 584.198853\n",
      "\tTesting Loss: 550.098612\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 557.388135\n",
      "\tTesting Loss: 533.549093\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 540.230057\n",
      "\tTesting Loss: 539.893148\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 540.295456\n",
      "\tTesting Loss: 539.924438\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 540.363485\n",
      "\tTesting Loss: 539.860819\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 540.276502\n",
      "\tTesting Loss: 539.800863\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 540.196950\n",
      "\tTesting Loss: 539.776286\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 540.172699\n",
      "\tTesting Loss: 539.754486\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 540.120496\n",
      "\tTesting Loss: 539.698954\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 540.080399\n",
      "\tTesting Loss: 539.704631\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 540.076096\n",
      "\tTesting Loss: 539.655192\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 540.051127\n",
      "\tTesting Loss: 539.653402\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 540.030741\n",
      "\tTesting Loss: 539.605876\n",
      "\tLearning Rate: 0.000984771\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 539.975723\n",
      "\tTesting Loss: 539.513377\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 539.925069\n",
      "\tTesting Loss: 539.500173\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 539.938499\n",
      "\tTesting Loss: 539.414836\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 540.194333\n",
      "\tTesting Loss: 539.631307\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 541.686859\n",
      "\tTesting Loss: 539.436442\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 541.762856\n",
      "\tTesting Loss: 538.829305\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 542.681071\n",
      "\tTesting Loss: 541.474325\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 544.771362\n",
      "\tTesting Loss: 540.664958\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 546.637932\n",
      "\tTesting Loss: 546.556356\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 548.812548\n",
      "\tTesting Loss: 548.758952\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.660538\n",
      "\tTesting Loss: 547.259033\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 544.827642\n",
      "\tTesting Loss: 546.018972\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.958898\n",
      "\tTesting Loss: 547.921916\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 548.699961\n",
      "\tTesting Loss: 548.658427\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 552.287053\n",
      "\tTesting Loss: 549.229370\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 561.983042\n",
      "\tTesting Loss: 558.104696\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 571.172468\n",
      "\tTesting Loss: 562.693563\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 571.270271\n",
      "\tTesting Loss: 558.300344\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 569.532463\n",
      "\tTesting Loss: 552.702667\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 569.895434\n",
      "\tTesting Loss: 554.473694\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 570.701421\n",
      "\tTesting Loss: 555.993825\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 571.757080\n",
      "\tTesting Loss: 556.862661\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 573.174985\n",
      "\tTesting Loss: 558.101471\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 574.349920\n",
      "\tTesting Loss: 559.015808\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 575.205602\n",
      "\tTesting Loss: 559.215210\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 575.838221\n",
      "\tTesting Loss: 559.333720\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 576.547012\n",
      "\tTesting Loss: 559.675985\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 577.061572\n",
      "\tTesting Loss: 560.179993\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 577.688494\n",
      "\tTesting Loss: 560.538635\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 578.373614\n",
      "\tTesting Loss: 560.934011\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 578.878581\n",
      "\tTesting Loss: 561.278839\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 579.295504\n",
      "\tTesting Loss: 561.580719\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 579.662059\n",
      "\tTesting Loss: 561.690145\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 580.007446\n",
      "\tTesting Loss: 561.729940\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 580.347443\n",
      "\tTesting Loss: 561.741852\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 580.678721\n",
      "\tTesting Loss: 561.819377\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 580.989265\n",
      "\tTesting Loss: 561.895386\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 581.278692\n",
      "\tTesting Loss: 561.909790\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 581.578799\n",
      "\tTesting Loss: 561.960124\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 581.871333\n",
      "\tTesting Loss: 561.951274\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 581.960622\n",
      "\tTesting Loss: 562.017517\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 581.002024\n",
      "\tTesting Loss: 559.450317\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 581.382868\n",
      "\tTesting Loss: 569.225250\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 581.869464\n",
      "\tTesting Loss: 571.713582\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 581.735560\n",
      "\tTesting Loss: 568.662710\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 582.409292\n",
      "\tTesting Loss: 570.372050\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 582.168633\n",
      "\tTesting Loss: 571.075704\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 582.726140\n",
      "\tTesting Loss: 571.579061\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 582.259038\n",
      "\tTesting Loss: 572.830444\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 583.013550\n",
      "\tTesting Loss: 572.706635\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 582.454280\n",
      "\tTesting Loss: 574.248789\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 583.257149\n",
      "\tTesting Loss: 573.780528\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 582.471006\n",
      "\tTesting Loss: 574.025391\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 583.502029\n",
      "\tTesting Loss: 574.580750\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 583.127149\n",
      "\tTesting Loss: 578.224264\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 583.405904\n",
      "\tTesting Loss: 574.384288\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 582.598424\n",
      "\tTesting Loss: 577.138692\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 583.642222\n",
      "\tTesting Loss: 573.626668\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 582.783374\n",
      "\tTesting Loss: 576.787974\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 583.793065\n",
      "\tTesting Loss: 556.251709\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 583.878479\n",
      "\tTesting Loss: 567.064962\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 583.823245\n",
      "\tTesting Loss: 567.783915\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 583.837125\n",
      "\tTesting Loss: 564.952881\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 583.868327\n",
      "\tTesting Loss: 562.365092\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 583.886490\n",
      "\tTesting Loss: 561.726115\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 583.890696\n",
      "\tTesting Loss: 562.263560\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 583.891159\n",
      "\tTesting Loss: 562.498983\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 583.894602\n",
      "\tTesting Loss: 562.274007\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 583.900050\n",
      "\tTesting Loss: 562.002421\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 583.905334\n",
      "\tTesting Loss: 561.837708\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 583.910080\n",
      "\tTesting Loss: 561.714071\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 583.914668\n",
      "\tTesting Loss: 561.562500\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 583.919421\n",
      "\tTesting Loss: 561.386007\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 583.924299\n",
      "\tTesting Loss: 561.202779\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 583.929235\n",
      "\tTesting Loss: 561.013743\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 583.934224\n",
      "\tTesting Loss: 560.823832\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 583.939117\n",
      "\tTesting Loss: 560.649862\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 583.943904\n",
      "\tTesting Loss: 560.494080\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 583.948685\n",
      "\tTesting Loss: 560.327891\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 583.953476\n",
      "\tTesting Loss: 560.159424\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 583.958242\n",
      "\tTesting Loss: 559.982534\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 583.962977\n",
      "\tTesting Loss: 559.805491\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 583.967606\n",
      "\tTesting Loss: 559.651642\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 583.972132\n",
      "\tTesting Loss: 559.494517\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 583.976629\n",
      "\tTesting Loss: 559.335632\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 583.981003\n",
      "\tTesting Loss: 559.183675\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 583.985344\n",
      "\tTesting Loss: 559.022369\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 583.989454\n",
      "\tTesting Loss: 558.865906\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 583.993566\n",
      "\tTesting Loss: 558.713013\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 583.997604\n",
      "\tTesting Loss: 558.564240\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 584.001561\n",
      "\tTesting Loss: 558.417582\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 584.005371\n",
      "\tTesting Loss: 558.277201\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 584.009010\n",
      "\tTesting Loss: 558.148519\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 584.012517\n",
      "\tTesting Loss: 558.020396\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 584.015930\n",
      "\tTesting Loss: 557.886719\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 584.019297\n",
      "\tTesting Loss: 557.759562\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 584.022568\n",
      "\tTesting Loss: 557.631104\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 584.025716\n",
      "\tTesting Loss: 557.512563\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 584.028788\n",
      "\tTesting Loss: 557.393717\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 584.031843\n",
      "\tTesting Loss: 557.265584\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 584.034785\n",
      "\tTesting Loss: 557.152049\n",
      "\tLearning Rate: 0.000886294\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 584.037699\n",
      "\tTesting Loss: 557.011353\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 584.040487\n",
      "\tTesting Loss: 556.903870\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 584.043025\n",
      "\tTesting Loss: 556.783752\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 584.045507\n",
      "\tTesting Loss: 556.673035\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 584.047953\n",
      "\tTesting Loss: 556.552165\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 584.050412\n",
      "\tTesting Loss: 556.435079\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 584.052734\n",
      "\tTesting Loss: 556.324259\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 584.055059\n",
      "\tTesting Loss: 556.203654\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 584.057355\n",
      "\tTesting Loss: 556.084106\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 584.059568\n",
      "\tTesting Loss: 555.975494\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 584.061752\n",
      "\tTesting Loss: 555.864217\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 584.063934\n",
      "\tTesting Loss: 555.750295\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 584.066025\n",
      "\tTesting Loss: 555.646464\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 584.068184\n",
      "\tTesting Loss: 555.537211\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 584.070254\n",
      "\tTesting Loss: 555.445190\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 584.072327\n",
      "\tTesting Loss: 555.355001\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 584.074320\n",
      "\tTesting Loss: 555.281901\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 584.076324\n",
      "\tTesting Loss: 555.203623\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 584.078298\n",
      "\tTesting Loss: 555.136464\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 584.080271\n",
      "\tTesting Loss: 555.078674\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 584.082143\n",
      "\tTesting Loss: 555.038788\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 584.084071\n",
      "\tTesting Loss: 554.997131\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 584.085920\n",
      "\tTesting Loss: 554.975057\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 584.087776\n",
      "\tTesting Loss: 554.937836\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 584.089549\n",
      "\tTesting Loss: 554.921824\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 584.091359\n",
      "\tTesting Loss: 554.894023\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 584.093104\n",
      "\tTesting Loss: 554.884074\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 584.094828\n",
      "\tTesting Loss: 554.869486\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 584.096581\n",
      "\tTesting Loss: 554.865957\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 584.098277\n",
      "\tTesting Loss: 554.863403\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 584.099920\n",
      "\tTesting Loss: 554.867188\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 584.101634\n",
      "\tTesting Loss: 554.866760\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 584.103231\n",
      "\tTesting Loss: 554.877930\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 584.104881\n",
      "\tTesting Loss: 554.883179\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 584.106532\n",
      "\tTesting Loss: 554.893799\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 584.108070\n",
      "\tTesting Loss: 554.899333\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 584.109678\n",
      "\tTesting Loss: 554.906962\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 584.111254\n",
      "\tTesting Loss: 554.913452\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 584.112752\n",
      "\tTesting Loss: 554.922852\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 584.114321\n",
      "\tTesting Loss: 554.927175\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 584.115814\n",
      "\tTesting Loss: 554.937022\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 584.117269\n",
      "\tTesting Loss: 554.942403\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 584.118724\n",
      "\tTesting Loss: 554.950073\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 584.120209\n",
      "\tTesting Loss: 554.953990\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 584.121585\n",
      "\tTesting Loss: 554.963959\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 584.123042\n",
      "\tTesting Loss: 554.965902\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 584.124367\n",
      "\tTesting Loss: 554.976888\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 584.125829\n",
      "\tTesting Loss: 554.975301\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 584.127040\n",
      "\tTesting Loss: 554.990662\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 584.128550\n",
      "\tTesting Loss: 554.987040\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 584.129766\n",
      "\tTesting Loss: 555.003815\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 584.131193\n",
      "\tTesting Loss: 555.000671\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 584.132408\n",
      "\tTesting Loss: 555.014374\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 584.133835\n",
      "\tTesting Loss: 555.010508\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 584.134974\n",
      "\tTesting Loss: 555.026347\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 584.136383\n",
      "\tTesting Loss: 555.022054\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 584.137487\n",
      "\tTesting Loss: 555.037354\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 584.138880\n",
      "\tTesting Loss: 555.031860\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 584.139969\n",
      "\tTesting Loss: 555.047770\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 584.141383\n",
      "\tTesting Loss: 555.040894\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 584.142362\n",
      "\tTesting Loss: 555.059550\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 584.143860\n",
      "\tTesting Loss: 555.047729\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 584.144689\n",
      "\tTesting Loss: 555.071920\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 584.146266\n",
      "\tTesting Loss: 555.054443\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 584.147013\n",
      "\tTesting Loss: 555.082204\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 584.148656\n",
      "\tTesting Loss: 555.059977\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 584.149228\n",
      "\tTesting Loss: 555.094482\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 584.151006\n",
      "\tTesting Loss: 555.064657\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 584.151344\n",
      "\tTesting Loss: 555.108154\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 584.153292\n",
      "\tTesting Loss: 555.066579\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 584.153453\n",
      "\tTesting Loss: 555.120097\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 584.155502\n",
      "\tTesting Loss: 555.071340\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 584.155462\n",
      "\tTesting Loss: 555.131063\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 584.157664\n",
      "\tTesting Loss: 555.074788\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 584.157372\n",
      "\tTesting Loss: 555.143656\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 584.159724\n",
      "\tTesting Loss: 555.077230\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 584.159205\n",
      "\tTesting Loss: 555.157013\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 584.161748\n",
      "\tTesting Loss: 555.079142\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 584.161057\n",
      "\tTesting Loss: 555.168254\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 584.163684\n",
      "\tTesting Loss: 555.082499\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 584.162771\n",
      "\tTesting Loss: 555.180796\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 584.165502\n",
      "\tTesting Loss: 555.086304\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 584.164541\n",
      "\tTesting Loss: 555.189941\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 584.167308\n",
      "\tTesting Loss: 555.088867\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 584.166278\n",
      "\tTesting Loss: 555.196899\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 584.169098\n",
      "\tTesting Loss: 555.093160\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 584.167928\n",
      "\tTesting Loss: 555.207987\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 584.170822\n",
      "\tTesting Loss: 555.095968\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 584.169678\n",
      "\tTesting Loss: 555.214193\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 584.172562\n",
      "\tTesting Loss: 555.099996\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 584.171366\n",
      "\tTesting Loss: 555.219442\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 584.174304\n",
      "\tTesting Loss: 555.102386\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 584.173096\n",
      "\tTesting Loss: 555.225515\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 584.176056\n",
      "\tTesting Loss: 555.105693\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 584.174670\n",
      "\tTesting Loss: 555.234477\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 584.177828\n",
      "\tTesting Loss: 555.105103\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 584.176178\n",
      "\tTesting Loss: 555.244263\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 584.179474\n",
      "\tTesting Loss: 555.105570\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 584.177643\n",
      "\tTesting Loss: 555.254883\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 584.180987\n",
      "\tTesting Loss: 555.105845\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 584.179141\n",
      "\tTesting Loss: 555.259521\n",
      "\tLearning Rate: 0.000797664\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 584.182510\n",
      "\tTesting Loss: 555.108683\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 584.180929\n",
      "\tTesting Loss: 555.258240\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 584.183538\n",
      "\tTesting Loss: 555.117137\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 584.183917\n",
      "\tTesting Loss: 555.204508\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 584.185771\n",
      "\tTesting Loss: 555.130829\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 584.186142\n",
      "\tTesting Loss: 555.184937\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 584.187968\n",
      "\tTesting Loss: 555.131602\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 584.188192\n",
      "\tTesting Loss: 555.180695\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 584.190063\n",
      "\tTesting Loss: 555.127340\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 584.189972\n",
      "\tTesting Loss: 555.182699\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 584.192098\n",
      "\tTesting Loss: 555.118896\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 584.191213\n",
      "\tTesting Loss: 555.199605\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 584.193924\n",
      "\tTesting Loss: 555.110708\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 584.191556\n",
      "\tTesting Loss: 555.244314\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 584.194870\n",
      "\tTesting Loss: 555.102519\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 584.192123\n",
      "\tTesting Loss: 555.267904\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 584.195498\n",
      "\tTesting Loss: 555.100260\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 584.193370\n",
      "\tTesting Loss: 555.261515\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 584.196678\n",
      "\tTesting Loss: 555.104533\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 584.194494\n",
      "\tTesting Loss: 555.262990\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 584.197960\n",
      "\tTesting Loss: 555.106700\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 584.195381\n",
      "\tTesting Loss: 555.273112\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 584.199036\n",
      "\tTesting Loss: 555.104106\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 584.196426\n",
      "\tTesting Loss: 555.277039\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 584.200180\n",
      "\tTesting Loss: 555.103007\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 584.197525\n",
      "\tTesting Loss: 555.277781\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 584.201314\n",
      "\tTesting Loss: 555.101206\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 584.198456\n",
      "\tTesting Loss: 555.285482\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 584.202148\n",
      "\tTesting Loss: 555.102641\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 584.199910\n",
      "\tTesting Loss: 555.273987\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 584.203473\n",
      "\tTesting Loss: 555.102661\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 584.201319\n",
      "\tTesting Loss: 555.267537\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 584.204936\n",
      "\tTesting Loss: 555.106038\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 584.202011\n",
      "\tTesting Loss: 555.281230\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 584.205902\n",
      "\tTesting Loss: 555.105062\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 584.202901\n",
      "\tTesting Loss: 555.288086\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 584.206797\n",
      "\tTesting Loss: 555.104085\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 584.204463\n",
      "\tTesting Loss: 555.273977\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 584.208282\n",
      "\tTesting Loss: 555.111471\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 584.205221\n",
      "\tTesting Loss: 555.281901\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 584.209310\n",
      "\tTesting Loss: 555.112172\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 584.205775\n",
      "\tTesting Loss: 555.296611\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 584.210058\n",
      "\tTesting Loss: 555.112732\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 584.206645\n",
      "\tTesting Loss: 555.299154\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 584.210935\n",
      "\tTesting Loss: 555.115743\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 584.207896\n",
      "\tTesting Loss: 555.291361\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 584.212316\n",
      "\tTesting Loss: 555.124349\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 584.208160\n",
      "\tTesting Loss: 555.325958\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 584.212662\n",
      "\tTesting Loss: 555.121226\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 584.210597\n",
      "\tTesting Loss: 555.274109\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 584.214864\n",
      "\tTesting Loss: 555.136617\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 584.210515\n",
      "\tTesting Loss: 555.326945\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 584.215185\n",
      "\tTesting Loss: 555.138753\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 584.211823\n",
      "\tTesting Loss: 555.319214\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 584.216359\n",
      "\tTesting Loss: 555.148560\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 584.212743\n",
      "\tTesting Loss: 555.328400\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 584.217344\n",
      "\tTesting Loss: 555.161865\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 584.213949\n",
      "\tTesting Loss: 555.328247\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 584.218646\n",
      "\tTesting Loss: 555.177246\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 584.213448\n",
      "\tTesting Loss: 555.438375\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 584.218109\n",
      "\tTesting Loss: 555.176046\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 584.216848\n",
      "\tTesting Loss: 555.295878\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 584.221110\n",
      "\tTesting Loss: 555.204142\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 584.216639\n",
      "\tTesting Loss: 555.385091\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 584.221624\n",
      "\tTesting Loss: 555.217265\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 584.216436\n",
      "\tTesting Loss: 555.495422\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 584.221120\n",
      "\tTesting Loss: 555.213643\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 584.219844\n",
      "\tTesting Loss: 555.330465\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 584.223867\n",
      "\tTesting Loss: 555.242879\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 584.219645\n",
      "\tTesting Loss: 555.406535\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 584.224655\n",
      "\tTesting Loss: 555.258830\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 584.218760\n",
      "\tTesting Loss: 555.609965\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 584.223633\n",
      "\tTesting Loss: 555.255432\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 584.222321\n",
      "\tTesting Loss: 555.376221\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 584.226557\n",
      "\tTesting Loss: 555.292175\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 584.222300\n",
      "\tTesting Loss: 555.453583\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 584.227516\n",
      "\tTesting Loss: 555.321136\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 584.221629\n",
      "\tTesting Loss: 555.688416\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 584.226832\n",
      "\tTesting Loss: 555.324605\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 584.224462\n",
      "\tTesting Loss: 555.461507\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 584.229251\n",
      "\tTesting Loss: 555.367167\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 584.224030\n",
      "\tTesting Loss: 555.642181\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 584.229329\n",
      "\tTesting Loss: 555.391479\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 584.224480\n",
      "\tTesting Loss: 555.720154\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 584.229757\n",
      "\tTesting Loss: 555.411774\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 584.224642\n",
      "\tTesting Loss: 555.822611\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 584.229716\n",
      "\tTesting Loss: 555.461772\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 584.225932\n",
      "\tTesting Loss: 555.692749\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 584.231392\n",
      "\tTesting Loss: 555.520650\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 584.224917\n",
      "\tTesting Loss: 556.169434\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 584.230403\n",
      "\tTesting Loss: 555.528259\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 584.228729\n",
      "\tTesting Loss: 555.591227\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 584.233846\n",
      "\tTesting Loss: 555.626567\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 584.224777\n",
      "\tTesting Loss: 556.840078\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 584.230164\n",
      "\tTesting Loss: 555.597005\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 584.231527\n",
      "\tTesting Loss: 555.631938\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 584.235179\n",
      "\tTesting Loss: 555.740295\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 584.232274\n",
      "\tTesting Loss: 555.765635\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 584.238047\n",
      "\tTesting Loss: 555.862732\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 584.222153\n",
      "\tTesting Loss: 559.584737\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 584.223770\n",
      "\tTesting Loss: 555.707662\n",
      "\tLearning Rate: 0.000717898\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 584.236547\n",
      "\tTesting Loss: 555.693970\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 584.214320\n",
      "\tTesting Loss: 562.826640\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 584.210805\n",
      "\tTesting Loss: 558.760193\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 584.228961\n",
      "\tTesting Loss: 555.598785\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 584.227407\n",
      "\tTesting Loss: 557.057007\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 584.231105\n",
      "\tTesting Loss: 555.773570\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 584.234767\n",
      "\tTesting Loss: 555.898824\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 584.236771\n",
      "\tTesting Loss: 556.043722\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 584.239212\n",
      "\tTesting Loss: 556.180003\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 584.241028\n",
      "\tTesting Loss: 556.307576\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 584.242874\n",
      "\tTesting Loss: 556.432556\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 584.244008\n",
      "\tTesting Loss: 556.529083\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 584.245631\n",
      "\tTesting Loss: 556.636220\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 584.245829\n",
      "\tTesting Loss: 556.700439\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 584.248052\n",
      "\tTesting Loss: 556.807332\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 584.244797\n",
      "\tTesting Loss: 556.744548\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 584.249936\n",
      "\tTesting Loss: 556.762848\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 584.210327\n",
      "\tTesting Loss: 569.642232\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 584.204569\n",
      "\tTesting Loss: 562.524994\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 584.230349\n",
      "\tTesting Loss: 556.214294\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 584.242513\n",
      "\tTesting Loss: 556.384399\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 584.234467\n",
      "\tTesting Loss: 558.075745\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 584.239553\n",
      "\tTesting Loss: 556.613251\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 584.245855\n",
      "\tTesting Loss: 556.796427\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 584.244250\n",
      "\tTesting Loss: 556.888489\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 584.249494\n",
      "\tTesting Loss: 557.140778\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 584.243713\n",
      "\tTesting Loss: 556.977112\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 584.249608\n",
      "\tTesting Loss: 557.218140\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 584.241943\n",
      "\tTesting Loss: 557.445496\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 584.246880\n",
      "\tTesting Loss: 557.159281\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 584.246699\n",
      "\tTesting Loss: 557.149862\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 584.250516\n",
      "\tTesting Loss: 557.380809\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 584.246084\n",
      "\tTesting Loss: 557.328206\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 584.251719\n",
      "\tTesting Loss: 557.543152\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 584.235967\n",
      "\tTesting Loss: 561.179901\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 584.238197\n",
      "\tTesting Loss: 557.194784\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 584.249608\n",
      "\tTesting Loss: 557.340434\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 584.228709\n",
      "\tTesting Loss: 564.259369\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 584.229075\n",
      "\tTesting Loss: 558.494283\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 584.245995\n",
      "\tTesting Loss: 557.314840\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 584.231987\n",
      "\tTesting Loss: 562.566833\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 584.233256\n",
      "\tTesting Loss: 557.797811\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 584.246760\n",
      "\tTesting Loss: 557.494141\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 584.237554\n",
      "\tTesting Loss: 560.153158\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 584.241913\n",
      "\tTesting Loss: 557.662404\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 584.249774\n",
      "\tTesting Loss: 557.876394\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 584.246984\n",
      "\tTesting Loss: 557.970805\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 584.252157\n",
      "\tTesting Loss: 558.209493\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 584.249888\n",
      "\tTesting Loss: 558.227336\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 584.254374\n",
      "\tTesting Loss: 558.404104\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 584.248357\n",
      "\tTesting Loss: 558.276164\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 584.253860\n",
      "\tTesting Loss: 558.425598\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 584.243073\n",
      "\tTesting Loss: 559.981506\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 584.248271\n",
      "\tTesting Loss: 558.330658\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 584.250267\n",
      "\tTesting Loss: 558.416656\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 584.252274\n",
      "\tTesting Loss: 558.622172\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 584.252569\n",
      "\tTesting Loss: 558.765076\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 584.255580\n",
      "\tTesting Loss: 558.934682\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 584.253835\n",
      "\tTesting Loss: 559.005188\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 584.258031\n",
      "\tTesting Loss: 559.105876\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 584.243022\n",
      "\tTesting Loss: 562.114583\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 584.248489\n",
      "\tTesting Loss: 558.857483\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 584.253342\n",
      "\tTesting Loss: 558.953796\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 584.250982\n",
      "\tTesting Loss: 559.103455\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 584.255747\n",
      "\tTesting Loss: 559.288879\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 584.247444\n",
      "\tTesting Loss: 559.433014\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 584.254303\n",
      "\tTesting Loss: 559.268422\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 584.238945\n",
      "\tTesting Loss: 563.752797\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 584.244049\n",
      "\tTesting Loss: 559.105998\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 584.252233\n",
      "\tTesting Loss: 559.263062\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 584.245384\n",
      "\tTesting Loss: 559.463114\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 584.253057\n",
      "\tTesting Loss: 559.532979\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 584.244469\n",
      "\tTesting Loss: 560.743744\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 584.251043\n",
      "\tTesting Loss: 559.558309\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 584.248205\n",
      "\tTesting Loss: 559.604116\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 584.253718\n",
      "\tTesting Loss: 559.769165\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 584.243390\n",
      "\tTesting Loss: 561.703786\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 584.250239\n",
      "\tTesting Loss: 559.767883\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 584.248718\n",
      "\tTesting Loss: 559.841695\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 584.253464\n",
      "\tTesting Loss: 560.003174\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 584.246641\n",
      "\tTesting Loss: 560.277242\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 584.253408\n",
      "\tTesting Loss: 560.097839\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 584.237757\n",
      "\tTesting Loss: 565.233785\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 584.243045\n",
      "\tTesting Loss: 559.952413\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 584.250229\n",
      "\tTesting Loss: 560.092672\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 584.245989\n",
      "\tTesting Loss: 560.305715\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 584.253227\n",
      "\tTesting Loss: 560.409912\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 584.235718\n",
      "\tTesting Loss: 566.819743\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 584.241381\n",
      "\tTesting Loss: 560.280670\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 584.250669\n",
      "\tTesting Loss: 560.403605\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 584.242549\n",
      "\tTesting Loss: 561.532349\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 584.250844\n",
      "\tTesting Loss: 560.622599\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 584.244868\n",
      "\tTesting Loss: 561.076742\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 584.251094\n",
      "\tTesting Loss: 560.721883\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 584.243512\n",
      "\tTesting Loss: 561.832601\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 584.250097\n",
      "\tTesting Loss: 560.813527\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 584.244619\n",
      "\tTesting Loss: 561.190928\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 584.250865\n",
      "\tTesting Loss: 560.971842\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 584.239021\n",
      "\tTesting Loss: 564.629100\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 584.245633\n",
      "\tTesting Loss: 560.971903\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 584.247205\n",
      "\tTesting Loss: 561.145833\n",
      "\tLearning Rate: 0.000646108\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 584.250234\n",
      "\tTesting Loss: 561.269938\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 584.249710\n",
      "\tTesting Loss: 561.497477\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 584.253601\n",
      "\tTesting Loss: 561.514425\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 584.240173\n",
      "\tTesting Loss: 565.338684\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 584.245570\n",
      "\tTesting Loss: 561.403341\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 584.248767\n",
      "\tTesting Loss: 561.499023\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 584.247782\n",
      "\tTesting Loss: 561.717855\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 584.251668\n",
      "\tTesting Loss: 561.777507\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 584.242889\n",
      "\tTesting Loss: 562.891703\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 584.249013\n",
      "\tTesting Loss: 561.770355\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 584.240000\n",
      "\tTesting Loss: 563.395203\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 584.245249\n",
      "\tTesting Loss: 561.763357\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 584.243060\n",
      "\tTesting Loss: 562.019694\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 584.247302\n",
      "\tTesting Loss: 562.001953\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 584.238088\n",
      "\tTesting Loss: 564.116038\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 584.244390\n",
      "\tTesting Loss: 562.048045\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 584.242434\n",
      "\tTesting Loss: 562.277893\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 584.246221\n",
      "\tTesting Loss: 562.261007\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 584.236247\n",
      "\tTesting Loss: 564.928019\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 584.242686\n",
      "\tTesting Loss: 562.268575\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 584.242437\n",
      "\tTesting Loss: 562.481049\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 584.245214\n",
      "\tTesting Loss: 562.495026\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 584.241099\n",
      "\tTesting Loss: 562.798848\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 584.246094\n",
      "\tTesting Loss: 562.740194\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 584.180573\n",
      "\tTesting Loss: 562.789663\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 577.618006\n",
      "\tTesting Loss: 553.213440\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 576.971967\n",
      "\tTesting Loss: 580.626241\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 583.474101\n",
      "\tTesting Loss: 571.237305\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 583.998149\n",
      "\tTesting Loss: 565.894328\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 584.023982\n",
      "\tTesting Loss: 578.057536\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 583.997968\n",
      "\tTesting Loss: 578.106303\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 584.082998\n",
      "\tTesting Loss: 574.003967\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 584.130394\n",
      "\tTesting Loss: 571.906026\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 584.148153\n",
      "\tTesting Loss: 571.046865\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 584.157466\n",
      "\tTesting Loss: 570.640096\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 584.164332\n",
      "\tTesting Loss: 570.332194\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 584.170283\n",
      "\tTesting Loss: 570.039805\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 584.175535\n",
      "\tTesting Loss: 569.760427\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 584.180163\n",
      "\tTesting Loss: 569.491160\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 584.184280\n",
      "\tTesting Loss: 569.222921\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 584.188029\n",
      "\tTesting Loss: 568.955424\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 584.191457\n",
      "\tTesting Loss: 568.679698\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 584.194570\n",
      "\tTesting Loss: 568.403920\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 584.197383\n",
      "\tTesting Loss: 568.108571\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 584.199844\n",
      "\tTesting Loss: 567.808360\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 584.202240\n",
      "\tTesting Loss: 567.517131\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 584.204539\n",
      "\tTesting Loss: 567.232208\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 584.206640\n",
      "\tTesting Loss: 566.950704\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 584.208654\n",
      "\tTesting Loss: 566.668884\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 584.210531\n",
      "\tTesting Loss: 566.394836\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 584.212331\n",
      "\tTesting Loss: 566.125478\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 584.213979\n",
      "\tTesting Loss: 565.864807\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 584.215520\n",
      "\tTesting Loss: 565.614950\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 584.216939\n",
      "\tTesting Loss: 565.381582\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 584.218272\n",
      "\tTesting Loss: 565.152659\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 584.219495\n",
      "\tTesting Loss: 564.924947\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 584.220645\n",
      "\tTesting Loss: 564.696899\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 584.221751\n",
      "\tTesting Loss: 564.462565\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 584.222824\n",
      "\tTesting Loss: 564.224131\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 584.223821\n",
      "\tTesting Loss: 563.979919\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 584.224772\n",
      "\tTesting Loss: 563.732493\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 584.225688\n",
      "\tTesting Loss: 563.478048\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 584.226542\n",
      "\tTesting Loss: 563.228231\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 584.227384\n",
      "\tTesting Loss: 562.991964\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 584.228144\n",
      "\tTesting Loss: 562.754272\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 584.228900\n",
      "\tTesting Loss: 562.530965\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 584.229541\n",
      "\tTesting Loss: 562.333608\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 584.230125\n",
      "\tTesting Loss: 562.164388\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 584.230667\n",
      "\tTesting Loss: 562.040639\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 584.231166\n",
      "\tTesting Loss: 561.990997\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 584.231628\n",
      "\tTesting Loss: 562.015981\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 584.232020\n",
      "\tTesting Loss: 562.089132\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 584.232381\n",
      "\tTesting Loss: 562.168009\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 584.232681\n",
      "\tTesting Loss: 562.246480\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 584.232976\n",
      "\tTesting Loss: 562.322673\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 584.233200\n",
      "\tTesting Loss: 562.397909\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 584.233411\n",
      "\tTesting Loss: 562.470256\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 584.233630\n",
      "\tTesting Loss: 562.541921\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 584.233777\n",
      "\tTesting Loss: 562.612162\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 584.233892\n",
      "\tTesting Loss: 562.681203\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 584.233953\n",
      "\tTesting Loss: 562.749919\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 584.233955\n",
      "\tTesting Loss: 562.816040\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 584.233915\n",
      "\tTesting Loss: 562.881022\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 584.233836\n",
      "\tTesting Loss: 562.944895\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 584.233704\n",
      "\tTesting Loss: 563.008403\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 584.233612\n",
      "\tTesting Loss: 563.069010\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 584.233452\n",
      "\tTesting Loss: 563.128764\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 584.233223\n",
      "\tTesting Loss: 563.188029\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 584.233007\n",
      "\tTesting Loss: 563.246724\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 584.232722\n",
      "\tTesting Loss: 563.304433\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 584.232409\n",
      "\tTesting Loss: 563.361318\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 584.232056\n",
      "\tTesting Loss: 563.417053\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 584.231634\n",
      "\tTesting Loss: 563.470947\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 584.231201\n",
      "\tTesting Loss: 563.525289\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 584.230723\n",
      "\tTesting Loss: 563.578705\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 584.230214\n",
      "\tTesting Loss: 563.630219\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 584.229675\n",
      "\tTesting Loss: 563.682454\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 584.229116\n",
      "\tTesting Loss: 563.733093\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 584.228539\n",
      "\tTesting Loss: 563.783712\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 584.227926\n",
      "\tTesting Loss: 563.832723\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 584.227313\n",
      "\tTesting Loss: 563.881561\n",
      "\tLearning Rate: 0.000581497\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 584.226629\n",
      "\tTesting Loss: 563.928833\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 584.225932\n",
      "\tTesting Loss: 563.971334\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 584.225271\n",
      "\tTesting Loss: 564.014567\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 584.224579\n",
      "\tTesting Loss: 564.056152\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 584.223849\n",
      "\tTesting Loss: 564.098287\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 584.223099\n",
      "\tTesting Loss: 564.140177\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 584.222343\n",
      "\tTesting Loss: 564.181396\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 584.221547\n",
      "\tTesting Loss: 564.222697\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 584.220751\n",
      "\tTesting Loss: 564.261698\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 584.219955\n",
      "\tTesting Loss: 564.302409\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 584.219152\n",
      "\tTesting Loss: 564.340576\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 584.218328\n",
      "\tTesting Loss: 564.380269\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 584.217499\n",
      "\tTesting Loss: 564.418233\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 584.216634\n",
      "\tTesting Loss: 564.455851\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 584.215769\n",
      "\tTesting Loss: 564.493540\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 584.214887\n",
      "\tTesting Loss: 564.530233\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 584.213961\n",
      "\tTesting Loss: 564.567871\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 584.213018\n",
      "\tTesting Loss: 564.604533\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 584.212041\n",
      "\tTesting Loss: 564.641388\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 584.211060\n",
      "\tTesting Loss: 564.678284\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 584.210052\n",
      "\tTesting Loss: 564.714701\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 584.209045\n",
      "\tTesting Loss: 564.749532\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 584.208018\n",
      "\tTesting Loss: 564.785126\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 584.206970\n",
      "\tTesting Loss: 564.820374\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 584.205910\n",
      "\tTesting Loss: 564.855896\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 584.204801\n",
      "\tTesting Loss: 564.890147\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 584.203715\n",
      "\tTesting Loss: 564.923686\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 584.202586\n",
      "\tTesting Loss: 564.956950\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 584.201426\n",
      "\tTesting Loss: 564.991964\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 584.200241\n",
      "\tTesting Loss: 565.025452\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 584.199056\n",
      "\tTesting Loss: 565.058207\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 584.197861\n",
      "\tTesting Loss: 565.090942\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 584.196635\n",
      "\tTesting Loss: 565.124674\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 584.195435\n",
      "\tTesting Loss: 565.157074\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 584.194204\n",
      "\tTesting Loss: 565.188985\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 584.192942\n",
      "\tTesting Loss: 565.220164\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 584.191655\n",
      "\tTesting Loss: 565.252197\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 584.190323\n",
      "\tTesting Loss: 565.282908\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 584.188934\n",
      "\tTesting Loss: 565.314168\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 584.187551\n",
      "\tTesting Loss: 565.344737\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 584.186162\n",
      "\tTesting Loss: 565.374695\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 584.184769\n",
      "\tTesting Loss: 565.404053\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 584.183339\n",
      "\tTesting Loss: 565.433553\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 584.181910\n",
      "\tTesting Loss: 565.463094\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 584.180430\n",
      "\tTesting Loss: 565.492106\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 584.178935\n",
      "\tTesting Loss: 565.520447\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 584.177439\n",
      "\tTesting Loss: 565.550232\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 584.175929\n",
      "\tTesting Loss: 565.578227\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 584.174383\n",
      "\tTesting Loss: 565.605225\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 584.172816\n",
      "\tTesting Loss: 565.633484\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 584.171234\n",
      "\tTesting Loss: 565.660828\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 584.169601\n",
      "\tTesting Loss: 565.688090\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 584.167969\n",
      "\tTesting Loss: 565.713613\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 584.166313\n",
      "\tTesting Loss: 565.742655\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 584.164653\n",
      "\tTesting Loss: 565.766703\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 584.162923\n",
      "\tTesting Loss: 565.794342\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 584.161209\n",
      "\tTesting Loss: 565.819885\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 584.159444\n",
      "\tTesting Loss: 565.845866\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 584.157649\n",
      "\tTesting Loss: 565.869883\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 584.155853\n",
      "\tTesting Loss: 565.896230\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 584.154048\n",
      "\tTesting Loss: 565.920339\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 584.152222\n",
      "\tTesting Loss: 565.945719\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 584.150327\n",
      "\tTesting Loss: 565.968811\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 584.148402\n",
      "\tTesting Loss: 565.993693\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 584.146489\n",
      "\tTesting Loss: 566.017090\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 584.144531\n",
      "\tTesting Loss: 566.038045\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 584.142555\n",
      "\tTesting Loss: 566.063019\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 584.140533\n",
      "\tTesting Loss: 566.082072\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 584.138496\n",
      "\tTesting Loss: 566.106618\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 584.136452\n",
      "\tTesting Loss: 566.124969\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 584.134351\n",
      "\tTesting Loss: 566.150736\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 584.132235\n",
      "\tTesting Loss: 566.165680\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 584.130117\n",
      "\tTesting Loss: 566.189819\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 584.127965\n",
      "\tTesting Loss: 566.207214\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 584.125783\n",
      "\tTesting Loss: 566.225159\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 584.123596\n",
      "\tTesting Loss: 566.247091\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 584.121373\n",
      "\tTesting Loss: 566.260386\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 584.119130\n",
      "\tTesting Loss: 566.285065\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 584.116821\n",
      "\tTesting Loss: 566.294444\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 584.114492\n",
      "\tTesting Loss: 566.318156\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 584.112162\n",
      "\tTesting Loss: 566.330526\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 584.109762\n",
      "\tTesting Loss: 566.352966\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 584.107325\n",
      "\tTesting Loss: 566.363037\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 584.104866\n",
      "\tTesting Loss: 566.383606\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 584.102371\n",
      "\tTesting Loss: 566.394267\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 584.099823\n",
      "\tTesting Loss: 566.412821\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 584.097249\n",
      "\tTesting Loss: 566.425069\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 584.094686\n",
      "\tTesting Loss: 566.440735\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 584.092051\n",
      "\tTesting Loss: 566.450806\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 584.089389\n",
      "\tTesting Loss: 566.463318\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 584.086678\n",
      "\tTesting Loss: 566.472473\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 584.083987\n",
      "\tTesting Loss: 566.489960\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 584.081197\n",
      "\tTesting Loss: 566.489644\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 584.078537\n",
      "\tTesting Loss: 566.523417\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 584.075653\n",
      "\tTesting Loss: 566.510050\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 584.072835\n",
      "\tTesting Loss: 566.531148\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 584.070005\n",
      "\tTesting Loss: 566.527832\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 584.067195\n",
      "\tTesting Loss: 566.552958\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 584.064178\n",
      "\tTesting Loss: 566.541087\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 584.061282\n",
      "\tTesting Loss: 566.566610\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 584.058217\n",
      "\tTesting Loss: 566.553335\n",
      "\tLearning Rate: 0.000523348\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 584.055242\n",
      "\tTesting Loss: 566.579061\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 584.052119\n",
      "\tTesting Loss: 566.561951\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 584.049288\n",
      "\tTesting Loss: 566.578532\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 584.046321\n",
      "\tTesting Loss: 566.569550\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 584.043457\n",
      "\tTesting Loss: 566.585592\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 584.040405\n",
      "\tTesting Loss: 566.570882\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 584.037498\n",
      "\tTesting Loss: 566.598124\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 584.034325\n",
      "\tTesting Loss: 566.571655\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 584.031176\n",
      "\tTesting Loss: 566.599060\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 584.027903\n",
      "\tTesting Loss: 566.572968\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 584.024826\n",
      "\tTesting Loss: 566.602722\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 584.021535\n",
      "\tTesting Loss: 566.570424\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 584.018481\n",
      "\tTesting Loss: 566.582886\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 584.015406\n",
      "\tTesting Loss: 566.572917\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 584.012271\n",
      "\tTesting Loss: 566.585368\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 584.008776\n",
      "\tTesting Loss: 566.558777\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 584.005559\n",
      "\tTesting Loss: 566.594838\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 584.001933\n",
      "\tTesting Loss: 566.547872\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 583.998350\n",
      "\tTesting Loss: 566.581136\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 583.994311\n",
      "\tTesting Loss: 566.536875\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 583.990550\n",
      "\tTesting Loss: 566.549927\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 583.986745\n",
      "\tTesting Loss: 566.523458\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 583.983287\n",
      "\tTesting Loss: 566.555044\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 583.979253\n",
      "\tTesting Loss: 566.496643\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 583.975601\n",
      "\tTesting Loss: 566.528605\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 583.971614\n",
      "\tTesting Loss: 566.475138\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 583.967809\n",
      "\tTesting Loss: 566.497874\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 583.963618\n",
      "\tTesting Loss: 566.445302\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 583.959946\n",
      "\tTesting Loss: 566.490977\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 583.955556\n",
      "\tTesting Loss: 566.405121\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 583.951487\n",
      "\tTesting Loss: 566.440552\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 583.947116\n",
      "\tTesting Loss: 566.376638\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 583.943130\n",
      "\tTesting Loss: 566.411784\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 583.938594\n",
      "\tTesting Loss: 566.336222\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 583.934458\n",
      "\tTesting Loss: 566.368591\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 583.929820\n",
      "\tTesting Loss: 566.298543\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 583.925710\n",
      "\tTesting Loss: 566.343262\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 583.920855\n",
      "\tTesting Loss: 566.251790\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 583.916545\n",
      "\tTesting Loss: 566.288951\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 583.911769\n",
      "\tTesting Loss: 566.209920\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 583.907415\n",
      "\tTesting Loss: 566.250590\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 583.902191\n",
      "\tTesting Loss: 566.151388\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 583.897792\n",
      "\tTesting Loss: 566.200083\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 583.892654\n",
      "\tTesting Loss: 566.099915\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 583.888008\n",
      "\tTesting Loss: 566.140737\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 583.882533\n",
      "\tTesting Loss: 566.040090\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 583.877996\n",
      "\tTesting Loss: 566.091207\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 583.872317\n",
      "\tTesting Loss: 565.973033\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 583.867605\n",
      "\tTesting Loss: 566.031036\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 583.861852\n",
      "\tTesting Loss: 565.908793\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 583.857269\n",
      "\tTesting Loss: 565.976613\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 583.851222\n",
      "\tTesting Loss: 565.840271\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 583.846771\n",
      "\tTesting Loss: 565.938721\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 583.840449\n",
      "\tTesting Loss: 565.767822\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 583.835943\n",
      "\tTesting Loss: 565.887767\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 583.829409\n",
      "\tTesting Loss: 565.683543\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 583.824313\n",
      "\tTesting Loss: 565.767507\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 583.818247\n",
      "\tTesting Loss: 565.613251\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 583.813210\n",
      "\tTesting Loss: 565.687297\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 583.806381\n",
      "\tTesting Loss: 565.516683\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 583.801580\n",
      "\tTesting Loss: 565.617696\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 583.794210\n",
      "\tTesting Loss: 565.410604\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 583.789569\n",
      "\tTesting Loss: 565.549733\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 583.781705\n",
      "\tTesting Loss: 565.311635\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 583.777456\n",
      "\tTesting Loss: 565.501567\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 583.769061\n",
      "\tTesting Loss: 565.198263\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 583.764582\n",
      "\tTesting Loss: 565.422607\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 583.756134\n",
      "\tTesting Loss: 565.076619\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 583.750458\n",
      "\tTesting Loss: 565.227539\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 583.742874\n",
      "\tTesting Loss: 564.970032\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 583.737940\n",
      "\tTesting Loss: 565.107585\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 583.728668\n",
      "\tTesting Loss: 564.828033\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 583.725171\n",
      "\tTesting Loss: 565.065511\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 583.714399\n",
      "\tTesting Loss: 564.686788\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 583.712657\n",
      "\tTesting Loss: 565.114522\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 583.700277\n",
      "\tTesting Loss: 564.554891\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 583.698039\n",
      "\tTesting Loss: 565.064901\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 583.685303\n",
      "\tTesting Loss: 564.421224\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 583.681519\n",
      "\tTesting Loss: 564.891164\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 583.669645\n",
      "\tTesting Loss: 564.285421\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 583.665222\n",
      "\tTesting Loss: 564.645630\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 583.653041\n",
      "\tTesting Loss: 564.109772\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 583.649658\n",
      "\tTesting Loss: 564.438822\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 583.635928\n",
      "\tTesting Loss: 563.903056\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 583.634781\n",
      "\tTesting Loss: 564.313802\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 583.618398\n",
      "\tTesting Loss: 563.742452\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 583.622075\n",
      "\tTesting Loss: 564.450928\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 583.601156\n",
      "\tTesting Loss: 563.578674\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 583.606333\n",
      "\tTesting Loss: 564.403341\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 583.584422\n",
      "\tTesting Loss: 563.390849\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 583.589839\n",
      "\tTesting Loss: 564.298859\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 583.566615\n",
      "\tTesting Loss: 563.209585\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 583.571386\n",
      "\tTesting Loss: 563.996765\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 583.547620\n",
      "\tTesting Loss: 563.023570\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 583.555072\n",
      "\tTesting Loss: 563.784322\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 583.527283\n",
      "\tTesting Loss: 562.853780\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 583.538956\n",
      "\tTesting Loss: 563.651774\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 583.507243\n",
      "\tTesting Loss: 562.666300\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 583.520772\n",
      "\tTesting Loss: 563.405273\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 583.487760\n",
      "\tTesting Loss: 562.471130\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 583.503761\n",
      "\tTesting Loss: 563.142110\n",
      "\tLearning Rate: 0.000471013\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 583.466316\n",
      "\tTesting Loss: 562.276957\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 583.484975\n",
      "\tTesting Loss: 562.899028\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 583.447105\n",
      "\tTesting Loss: 561.980275\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 583.466380\n",
      "\tTesting Loss: 562.613322\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 583.425674\n",
      "\tTesting Loss: 561.758057\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 583.447006\n",
      "\tTesting Loss: 562.234375\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 583.403483\n",
      "\tTesting Loss: 561.510508\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 583.426468\n",
      "\tTesting Loss: 561.743734\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 583.379771\n",
      "\tTesting Loss: 561.202087\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 583.401993\n",
      "\tTesting Loss: 561.150859\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 583.354357\n",
      "\tTesting Loss: 560.830963\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 583.371618\n",
      "\tTesting Loss: 560.389526\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 583.328064\n",
      "\tTesting Loss: 560.285563\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 583.335383\n",
      "\tTesting Loss: 559.619954\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 583.304303\n",
      "\tTesting Loss: 559.501953\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 583.294851\n",
      "\tTesting Loss: 559.059926\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 583.280909\n",
      "\tTesting Loss: 558.648051\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 583.262731\n",
      "\tTesting Loss: 558.376587\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 583.245372\n",
      "\tTesting Loss: 558.149231\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 583.231532\n",
      "\tTesting Loss: 557.868764\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 583.216141\n",
      "\tTesting Loss: 557.555745\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 583.199150\n",
      "\tTesting Loss: 557.402995\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 583.181081\n",
      "\tTesting Loss: 557.330088\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 583.163569\n",
      "\tTesting Loss: 557.288656\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 583.146319\n",
      "\tTesting Loss: 557.243825\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 583.129845\n",
      "\tTesting Loss: 557.200663\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 583.113017\n",
      "\tTesting Loss: 557.141012\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 583.096695\n",
      "\tTesting Loss: 557.032532\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 583.077710\n",
      "\tTesting Loss: 556.913808\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 583.055155\n",
      "\tTesting Loss: 556.788269\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 583.030680\n",
      "\tTesting Loss: 556.671163\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 583.007907\n",
      "\tTesting Loss: 556.596202\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 582.984940\n",
      "\tTesting Loss: 556.556580\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 582.961306\n",
      "\tTesting Loss: 556.564718\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 582.940959\n",
      "\tTesting Loss: 556.705485\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 582.929932\n",
      "\tTesting Loss: 556.778585\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 582.913798\n",
      "\tTesting Loss: 556.898661\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 582.906611\n",
      "\tTesting Loss: 557.201518\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 582.906380\n",
      "\tTesting Loss: 557.231018\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 582.996989\n",
      "\tTesting Loss: 556.538432\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 582.906886\n",
      "\tTesting Loss: 563.387889\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 582.901672\n",
      "\tTesting Loss: 557.945079\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 582.899272\n",
      "\tTesting Loss: 557.265045\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 582.947749\n",
      "\tTesting Loss: 558.438578\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 582.816724\n",
      "\tTesting Loss: 559.491933\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 582.799474\n",
      "\tTesting Loss: 558.494344\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 582.774017\n",
      "\tTesting Loss: 557.653544\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 582.746490\n",
      "\tTesting Loss: 557.437724\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 582.718953\n",
      "\tTesting Loss: 557.325907\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 582.696754\n",
      "\tTesting Loss: 557.222738\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 582.671733\n",
      "\tTesting Loss: 557.072144\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 582.641347\n",
      "\tTesting Loss: 556.899801\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 582.611643\n",
      "\tTesting Loss: 556.713826\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 582.576594\n",
      "\tTesting Loss: 556.532277\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 582.543727\n",
      "\tTesting Loss: 556.360392\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 582.500679\n",
      "\tTesting Loss: 556.218414\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 582.468051\n",
      "\tTesting Loss: 556.072713\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 582.422078\n",
      "\tTesting Loss: 555.946035\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 582.388524\n",
      "\tTesting Loss: 555.833394\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 582.349419\n",
      "\tTesting Loss: 555.763082\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 582.307899\n",
      "\tTesting Loss: 555.678548\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 582.277865\n",
      "\tTesting Loss: 555.654989\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 582.206156\n",
      "\tTesting Loss: 555.652445\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 582.240738\n",
      "\tTesting Loss: 555.302450\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 582.173398\n",
      "\tTesting Loss: 563.153015\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 582.390317\n",
      "\tTesting Loss: 557.511810\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 582.232841\n",
      "\tTesting Loss: 556.552073\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 582.286814\n",
      "\tTesting Loss: 556.671611\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 582.232752\n",
      "\tTesting Loss: 559.400787\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 582.216947\n",
      "\tTesting Loss: 568.922323\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 582.364978\n",
      "\tTesting Loss: 562.607157\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 582.284134\n",
      "\tTesting Loss: 558.488851\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 581.136642\n",
      "\tTesting Loss: 556.190145\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 550.523982\n",
      "\tTesting Loss: 540.486979\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 540.719429\n",
      "\tTesting Loss: 540.011332\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 540.397491\n",
      "\tTesting Loss: 539.889181\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 540.360336\n",
      "\tTesting Loss: 539.889791\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 540.363574\n",
      "\tTesting Loss: 539.887288\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 540.353338\n",
      "\tTesting Loss: 539.884074\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 540.341883\n",
      "\tTesting Loss: 539.878184\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 540.330877\n",
      "\tTesting Loss: 539.868937\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 540.321470\n",
      "\tTesting Loss: 539.863464\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 540.312101\n",
      "\tTesting Loss: 539.858948\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 540.302363\n",
      "\tTesting Loss: 539.853841\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 540.291758\n",
      "\tTesting Loss: 539.848551\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 540.278676\n",
      "\tTesting Loss: 539.842000\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 540.263250\n",
      "\tTesting Loss: 539.835073\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 540.246007\n",
      "\tTesting Loss: 539.829651\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 540.227364\n",
      "\tTesting Loss: 539.827362\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 540.207756\n",
      "\tTesting Loss: 539.823690\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 540.194628\n",
      "\tTesting Loss: 539.826182\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 540.186223\n",
      "\tTesting Loss: 539.833171\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 540.181269\n",
      "\tTesting Loss: 539.839905\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 540.178655\n",
      "\tTesting Loss: 539.849508\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 540.180585\n",
      "\tTesting Loss: 539.863068\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 540.187917\n",
      "\tTesting Loss: 539.887217\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 540.212250\n",
      "\tTesting Loss: 539.927022\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 540.276629\n",
      "\tTesting Loss: 539.997823\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 540.471603\n",
      "\tTesting Loss: 540.064524\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 540.788142\n",
      "\tTesting Loss: 540.144236\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 541.351077\n",
      "\tTesting Loss: 540.285380\n",
      "\tLearning Rate: 0.000423912\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 541.985016\n",
      "\tTesting Loss: 540.348918\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 542.628077\n",
      "\tTesting Loss: 540.530375\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 543.209791\n",
      "\tTesting Loss: 540.552124\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 543.578237\n",
      "\tTesting Loss: 540.756805\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 543.906751\n",
      "\tTesting Loss: 540.596700\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 543.841672\n",
      "\tTesting Loss: 541.105967\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 544.359726\n",
      "\tTesting Loss: 540.604136\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 544.022484\n",
      "\tTesting Loss: 541.291667\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 544.761363\n",
      "\tTesting Loss: 540.861247\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 544.565341\n",
      "\tTesting Loss: 541.352285\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 545.159698\n",
      "\tTesting Loss: 541.263804\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 545.327316\n",
      "\tTesting Loss: 541.358215\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 545.469007\n",
      "\tTesting Loss: 541.382812\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 545.574519\n",
      "\tTesting Loss: 541.425863\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 545.612859\n",
      "\tTesting Loss: 541.441447\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 545.684385\n",
      "\tTesting Loss: 541.478302\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 545.750310\n",
      "\tTesting Loss: 541.485067\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 545.803421\n",
      "\tTesting Loss: 541.514231\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 545.861949\n",
      "\tTesting Loss: 541.520172\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 545.910746\n",
      "\tTesting Loss: 541.541158\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 545.992638\n",
      "\tTesting Loss: 541.550700\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 546.108459\n",
      "\tTesting Loss: 541.575378\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 546.191533\n",
      "\tTesting Loss: 541.579915\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 546.305728\n",
      "\tTesting Loss: 541.592845\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 546.444758\n",
      "\tTesting Loss: 541.614237\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 546.541066\n",
      "\tTesting Loss: 541.621287\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 546.690674\n",
      "\tTesting Loss: 541.638387\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 546.885104\n",
      "\tTesting Loss: 541.638123\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.041824\n",
      "\tTesting Loss: 541.653880\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.231578\n",
      "\tTesting Loss: 541.728312\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.411636\n",
      "\tTesting Loss: 541.767242\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.594040\n",
      "\tTesting Loss: 542.270142\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.769012\n",
      "\tTesting Loss: 542.109477\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.877853\n",
      "\tTesting Loss: 543.057739\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 548.113632\n",
      "\tTesting Loss: 542.635234\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 548.156336\n",
      "\tTesting Loss: 543.645050\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 548.339022\n",
      "\tTesting Loss: 543.031779\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 548.372706\n",
      "\tTesting Loss: 544.193919\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 548.620051\n",
      "\tTesting Loss: 543.688527\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 548.640147\n",
      "\tTesting Loss: 544.747701\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 548.831314\n",
      "\tTesting Loss: 544.177389\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 548.841611\n",
      "\tTesting Loss: 545.061178\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 548.992383\n",
      "\tTesting Loss: 544.495097\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 549.001437\n",
      "\tTesting Loss: 545.521108\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 549.126328\n",
      "\tTesting Loss: 544.843435\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 549.135854\n",
      "\tTesting Loss: 545.943726\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 549.293849\n",
      "\tTesting Loss: 545.425995\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 549.270526\n",
      "\tTesting Loss: 546.206085\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 549.395233\n",
      "\tTesting Loss: 545.790049\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 549.395457\n",
      "\tTesting Loss: 546.461812\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 549.526469\n",
      "\tTesting Loss: 546.419718\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 549.538671\n",
      "\tTesting Loss: 546.635905\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 549.614677\n",
      "\tTesting Loss: 546.887146\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 549.681562\n",
      "\tTesting Loss: 546.901632\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 549.681427\n",
      "\tTesting Loss: 546.984172\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 549.687787\n",
      "\tTesting Loss: 547.041128\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 549.716026\n",
      "\tTesting Loss: 547.030477\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 549.716370\n",
      "\tTesting Loss: 547.167623\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 549.747874\n",
      "\tTesting Loss: 547.092957\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 549.752322\n",
      "\tTesting Loss: 547.293559\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 549.782552\n",
      "\tTesting Loss: 547.224904\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 549.773570\n",
      "\tTesting Loss: 547.311808\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 549.779566\n",
      "\tTesting Loss: 547.247681\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 549.800097\n",
      "\tTesting Loss: 547.336853\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 549.807050\n",
      "\tTesting Loss: 547.218781\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 549.790187\n",
      "\tTesting Loss: 547.117513\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 549.804921\n",
      "\tTesting Loss: 547.201609\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 549.830765\n",
      "\tTesting Loss: 547.086161\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 549.801450\n",
      "\tTesting Loss: 546.999939\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 549.833880\n",
      "\tTesting Loss: 547.105001\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 549.843262\n",
      "\tTesting Loss: 546.881388\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 549.834808\n",
      "\tTesting Loss: 546.975026\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 549.854711\n",
      "\tTesting Loss: 546.905965\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 549.867147\n",
      "\tTesting Loss: 546.921122\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 549.859179\n",
      "\tTesting Loss: 546.785645\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 549.844195\n",
      "\tTesting Loss: 546.673747\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 549.837090\n",
      "\tTesting Loss: 546.721639\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 549.822861\n",
      "\tTesting Loss: 546.566284\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 549.820600\n",
      "\tTesting Loss: 546.750977\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 549.834709\n",
      "\tTesting Loss: 546.717163\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 549.838781\n",
      "\tTesting Loss: 546.774546\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 549.852549\n",
      "\tTesting Loss: 546.814168\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 549.850093\n",
      "\tTesting Loss: 546.821543\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 549.910512\n",
      "\tTesting Loss: 547.064260\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 549.927409\n",
      "\tTesting Loss: 546.790405\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 549.916545\n",
      "\tTesting Loss: 547.026428\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 549.962868\n",
      "\tTesting Loss: 546.953451\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 549.949626\n",
      "\tTesting Loss: 546.967163\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 549.984085\n",
      "\tTesting Loss: 546.987813\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 550.023356\n",
      "\tTesting Loss: 547.103556\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 550.037104\n",
      "\tTesting Loss: 547.064494\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 550.070028\n",
      "\tTesting Loss: 547.125997\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 550.085805\n",
      "\tTesting Loss: 547.188477\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 550.091975\n",
      "\tTesting Loss: 547.136475\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 550.126684\n",
      "\tTesting Loss: 547.239014\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 550.133820\n",
      "\tTesting Loss: 547.205607\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 550.150538\n",
      "\tTesting Loss: 547.266439\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 550.175555\n",
      "\tTesting Loss: 547.288310\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 550.204989\n",
      "\tTesting Loss: 547.354421\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 550.212466\n",
      "\tTesting Loss: 547.420919\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 550.263769\n",
      "\tTesting Loss: 547.554270\n",
      "\tLearning Rate: 0.000381520\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 550.260213\n",
      "\tTesting Loss: 547.426554\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 550.286865\n",
      "\tTesting Loss: 547.703817\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 550.339864\n",
      "\tTesting Loss: 547.708293\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 550.348618\n",
      "\tTesting Loss: 547.654724\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 550.339300\n",
      "\tTesting Loss: 547.763367\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 550.371066\n",
      "\tTesting Loss: 547.809977\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 550.390437\n",
      "\tTesting Loss: 547.846558\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 550.422539\n",
      "\tTesting Loss: 547.950104\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 550.457479\n",
      "\tTesting Loss: 547.939321\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 550.481822\n",
      "\tTesting Loss: 548.097412\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 550.519979\n",
      "\tTesting Loss: 548.067708\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 550.518196\n",
      "\tTesting Loss: 548.119161\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 550.564957\n",
      "\tTesting Loss: 548.239726\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 550.578201\n",
      "\tTesting Loss: 548.281158\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 550.627940\n",
      "\tTesting Loss: 548.338470\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 550.633052\n",
      "\tTesting Loss: 548.368083\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 550.651759\n",
      "\tTesting Loss: 548.443288\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 550.687948\n",
      "\tTesting Loss: 548.535787\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 550.711611\n",
      "\tTesting Loss: 548.553548\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 550.729177\n",
      "\tTesting Loss: 548.655497\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 550.756271\n",
      "\tTesting Loss: 548.626668\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 550.785314\n",
      "\tTesting Loss: 548.837260\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 550.830963\n",
      "\tTesting Loss: 548.816223\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 550.832367\n",
      "\tTesting Loss: 548.935486\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 550.882940\n",
      "\tTesting Loss: 548.896037\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 550.881584\n",
      "\tTesting Loss: 549.076192\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 550.935959\n",
      "\tTesting Loss: 548.976135\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 550.937892\n",
      "\tTesting Loss: 549.190592\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 551.001734\n",
      "\tTesting Loss: 549.029093\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 550.985758\n",
      "\tTesting Loss: 549.170898\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 551.013311\n",
      "\tTesting Loss: 549.363851\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 551.071805\n",
      "\tTesting Loss: 549.240580\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 551.085299\n",
      "\tTesting Loss: 549.467387\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 551.125997\n",
      "\tTesting Loss: 549.556702\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 551.163668\n",
      "\tTesting Loss: 549.489756\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 551.201602\n",
      "\tTesting Loss: 549.677226\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 551.201050\n",
      "\tTesting Loss: 549.788411\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 551.266177\n",
      "\tTesting Loss: 549.702271\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 551.271085\n",
      "\tTesting Loss: 549.924754\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 551.306274\n",
      "\tTesting Loss: 549.919820\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 551.380066\n",
      "\tTesting Loss: 550.092539\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 551.369171\n",
      "\tTesting Loss: 550.304301\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 551.439087\n",
      "\tTesting Loss: 550.072113\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 551.432927\n",
      "\tTesting Loss: 550.731069\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 551.504148\n",
      "\tTesting Loss: 550.442159\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 551.519119\n",
      "\tTesting Loss: 550.782420\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 551.519366\n",
      "\tTesting Loss: 551.026408\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 551.646706\n",
      "\tTesting Loss: 551.394470\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 551.638295\n",
      "\tTesting Loss: 551.641479\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 551.748271\n",
      "\tTesting Loss: 551.663981\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 551.712423\n",
      "\tTesting Loss: 552.260468\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 551.906481\n",
      "\tTesting Loss: 552.669067\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 551.913338\n",
      "\tTesting Loss: 553.020772\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 552.084162\n",
      "\tTesting Loss: 553.045349\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 552.106166\n",
      "\tTesting Loss: 553.972260\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 552.344335\n",
      "\tTesting Loss: 553.453715\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 552.456739\n",
      "\tTesting Loss: 554.242177\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 552.622630\n",
      "\tTesting Loss: 554.160441\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 552.790833\n",
      "\tTesting Loss: 553.523051\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 552.726746\n",
      "\tTesting Loss: 554.766205\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 553.063258\n",
      "\tTesting Loss: 553.100382\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 552.731018\n",
      "\tTesting Loss: 555.131755\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 553.215892\n",
      "\tTesting Loss: 552.782735\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 552.895533\n",
      "\tTesting Loss: 554.493683\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 553.121086\n",
      "\tTesting Loss: 553.820455\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 553.040281\n",
      "\tTesting Loss: 554.398610\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 553.153740\n",
      "\tTesting Loss: 553.896851\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 553.140231\n",
      "\tTesting Loss: 554.135895\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 553.160924\n",
      "\tTesting Loss: 554.338949\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 553.227371\n",
      "\tTesting Loss: 554.263234\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 553.279795\n",
      "\tTesting Loss: 553.830760\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 553.190923\n",
      "\tTesting Loss: 554.944641\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 553.419474\n",
      "\tTesting Loss: 553.714945\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 553.289543\n",
      "\tTesting Loss: 553.879018\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 553.161438\n",
      "\tTesting Loss: 555.168915\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 553.665883\n",
      "\tTesting Loss: 553.485270\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 553.353559\n",
      "\tTesting Loss: 555.241923\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 553.795751\n",
      "\tTesting Loss: 554.087555\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 553.598078\n",
      "\tTesting Loss: 553.875712\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 553.494860\n",
      "\tTesting Loss: 555.117208\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 553.835459\n",
      "\tTesting Loss: 555.085073\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 554.107356\n",
      "\tTesting Loss: 554.290365\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 553.856715\n",
      "\tTesting Loss: 554.643758\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 553.979993\n",
      "\tTesting Loss: 554.580048\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 554.005707\n",
      "\tTesting Loss: 555.612101\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 554.559217\n",
      "\tTesting Loss: 555.194743\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 554.440435\n",
      "\tTesting Loss: 555.252726\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 554.513278\n",
      "\tTesting Loss: 554.789581\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 554.388056\n",
      "\tTesting Loss: 555.546224\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 554.689326\n",
      "\tTesting Loss: 555.730570\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 554.863093\n",
      "\tTesting Loss: 556.121908\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 555.146022\n",
      "\tTesting Loss: 555.382894\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 554.853414\n",
      "\tTesting Loss: 555.617116\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 554.930201\n",
      "\tTesting Loss: 555.383077\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 554.829778\n",
      "\tTesting Loss: 556.106781\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 555.187429\n",
      "\tTesting Loss: 555.951457\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 555.187472\n",
      "\tTesting Loss: 556.302450\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 555.349843\n",
      "\tTesting Loss: 555.699382\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 555.119659\n",
      "\tTesting Loss: 556.213664\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 555.365072\n",
      "\tTesting Loss: 555.917297\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 555.174622\n",
      "\tTesting Loss: 556.433187\n",
      "\tLearning Rate: 0.000343368\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 555.449165\n",
      "\tTesting Loss: 556.103495\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 555.368098\n",
      "\tTesting Loss: 557.038635\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 555.896322\n",
      "\tTesting Loss: 556.509521\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 555.571747\n",
      "\tTesting Loss: 556.474019\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 555.575902\n",
      "\tTesting Loss: 556.056122\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 555.357758\n",
      "\tTesting Loss: 556.614278\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 555.633036\n",
      "\tTesting Loss: 556.357066\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 555.510691\n",
      "\tTesting Loss: 557.018148\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 555.955577\n",
      "\tTesting Loss: 556.363139\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 555.659215\n",
      "\tTesting Loss: 556.905843\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 555.916733\n",
      "\tTesting Loss: 556.617320\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 555.725942\n",
      "\tTesting Loss: 557.035563\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 555.969650\n",
      "\tTesting Loss: 556.565796\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 555.740046\n",
      "\tTesting Loss: 556.988708\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 555.979734\n",
      "\tTesting Loss: 556.476644\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 555.671789\n",
      "\tTesting Loss: 557.014333\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 555.973709\n",
      "\tTesting Loss: 556.560750\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 555.793393\n",
      "\tTesting Loss: 557.030965\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 556.053843\n",
      "\tTesting Loss: 556.688894\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 555.914848\n",
      "\tTesting Loss: 557.317586\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 556.249898\n",
      "\tTesting Loss: 556.774048\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 555.939578\n",
      "\tTesting Loss: 557.145671\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 556.161725\n",
      "\tTesting Loss: 556.498820\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 555.897072\n",
      "\tTesting Loss: 556.926819\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 556.048406\n",
      "\tTesting Loss: 557.257507\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 556.287460\n",
      "\tTesting Loss: 557.133626\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 556.224416\n",
      "\tTesting Loss: 557.281148\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 556.400444\n",
      "\tTesting Loss: 556.782308\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 556.075984\n",
      "\tTesting Loss: 556.834473\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 556.093513\n",
      "\tTesting Loss: 556.717611\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 556.030467\n",
      "\tTesting Loss: 557.325378\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 556.364599\n",
      "\tTesting Loss: 557.073588\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 556.343491\n",
      "\tTesting Loss: 557.496969\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 556.593694\n",
      "\tTesting Loss: 556.788818\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 556.167297\n",
      "\tTesting Loss: 557.328247\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 556.437419\n",
      "\tTesting Loss: 556.811361\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 556.192663\n",
      "\tTesting Loss: 557.390727\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 556.525040\n",
      "\tTesting Loss: 557.085470\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 556.390401\n",
      "\tTesting Loss: 557.681661\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 556.727430\n",
      "\tTesting Loss: 557.128906\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 556.483358\n",
      "\tTesting Loss: 557.782735\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 556.834284\n",
      "\tTesting Loss: 557.344737\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 556.582619\n",
      "\tTesting Loss: 557.727926\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 556.821910\n",
      "\tTesting Loss: 557.333252\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 556.627886\n",
      "\tTesting Loss: 557.745605\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 556.899796\n",
      "\tTesting Loss: 557.363383\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 556.753543\n",
      "\tTesting Loss: 557.734690\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 556.933055\n",
      "\tTesting Loss: 557.203746\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 556.553375\n",
      "\tTesting Loss: 557.955953\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 557.008456\n",
      "\tTesting Loss: 557.557821\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 556.851690\n",
      "\tTesting Loss: 557.910441\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 557.182159\n",
      "\tTesting Loss: 557.277639\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 556.877001\n",
      "\tTesting Loss: 557.710948\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 557.076401\n",
      "\tTesting Loss: 557.421387\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 556.811887\n",
      "\tTesting Loss: 558.269572\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 557.371597\n",
      "\tTesting Loss: 557.750366\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 557.140678\n",
      "\tTesting Loss: 558.303721\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 557.574855\n",
      "\tTesting Loss: 557.280334\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 556.995204\n",
      "\tTesting Loss: 557.510803\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 557.149244\n",
      "\tTesting Loss: 557.111715\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 556.864721\n",
      "\tTesting Loss: 558.116374\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 557.405492\n",
      "\tTesting Loss: 557.883942\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 557.307617\n",
      "\tTesting Loss: 558.552439\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 557.883779\n",
      "\tTesting Loss: 557.582255\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 557.334162\n",
      "\tTesting Loss: 557.697266\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 557.420156\n",
      "\tTesting Loss: 556.971781\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 556.988851\n",
      "\tTesting Loss: 557.905762\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 557.541972\n",
      "\tTesting Loss: 557.531311\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 557.339714\n",
      "\tTesting Loss: 558.248505\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 557.833247\n",
      "\tTesting Loss: 557.773031\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 557.481501\n",
      "\tTesting Loss: 558.127747\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 557.771502\n",
      "\tTesting Loss: 557.626139\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 557.541944\n",
      "\tTesting Loss: 557.789530\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 557.712341\n",
      "\tTesting Loss: 557.414998\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 557.532806\n",
      "\tTesting Loss: 557.935160\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 557.805934\n",
      "\tTesting Loss: 557.877462\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 557.780029\n",
      "\tTesting Loss: 558.350545\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 558.163704\n",
      "\tTesting Loss: 558.005798\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 558.000659\n",
      "\tTesting Loss: 558.222270\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 558.198415\n",
      "\tTesting Loss: 557.114339\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 557.498024\n",
      "\tTesting Loss: 557.711894\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 557.797862\n",
      "\tTesting Loss: 557.340271\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 557.659602\n",
      "\tTesting Loss: 558.154541\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 558.217333\n",
      "\tTesting Loss: 558.157003\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 558.247546\n",
      "\tTesting Loss: 558.754211\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 558.747055\n",
      "\tTesting Loss: 557.921600\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 558.229813\n",
      "\tTesting Loss: 557.739726\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 558.209577\n",
      "\tTesting Loss: 557.064128\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 557.837227\n",
      "\tTesting Loss: 558.363953\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 558.560201\n",
      "\tTesting Loss: 558.293233\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 558.592934\n",
      "\tTesting Loss: 558.565308\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 558.895368\n",
      "\tTesting Loss: 557.728292\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 558.517309\n",
      "\tTesting Loss: 556.842529\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 558.072342\n",
      "\tTesting Loss: 558.115295\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 558.441015\n",
      "\tTesting Loss: 558.551778\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 558.825317\n",
      "\tTesting Loss: 557.847188\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 558.775777\n",
      "\tTesting Loss: 557.107493\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 558.445264\n",
      "\tTesting Loss: 558.356628\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 558.942909\n",
      "\tTesting Loss: 558.032878\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 558.846588\n",
      "\tTesting Loss: 558.460185\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 559.190308\n",
      "\tTesting Loss: 557.762573\n",
      "\tLearning Rate: 0.000309032\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 558.829402\n",
      "\tTesting Loss: 558.302429\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 559.071078\n",
      "\tTesting Loss: 558.470154\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 559.279277\n",
      "\tTesting Loss: 558.235870\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 559.299075\n",
      "\tTesting Loss: 558.835938\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 559.651204\n",
      "\tTesting Loss: 558.573079\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 559.490840\n",
      "\tTesting Loss: 558.904215\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 559.734823\n",
      "\tTesting Loss: 558.725871\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 559.637088\n",
      "\tTesting Loss: 558.776337\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 559.772369\n",
      "\tTesting Loss: 558.958232\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 559.796761\n",
      "\tTesting Loss: 559.172414\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 559.983429\n",
      "\tTesting Loss: 559.120850\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 559.950335\n",
      "\tTesting Loss: 559.093750\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 560.091258\n",
      "\tTesting Loss: 558.905172\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 559.880870\n",
      "\tTesting Loss: 558.870789\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 559.931325\n",
      "\tTesting Loss: 559.138947\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 559.995687\n",
      "\tTesting Loss: 559.600647\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 560.300674\n",
      "\tTesting Loss: 559.589793\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 560.289698\n",
      "\tTesting Loss: 559.384054\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 560.300092\n",
      "\tTesting Loss: 559.641622\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 560.327087\n",
      "\tTesting Loss: 559.652649\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 560.413417\n",
      "\tTesting Loss: 559.772441\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 560.455144\n",
      "\tTesting Loss: 559.786682\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 560.608604\n",
      "\tTesting Loss: 559.715169\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 560.490285\n",
      "\tTesting Loss: 559.560974\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 560.467463\n",
      "\tTesting Loss: 559.858042\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 560.578509\n",
      "\tTesting Loss: 560.128235\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 560.746361\n",
      "\tTesting Loss: 560.144674\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 560.631594\n",
      "\tTesting Loss: 560.668376\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 560.946213\n",
      "\tTesting Loss: 560.651103\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 560.787262\n",
      "\tTesting Loss: 560.457011\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 560.899236\n",
      "\tTesting Loss: 559.970825\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 560.701747\n",
      "\tTesting Loss: 560.236532\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 560.896993\n",
      "\tTesting Loss: 560.537272\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 560.773941\n",
      "\tTesting Loss: 561.433472\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 561.200562\n",
      "\tTesting Loss: 561.450175\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 561.011744\n",
      "\tTesting Loss: 561.325612\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 561.106962\n",
      "\tTesting Loss: 560.894175\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 560.972326\n",
      "\tTesting Loss: 560.906453\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 561.070577\n",
      "\tTesting Loss: 560.972473\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 560.955643\n",
      "\tTesting Loss: 561.154582\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 561.098780\n",
      "\tTesting Loss: 561.466654\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 561.072820\n",
      "\tTesting Loss: 561.624125\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 561.139308\n",
      "\tTesting Loss: 562.087077\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 561.254410\n",
      "\tTesting Loss: 562.091593\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 561.388412\n",
      "\tTesting Loss: 561.549032\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 561.151357\n",
      "\tTesting Loss: 561.111186\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 561.098424\n",
      "\tTesting Loss: 561.123128\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 561.123617\n",
      "\tTesting Loss: 561.021200\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 561.062754\n",
      "\tTesting Loss: 561.169108\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 561.104190\n",
      "\tTesting Loss: 561.744812\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 561.335271\n",
      "\tTesting Loss: 562.538717\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 561.546773\n",
      "\tTesting Loss: 563.259623\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 561.707189\n",
      "\tTesting Loss: 563.348185\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 561.704775\n",
      "\tTesting Loss: 562.853292\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 561.568868\n",
      "\tTesting Loss: 562.358887\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 561.469330\n",
      "\tTesting Loss: 561.881042\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 561.427684\n",
      "\tTesting Loss: 561.615519\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 561.368121\n",
      "\tTesting Loss: 561.571798\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 561.347605\n",
      "\tTesting Loss: 561.859884\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 561.372307\n",
      "\tTesting Loss: 562.460124\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 561.545642\n",
      "\tTesting Loss: 563.419840\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 561.755587\n",
      "\tTesting Loss: 564.148407\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 561.873929\n",
      "\tTesting Loss: 563.969859\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 561.846949\n",
      "\tTesting Loss: 563.621134\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 561.801272\n",
      "\tTesting Loss: 563.417013\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 561.787155\n",
      "\tTesting Loss: 563.234924\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 561.755206\n",
      "\tTesting Loss: 562.993815\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 561.719060\n",
      "\tTesting Loss: 562.920369\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 561.713847\n",
      "\tTesting Loss: 563.029968\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 561.699117\n",
      "\tTesting Loss: 563.398966\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 561.817281\n",
      "\tTesting Loss: 563.924428\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 561.867706\n",
      "\tTesting Loss: 564.695994\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 562.038361\n",
      "\tTesting Loss: 564.846619\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 562.037593\n",
      "\tTesting Loss: 564.794444\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 562.112528\n",
      "\tTesting Loss: 564.472005\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 561.957876\n",
      "\tTesting Loss: 564.157806\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 561.941010\n",
      "\tTesting Loss: 563.538879\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 561.755788\n",
      "\tTesting Loss: 563.257599\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 561.808281\n",
      "\tTesting Loss: 563.022013\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 561.715378\n",
      "\tTesting Loss: 562.834951\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 561.685669\n",
      "\tTesting Loss: 563.005819\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 561.669062\n",
      "\tTesting Loss: 564.306244\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 561.941874\n",
      "\tTesting Loss: 565.616445\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 562.130051\n",
      "\tTesting Loss: 566.612020\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 562.370021\n",
      "\tTesting Loss: 566.578125\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 562.391154\n",
      "\tTesting Loss: 565.940979\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 562.366793\n",
      "\tTesting Loss: 565.127594\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 562.147003\n",
      "\tTesting Loss: 564.346924\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 561.953044\n",
      "\tTesting Loss: 563.473206\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 561.853505\n",
      "\tTesting Loss: 562.744883\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 561.778493\n",
      "\tTesting Loss: 561.874573\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 561.495504\n",
      "\tTesting Loss: 562.026886\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 561.474706\n",
      "\tTesting Loss: 564.162913\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 561.724533\n",
      "\tTesting Loss: 566.720113\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 562.200528\n",
      "\tTesting Loss: 567.716939\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 562.548548\n",
      "\tTesting Loss: 567.343160\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 562.791326\n",
      "\tTesting Loss: 566.312276\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 562.583074\n",
      "\tTesting Loss: 565.585958\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 562.417379\n",
      "\tTesting Loss: 565.084747\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 562.292623\n",
      "\tTesting Loss: 564.776235\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 562.154989\n",
      "\tTesting Loss: 565.059469\n",
      "\tLearning Rate: 0.000278128\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 562.192215\n",
      "\tTesting Loss: 565.457530\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 562.225937\n",
      "\tTesting Loss: 566.614461\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 562.451385\n",
      "\tTesting Loss: 567.063985\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 562.610072\n",
      "\tTesting Loss: 566.907572\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 562.707225\n",
      "\tTesting Loss: 566.435435\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 562.616913\n",
      "\tTesting Loss: 565.990987\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 562.520365\n",
      "\tTesting Loss: 565.580587\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 562.370290\n",
      "\tTesting Loss: 565.497762\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 562.322225\n",
      "\tTesting Loss: 565.524272\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 562.260432\n",
      "\tTesting Loss: 565.933167\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 562.371363\n",
      "\tTesting Loss: 566.439819\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 562.442067\n",
      "\tTesting Loss: 566.943329\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 562.602010\n",
      "\tTesting Loss: 567.061808\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 562.673925\n",
      "\tTesting Loss: 566.998108\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 562.731959\n",
      "\tTesting Loss: 566.707499\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 562.658483\n",
      "\tTesting Loss: 566.183004\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 562.500788\n",
      "\tTesting Loss: 565.554688\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 562.339050\n",
      "\tTesting Loss: 565.119161\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 562.213287\n",
      "\tTesting Loss: 564.574849\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 562.094493\n",
      "\tTesting Loss: 564.295166\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 562.013860\n",
      "\tTesting Loss: 564.440643\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 562.017731\n",
      "\tTesting Loss: 565.754649\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 562.260058\n",
      "\tTesting Loss: 567.464742\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 562.565295\n",
      "\tTesting Loss: 568.349141\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 562.883789\n",
      "\tTesting Loss: 568.317647\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 563.137761\n",
      "\tTesting Loss: 567.928080\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 563.153458\n",
      "\tTesting Loss: 567.129466\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 562.939997\n",
      "\tTesting Loss: 566.456055\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 562.734866\n",
      "\tTesting Loss: 566.268453\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 562.644124\n",
      "\tTesting Loss: 566.078725\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 562.478963\n",
      "\tTesting Loss: 566.142487\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 562.485504\n",
      "\tTesting Loss: 566.328339\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 562.458511\n",
      "\tTesting Loss: 566.654724\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 562.567062\n",
      "\tTesting Loss: 567.041209\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 562.594528\n",
      "\tTesting Loss: 567.279694\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 562.703328\n",
      "\tTesting Loss: 567.488403\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 562.775441\n",
      "\tTesting Loss: 567.643911\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 562.822149\n",
      "\tTesting Loss: 567.626068\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 562.852440\n",
      "\tTesting Loss: 567.459412\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 562.875799\n",
      "\tTesting Loss: 567.107096\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 562.754105\n",
      "\tTesting Loss: 566.599691\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 562.625086\n",
      "\tTesting Loss: 565.852509\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 562.388247\n",
      "\tTesting Loss: 565.018626\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 562.183787\n",
      "\tTesting Loss: 564.007711\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 561.975962\n",
      "\tTesting Loss: 563.527954\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 562.038335\n",
      "\tTesting Loss: 561.981079\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 561.907349\n",
      "\tTesting Loss: 560.438751\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 561.690893\n",
      "\tTesting Loss: 559.389089\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 561.443077\n",
      "\tTesting Loss: 562.265462\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 560.651733\n",
      "\tTesting Loss: 568.983917\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 562.087407\n",
      "\tTesting Loss: 569.378713\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 563.356018\n",
      "\tTesting Loss: 568.110494\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 563.574033\n",
      "\tTesting Loss: 567.783793\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 563.312597\n",
      "\tTesting Loss: 568.272990\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 563.249252\n",
      "\tTesting Loss: 568.232300\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 563.196452\n",
      "\tTesting Loss: 567.885101\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 563.170069\n",
      "\tTesting Loss: 567.669800\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 563.140076\n",
      "\tTesting Loss: 567.568868\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 563.070089\n",
      "\tTesting Loss: 567.551819\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 563.005641\n",
      "\tTesting Loss: 567.579193\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 563.004313\n",
      "\tTesting Loss: 567.524495\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 563.017209\n",
      "\tTesting Loss: 567.199941\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 562.935366\n",
      "\tTesting Loss: 566.923035\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 562.854950\n",
      "\tTesting Loss: 566.777527\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 562.785090\n",
      "\tTesting Loss: 566.702983\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 562.753021\n",
      "\tTesting Loss: 566.897766\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 562.778664\n",
      "\tTesting Loss: 566.948283\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 562.766983\n",
      "\tTesting Loss: 567.012441\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 562.789515\n",
      "\tTesting Loss: 566.977926\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 562.793869\n",
      "\tTesting Loss: 566.826436\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 562.765808\n",
      "\tTesting Loss: 566.582896\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 562.711657\n",
      "\tTesting Loss: 566.326955\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 562.646739\n",
      "\tTesting Loss: 566.090434\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 562.578629\n",
      "\tTesting Loss: 565.916178\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 562.504496\n",
      "\tTesting Loss: 565.819275\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 562.446734\n",
      "\tTesting Loss: 565.960510\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 562.466838\n",
      "\tTesting Loss: 566.283264\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 562.481664\n",
      "\tTesting Loss: 566.587931\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 562.551593\n",
      "\tTesting Loss: 566.990641\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 562.700165\n",
      "\tTesting Loss: 567.117391\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 562.782051\n",
      "\tTesting Loss: 567.095612\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 562.858292\n",
      "\tTesting Loss: 567.061259\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 562.893684\n",
      "\tTesting Loss: 566.600382\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 562.832347\n",
      "\tTesting Loss: 566.280863\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 562.796249\n",
      "\tTesting Loss: 565.380859\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 562.552689\n",
      "\tTesting Loss: 564.811310\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 562.481766\n",
      "\tTesting Loss: 563.882324\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 562.202352\n",
      "\tTesting Loss: 563.706706\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 562.366135\n",
      "\tTesting Loss: 562.143656\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 561.860463\n",
      "\tTesting Loss: 562.044851\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 562.419973\n",
      "\tTesting Loss: 561.154277\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 561.466123\n",
      "\tTesting Loss: 557.355733\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 561.082728\n",
      "\tTesting Loss: 564.730927\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 562.133570\n",
      "\tTesting Loss: 568.698893\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 562.900792\n",
      "\tTesting Loss: 569.743164\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 563.856750\n",
      "\tTesting Loss: 569.051432\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 564.204664\n",
      "\tTesting Loss: 568.312276\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 563.868871\n",
      "\tTesting Loss: 567.166748\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 563.314326\n",
      "\tTesting Loss: 567.289408\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 563.209920\n",
      "\tTesting Loss: 567.496908\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 563.192571\n",
      "\tTesting Loss: 567.551412\n",
      "\tLearning Rate: 0.000250316\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 563.297775\n",
      "\tTesting Loss: 567.434835\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 563.379766\n",
      "\tTesting Loss: 567.105632\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 563.294118\n",
      "\tTesting Loss: 567.020020\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 563.214798\n",
      "\tTesting Loss: 566.960978\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 563.174538\n",
      "\tTesting Loss: 566.981547\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 563.141675\n",
      "\tTesting Loss: 566.846130\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 563.145925\n",
      "\tTesting Loss: 566.678101\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 563.090426\n",
      "\tTesting Loss: 566.591614\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 563.065623\n",
      "\tTesting Loss: 566.367249\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 562.968625\n",
      "\tTesting Loss: 566.434489\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 562.997464\n",
      "\tTesting Loss: 566.133423\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 562.884440\n",
      "\tTesting Loss: 566.232015\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 562.919586\n",
      "\tTesting Loss: 565.987142\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 562.815536\n",
      "\tTesting Loss: 566.072795\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 562.858688\n",
      "\tTesting Loss: 565.758525\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 562.704173\n",
      "\tTesting Loss: 565.870321\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 562.778641\n",
      "\tTesting Loss: 565.536357\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 562.655691\n",
      "\tTesting Loss: 565.582540\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 562.718872\n",
      "\tTesting Loss: 565.268412\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 562.576177\n",
      "\tTesting Loss: 565.362976\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 562.653788\n",
      "\tTesting Loss: 565.000234\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 562.511261\n",
      "\tTesting Loss: 565.055481\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 562.583827\n",
      "\tTesting Loss: 564.747070\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 562.419258\n",
      "\tTesting Loss: 564.810181\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 562.495102\n",
      "\tTesting Loss: 564.653158\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 562.356420\n",
      "\tTesting Loss: 564.889242\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 562.402074\n",
      "\tTesting Loss: 565.156514\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 562.370585\n",
      "\tTesting Loss: 565.625326\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 562.481262\n",
      "\tTesting Loss: 566.181905\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 562.630203\n",
      "\tTesting Loss: 566.655579\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 562.872726\n",
      "\tTesting Loss: 566.836263\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 563.009893\n",
      "\tTesting Loss: 566.821798\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 563.132843\n",
      "\tTesting Loss: 566.435486\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 563.106339\n",
      "\tTesting Loss: 566.085185\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 563.155446\n",
      "\tTesting Loss: 565.542074\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 563.089773\n",
      "\tTesting Loss: 565.167480\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 562.925575\n",
      "\tTesting Loss: 565.039185\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 562.837738\n",
      "\tTesting Loss: 565.674601\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 563.027379\n",
      "\tTesting Loss: 564.941915\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 561.543360\n",
      "\tTesting Loss: 564.548340\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 558.973501\n",
      "\tTesting Loss: 543.099213\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 546.978419\n",
      "\tTesting Loss: 542.706055\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.295197\n",
      "\tTesting Loss: 547.150553\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 557.899785\n",
      "\tTesting Loss: 549.424672\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 554.484441\n",
      "\tTesting Loss: 559.220581\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 557.947848\n",
      "\tTesting Loss: 555.186483\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 557.996966\n",
      "\tTesting Loss: 561.137909\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 560.524529\n",
      "\tTesting Loss: 562.245026\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 561.275584\n",
      "\tTesting Loss: 563.456462\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 561.774272\n",
      "\tTesting Loss: 564.442139\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 562.230748\n",
      "\tTesting Loss: 565.162913\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 562.501973\n",
      "\tTesting Loss: 565.513936\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 562.658801\n",
      "\tTesting Loss: 565.554199\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 562.727750\n",
      "\tTesting Loss: 565.533020\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 562.742289\n",
      "\tTesting Loss: 565.484111\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 562.764277\n",
      "\tTesting Loss: 565.399902\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 562.748052\n",
      "\tTesting Loss: 565.370687\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 562.766731\n",
      "\tTesting Loss: 565.288757\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 562.777089\n",
      "\tTesting Loss: 565.322021\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 562.819267\n",
      "\tTesting Loss: 565.372375\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 562.843887\n",
      "\tTesting Loss: 565.397746\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 562.852590\n",
      "\tTesting Loss: 565.375264\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 562.868851\n",
      "\tTesting Loss: 565.358988\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 562.881421\n",
      "\tTesting Loss: 565.359273\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 562.885050\n",
      "\tTesting Loss: 565.376994\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 562.873998\n",
      "\tTesting Loss: 565.415426\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 562.884735\n",
      "\tTesting Loss: 565.327942\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 562.889064\n",
      "\tTesting Loss: 565.276591\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 562.905045\n",
      "\tTesting Loss: 565.228495\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 562.876269\n",
      "\tTesting Loss: 565.328756\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 562.863833\n",
      "\tTesting Loss: 565.412577\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 562.903987\n",
      "\tTesting Loss: 565.294006\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 562.895599\n",
      "\tTesting Loss: 565.243693\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 562.894379\n",
      "\tTesting Loss: 565.122599\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 562.862386\n",
      "\tTesting Loss: 565.113383\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 562.839238\n",
      "\tTesting Loss: 565.154826\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 562.814092\n",
      "\tTesting Loss: 565.323507\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 562.864609\n",
      "\tTesting Loss: 565.172058\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 562.876406\n",
      "\tTesting Loss: 565.007568\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 562.858337\n",
      "\tTesting Loss: 564.861674\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 562.786626\n",
      "\tTesting Loss: 565.049194\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 562.773661\n",
      "\tTesting Loss: 565.220723\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 562.800385\n",
      "\tTesting Loss: 565.287313\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 562.852442\n",
      "\tTesting Loss: 565.182699\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 562.901774\n",
      "\tTesting Loss: 564.842753\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 562.819448\n",
      "\tTesting Loss: 564.795329\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 562.794917\n",
      "\tTesting Loss: 564.913839\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 562.743563\n",
      "\tTesting Loss: 565.160116\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 562.745850\n",
      "\tTesting Loss: 565.377787\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 562.784978\n",
      "\tTesting Loss: 565.354533\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 562.868172\n",
      "\tTesting Loss: 565.014282\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 562.831624\n",
      "\tTesting Loss: 564.836792\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 562.805939\n",
      "\tTesting Loss: 564.729757\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 562.726273\n",
      "\tTesting Loss: 564.938293\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 562.712245\n",
      "\tTesting Loss: 565.187378\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 562.703237\n",
      "\tTesting Loss: 565.451009\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 562.779653\n",
      "\tTesting Loss: 565.320903\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 562.833677\n",
      "\tTesting Loss: 565.067546\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 562.892265\n",
      "\tTesting Loss: 564.596110\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 562.794963\n",
      "\tTesting Loss: 564.377238\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 562.677027\n",
      "\tTesting Loss: 564.611430\n",
      "\tLearning Rate: 0.000225284\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 562.600360\n",
      "\tTesting Loss: 565.183207\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 562.588053\n",
      "\tTesting Loss: 565.847738\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 562.728673\n",
      "\tTesting Loss: 565.769206\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 562.902217\n",
      "\tTesting Loss: 565.289022\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 562.974355\n",
      "\tTesting Loss: 564.587565\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 562.838430\n",
      "\tTesting Loss: 564.422770\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 562.679367\n",
      "\tTesting Loss: 564.985881\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 562.617226\n",
      "\tTesting Loss: 565.638285\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 562.679738\n",
      "\tTesting Loss: 565.789307\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 562.797910\n",
      "\tTesting Loss: 565.584269\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 562.944433\n",
      "\tTesting Loss: 564.979980\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 562.948545\n",
      "\tTesting Loss: 564.140971\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 562.754496\n",
      "\tTesting Loss: 563.904663\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 562.556656\n",
      "\tTesting Loss: 564.674561\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 562.494400\n",
      "\tTesting Loss: 565.875895\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 562.591545\n",
      "\tTesting Loss: 566.214132\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 562.701574\n",
      "\tTesting Loss: 566.001973\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 562.943659\n",
      "\tTesting Loss: 565.456523\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 563.195442\n",
      "\tTesting Loss: 563.998881\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 563.033127\n",
      "\tTesting Loss: 562.850301\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 562.809067\n",
      "\tTesting Loss: 562.259196\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 562.440821\n",
      "\tTesting Loss: 565.406718\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 562.587947\n",
      "\tTesting Loss: 567.310547\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 562.798004\n",
      "\tTesting Loss: 567.225972\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 562.982391\n",
      "\tTesting Loss: 566.395874\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 563.273972\n",
      "\tTesting Loss: 565.195964\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 563.438057\n",
      "\tTesting Loss: 563.701945\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 563.129763\n",
      "\tTesting Loss: 562.567790\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 562.787493\n",
      "\tTesting Loss: 564.793335\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 562.531334\n",
      "\tTesting Loss: 567.177266\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 562.788020\n",
      "\tTesting Loss: 567.096781\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 563.041051\n",
      "\tTesting Loss: 566.139140\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 563.295410\n",
      "\tTesting Loss: 564.692464\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 563.199369\n",
      "\tTesting Loss: 563.806315\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 562.945671\n",
      "\tTesting Loss: 564.149740\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 562.588613\n",
      "\tTesting Loss: 565.911174\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 562.619787\n",
      "\tTesting Loss: 566.503357\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 562.799286\n",
      "\tTesting Loss: 566.158427\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 563.008881\n",
      "\tTesting Loss: 565.258321\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 563.120885\n",
      "\tTesting Loss: 564.057190\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 562.995458\n",
      "\tTesting Loss: 563.091573\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 562.725327\n",
      "\tTesting Loss: 563.765625\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 562.406118\n",
      "\tTesting Loss: 566.236410\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 562.557788\n",
      "\tTesting Loss: 567.056641\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 562.704666\n",
      "\tTesting Loss: 566.746480\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 562.902217\n",
      "\tTesting Loss: 566.102600\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 563.270935\n",
      "\tTesting Loss: 565.005514\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 563.388987\n",
      "\tTesting Loss: 563.120870\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 562.857117\n",
      "\tTesting Loss: 564.221639\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 563.611994\n",
      "\tTesting Loss: 561.618408\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 562.642934\n",
      "\tTesting Loss: 561.153870\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 560.768847\n",
      "\tTesting Loss: 555.351674\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 557.964981\n",
      "\tTesting Loss: 550.779358\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 557.799894\n",
      "\tTesting Loss: 558.002258\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 558.509089\n",
      "\tTesting Loss: 562.520426\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 561.669642\n",
      "\tTesting Loss: 561.194336\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 561.287954\n",
      "\tTesting Loss: 563.526835\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 562.194585\n",
      "\tTesting Loss: 563.026265\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 562.250351\n",
      "\tTesting Loss: 564.099304\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 562.595678\n",
      "\tTesting Loss: 564.183533\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 562.681656\n",
      "\tTesting Loss: 564.373210\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 562.749680\n",
      "\tTesting Loss: 564.299723\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 562.758499\n",
      "\tTesting Loss: 564.197815\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 562.722855\n",
      "\tTesting Loss: 564.072428\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 562.681625\n",
      "\tTesting Loss: 563.946126\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 562.639170\n",
      "\tTesting Loss: 563.835734\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 562.591258\n",
      "\tTesting Loss: 563.734233\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 562.551437\n",
      "\tTesting Loss: 563.653320\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 562.496966\n",
      "\tTesting Loss: 563.663289\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 562.487190\n",
      "\tTesting Loss: 563.647380\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 562.438131\n",
      "\tTesting Loss: 563.645671\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 562.418831\n",
      "\tTesting Loss: 563.615662\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 562.389226\n",
      "\tTesting Loss: 563.610921\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 562.365634\n",
      "\tTesting Loss: 563.619019\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 562.321673\n",
      "\tTesting Loss: 563.565715\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 562.320409\n",
      "\tTesting Loss: 563.558980\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 562.313395\n",
      "\tTesting Loss: 563.602824\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 562.302490\n",
      "\tTesting Loss: 563.568339\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 562.292745\n",
      "\tTesting Loss: 563.540792\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 562.289693\n",
      "\tTesting Loss: 563.520793\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 562.255524\n",
      "\tTesting Loss: 563.542358\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 562.245102\n",
      "\tTesting Loss: 563.551595\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 562.237671\n",
      "\tTesting Loss: 563.559977\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 562.233665\n",
      "\tTesting Loss: 563.557109\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 562.233124\n",
      "\tTesting Loss: 563.518677\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 562.207916\n",
      "\tTesting Loss: 563.513977\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 562.186549\n",
      "\tTesting Loss: 563.473958\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 562.168561\n",
      "\tTesting Loss: 563.470398\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 562.162791\n",
      "\tTesting Loss: 563.446798\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 562.126282\n",
      "\tTesting Loss: 563.422363\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 562.107010\n",
      "\tTesting Loss: 563.432861\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 562.105222\n",
      "\tTesting Loss: 563.333984\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 562.089742\n",
      "\tTesting Loss: 563.283936\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 562.062935\n",
      "\tTesting Loss: 563.276021\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 562.038737\n",
      "\tTesting Loss: 563.342773\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 562.044314\n",
      "\tTesting Loss: 563.274211\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 562.002090\n",
      "\tTesting Loss: 563.291951\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 562.029388\n",
      "\tTesting Loss: 563.122477\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 562.022491\n",
      "\tTesting Loss: 563.056234\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 561.990613\n",
      "\tTesting Loss: 562.979594\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 561.940755\n",
      "\tTesting Loss: 563.130168\n",
      "\tLearning Rate: 0.000202756\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 561.899740\n",
      "\tTesting Loss: 563.261597\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 561.902448\n",
      "\tTesting Loss: 563.353292\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 562.021960\n",
      "\tTesting Loss: 562.994181\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 562.016986\n",
      "\tTesting Loss: 562.751139\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 561.899968\n",
      "\tTesting Loss: 562.911499\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 561.823115\n",
      "\tTesting Loss: 563.233215\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 561.842061\n",
      "\tTesting Loss: 563.322876\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 561.906776\n",
      "\tTesting Loss: 563.075399\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 562.005681\n",
      "\tTesting Loss: 562.591797\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 562.007645\n",
      "\tTesting Loss: 562.298319\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 561.755697\n",
      "\tTesting Loss: 562.848511\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 561.631404\n",
      "\tTesting Loss: 563.609070\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 561.774971\n",
      "\tTesting Loss: 563.635539\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 561.923930\n",
      "\tTesting Loss: 562.985962\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 562.158198\n",
      "\tTesting Loss: 561.782979\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 561.999095\n",
      "\tTesting Loss: 561.493368\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 561.451274\n",
      "\tTesting Loss: 563.387634\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 561.529147\n",
      "\tTesting Loss: 564.314941\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 561.822505\n",
      "\tTesting Loss: 563.838643\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 562.109131\n",
      "\tTesting Loss: 562.339315\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 562.327934\n",
      "\tTesting Loss: 560.635701\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 561.880239\n",
      "\tTesting Loss: 561.810567\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 561.171964\n",
      "\tTesting Loss: 564.897847\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 561.894358\n",
      "\tTesting Loss: 564.785421\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 562.006882\n",
      "\tTesting Loss: 563.491618\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 562.389002\n",
      "\tTesting Loss: 561.184855\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 562.416674\n",
      "\tTesting Loss: 559.602946\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 561.335337\n",
      "\tTesting Loss: 564.098389\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 561.575409\n",
      "\tTesting Loss: 565.170492\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 562.020200\n",
      "\tTesting Loss: 564.326294\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 562.279027\n",
      "\tTesting Loss: 561.807434\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 562.272593\n",
      "\tTesting Loss: 560.300395\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 561.779650\n",
      "\tTesting Loss: 561.749329\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 561.213679\n",
      "\tTesting Loss: 564.626607\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 561.859553\n",
      "\tTesting Loss: 564.486165\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 562.142578\n",
      "\tTesting Loss: 562.847534\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 562.317909\n",
      "\tTesting Loss: 560.926941\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 562.108124\n",
      "\tTesting Loss: 560.648722\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 561.228078\n",
      "\tTesting Loss: 564.136088\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 561.681320\n",
      "\tTesting Loss: 564.657898\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 561.977066\n",
      "\tTesting Loss: 563.879476\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 562.233683\n",
      "\tTesting Loss: 561.778158\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 562.291819\n",
      "\tTesting Loss: 559.882446\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 561.803640\n",
      "\tTesting Loss: 561.734273\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 561.134796\n",
      "\tTesting Loss: 565.141927\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 561.885139\n",
      "\tTesting Loss: 564.996297\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 562.005300\n",
      "\tTesting Loss: 563.533875\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 562.338058\n",
      "\tTesting Loss: 561.633789\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 562.522809\n",
      "\tTesting Loss: 558.605082\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 561.665527\n",
      "\tTesting Loss: 562.592204\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 561.213308\n",
      "\tTesting Loss: 566.040019\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 562.094777\n",
      "\tTesting Loss: 565.375285\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 562.140986\n",
      "\tTesting Loss: 563.101379\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 562.330119\n",
      "\tTesting Loss: 561.287557\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 562.438311\n",
      "\tTesting Loss: 559.614136\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 561.506177\n",
      "\tTesting Loss: 563.450562\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 561.461034\n",
      "\tTesting Loss: 564.771159\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 561.949748\n",
      "\tTesting Loss: 564.253764\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 562.174876\n",
      "\tTesting Loss: 561.784159\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 562.238017\n",
      "\tTesting Loss: 560.225993\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 562.138397\n",
      "\tTesting Loss: 560.435506\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 561.193031\n",
      "\tTesting Loss: 564.494466\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 561.685359\n",
      "\tTesting Loss: 564.690430\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 562.002406\n",
      "\tTesting Loss: 563.649841\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 562.264247\n",
      "\tTesting Loss: 561.392822\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 562.350118\n",
      "\tTesting Loss: 559.248881\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 561.830579\n",
      "\tTesting Loss: 561.507039\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 561.165721\n",
      "\tTesting Loss: 565.315592\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 561.934072\n",
      "\tTesting Loss: 564.992452\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 562.031672\n",
      "\tTesting Loss: 563.304850\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 562.267443\n",
      "\tTesting Loss: 561.767110\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 562.570417\n",
      "\tTesting Loss: 558.515157\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 562.124964\n",
      "\tTesting Loss: 560.212179\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 561.056305\n",
      "\tTesting Loss: 566.340149\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 562.050573\n",
      "\tTesting Loss: 565.683370\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 562.133728\n",
      "\tTesting Loss: 563.585449\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 562.166395\n",
      "\tTesting Loss: 562.319153\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 562.677236\n",
      "\tTesting Loss: 559.591593\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 562.478882\n",
      "\tTesting Loss: 558.860270\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 561.043803\n",
      "\tTesting Loss: 566.030558\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 561.947540\n",
      "\tTesting Loss: 565.329732\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 562.177233\n",
      "\tTesting Loss: 563.325562\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 562.220220\n",
      "\tTesting Loss: 561.096212\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 562.298894\n",
      "\tTesting Loss: 559.782064\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 561.836019\n",
      "\tTesting Loss: 561.692546\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 561.188993\n",
      "\tTesting Loss: 564.439148\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 561.742452\n",
      "\tTesting Loss: 564.190328\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 561.969925\n",
      "\tTesting Loss: 562.327861\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 562.198878\n",
      "\tTesting Loss: 560.559469\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 562.291656\n",
      "\tTesting Loss: 559.159058\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 561.365611\n",
      "\tTesting Loss: 563.206604\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 561.314418\n",
      "\tTesting Loss: 564.621134\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 561.745117\n",
      "\tTesting Loss: 564.085388\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 561.954285\n",
      "\tTesting Loss: 562.169718\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 562.193263\n",
      "\tTesting Loss: 560.594849\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 562.422668\n",
      "\tTesting Loss: 558.043009\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 561.636759\n",
      "\tTesting Loss: 562.501953\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 561.137405\n",
      "\tTesting Loss: 565.489685\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 561.883204\n",
      "\tTesting Loss: 564.798299\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 561.841304\n",
      "\tTesting Loss: 562.956991\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 562.158544\n",
      "\tTesting Loss: 561.491007\n",
      "\tLearning Rate: 0.000182480\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 562.580154\n",
      "\tTesting Loss: 557.890177\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 562.107984\n",
      "\tTesting Loss: 561.334941\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 560.952540\n",
      "\tTesting Loss: 565.902222\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 561.985240\n",
      "\tTesting Loss: 564.776530\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 562.343000\n",
      "\tTesting Loss: 560.758260\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 562.010623\n",
      "\tTesting Loss: 559.944702\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 561.486354\n",
      "\tTesting Loss: 561.960388\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 561.382985\n",
      "\tTesting Loss: 562.983643\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 561.677340\n",
      "\tTesting Loss: 562.746867\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 561.991516\n",
      "\tTesting Loss: 560.666768\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 561.864805\n",
      "\tTesting Loss: 559.943217\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 561.336789\n",
      "\tTesting Loss: 562.042074\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 561.235227\n",
      "\tTesting Loss: 563.330953\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 561.505531\n",
      "\tTesting Loss: 563.175924\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 561.774048\n",
      "\tTesting Loss: 561.689860\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 562.099213\n",
      "\tTesting Loss: 559.366048\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 562.174634\n",
      "\tTesting Loss: 558.425151\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 560.968608\n",
      "\tTesting Loss: 564.489665\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 561.467832\n",
      "\tTesting Loss: 564.721781\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 561.799535\n",
      "\tTesting Loss: 563.492411\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 561.962021\n",
      "\tTesting Loss: 561.072571\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 562.243408\n",
      "\tTesting Loss: 558.610189\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 562.166809\n",
      "\tTesting Loss: 559.051086\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 560.871572\n",
      "\tTesting Loss: 565.391907\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 561.684489\n",
      "\tTesting Loss: 564.861430\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 561.830119\n",
      "\tTesting Loss: 562.898499\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 562.065608\n",
      "\tTesting Loss: 560.224121\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 562.275037\n",
      "\tTesting Loss: 558.280151\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 561.506399\n",
      "\tTesting Loss: 562.443197\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 561.021449\n",
      "\tTesting Loss: 564.469320\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 561.649231\n",
      "\tTesting Loss: 563.863627\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 561.856967\n",
      "\tTesting Loss: 561.073263\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 561.994026\n",
      "\tTesting Loss: 559.298869\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 562.003517\n",
      "\tTesting Loss: 559.475484\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 561.036926\n",
      "\tTesting Loss: 563.791260\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 561.346039\n",
      "\tTesting Loss: 564.042094\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 561.587906\n",
      "\tTesting Loss: 563.057027\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 561.948209\n",
      "\tTesting Loss: 560.230225\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 562.163261\n",
      "\tTesting Loss: 557.014384\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 561.846451\n",
      "\tTesting Loss: 559.540080\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 560.785700\n",
      "\tTesting Loss: 565.742676\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 561.719233\n",
      "\tTesting Loss: 564.916321\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 561.623853\n",
      "\tTesting Loss: 562.932353\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 561.899991\n",
      "\tTesting Loss: 561.075989\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 562.412921\n",
      "\tTesting Loss: 557.685527\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 562.022031\n",
      "\tTesting Loss: 559.322937\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 560.720090\n",
      "\tTesting Loss: 565.729655\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 561.634745\n",
      "\tTesting Loss: 564.795817\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 561.799037\n",
      "\tTesting Loss: 562.065959\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 561.982615\n",
      "\tTesting Loss: 559.504049\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 562.167618\n",
      "\tTesting Loss: 558.427389\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 561.315989\n",
      "\tTesting Loss: 562.824097\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 561.020490\n",
      "\tTesting Loss: 564.044454\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 561.487429\n",
      "\tTesting Loss: 563.277568\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 561.800883\n",
      "\tTesting Loss: 560.012248\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 561.888985\n",
      "\tTesting Loss: 558.420573\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 561.611852\n",
      "\tTesting Loss: 560.553874\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 560.834946\n",
      "\tTesting Loss: 564.014587\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 561.300560\n",
      "\tTesting Loss: 563.780924\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 561.474386\n",
      "\tTesting Loss: 562.364787\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 561.826055\n",
      "\tTesting Loss: 560.074870\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 562.170008\n",
      "\tTesting Loss: 556.311279\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 561.662618\n",
      "\tTesting Loss: 559.925191\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 560.656990\n",
      "\tTesting Loss: 565.957906\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 561.623881\n",
      "\tTesting Loss: 565.118469\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 561.508245\n",
      "\tTesting Loss: 562.995565\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 561.761742\n",
      "\tTesting Loss: 561.153483\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 562.353312\n",
      "\tTesting Loss: 557.622904\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 562.033259\n",
      "\tTesting Loss: 558.711609\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 560.624084\n",
      "\tTesting Loss: 565.573812\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 561.422363\n",
      "\tTesting Loss: 564.739889\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 561.715530\n",
      "\tTesting Loss: 561.955770\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 561.793477\n",
      "\tTesting Loss: 559.010640\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 561.888179\n",
      "\tTesting Loss: 558.426188\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 561.229808\n",
      "\tTesting Loss: 561.953817\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 560.824593\n",
      "\tTesting Loss: 563.263062\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 561.191254\n",
      "\tTesting Loss: 562.810933\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 561.613174\n",
      "\tTesting Loss: 559.418457\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 561.752515\n",
      "\tTesting Loss: 557.590841\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 561.437215\n",
      "\tTesting Loss: 560.327311\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 560.679738\n",
      "\tTesting Loss: 563.807475\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 561.166061\n",
      "\tTesting Loss: 563.573649\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 561.277664\n",
      "\tTesting Loss: 561.958862\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 561.565989\n",
      "\tTesting Loss: 559.946696\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 562.108816\n",
      "\tTesting Loss: 556.414185\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 561.673971\n",
      "\tTesting Loss: 558.941793\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 560.511592\n",
      "\tTesting Loss: 565.603556\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 561.406815\n",
      "\tTesting Loss: 564.939555\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 561.258876\n",
      "\tTesting Loss: 562.907552\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 561.410950\n",
      "\tTesting Loss: 561.804382\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 562.244771\n",
      "\tTesting Loss: 557.671265\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 562.084941\n",
      "\tTesting Loss: 555.334819\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 561.619395\n",
      "\tTesting Loss: 562.764587\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 560.660634\n",
      "\tTesting Loss: 566.561849\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 561.526566\n",
      "\tTesting Loss: 564.699280\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 561.535156\n",
      "\tTesting Loss: 561.936890\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 561.830223\n",
      "\tTesting Loss: 558.806966\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 561.854716\n",
      "\tTesting Loss: 558.590088\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 560.861018\n",
      "\tTesting Loss: 563.353149\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 560.794800\n",
      "\tTesting Loss: 563.050659\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 561.276649\n",
      "\tTesting Loss: 561.505473\n",
      "\tLearning Rate: 0.000164232\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 561.631022\n",
      "\tTesting Loss: 557.269002\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 561.266719\n",
      "\tTesting Loss: 559.717509\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 560.338552\n",
      "\tTesting Loss: 563.149821\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 560.947398\n",
      "\tTesting Loss: 562.361735\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 561.412727\n",
      "\tTesting Loss: 559.238363\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 561.340749\n",
      "\tTesting Loss: 558.087728\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 560.650075\n",
      "\tTesting Loss: 561.678772\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 560.505849\n",
      "\tTesting Loss: 562.410624\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 560.967860\n",
      "\tTesting Loss: 561.633748\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 561.297803\n",
      "\tTesting Loss: 558.361532\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 561.158468\n",
      "\tTesting Loss: 558.316182\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 560.524109\n",
      "\tTesting Loss: 561.681702\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 560.523198\n",
      "\tTesting Loss: 562.333069\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 560.849365\n",
      "\tTesting Loss: 561.670634\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 561.153488\n",
      "\tTesting Loss: 558.849813\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 561.242831\n",
      "\tTesting Loss: 557.531148\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 560.809036\n",
      "\tTesting Loss: 560.412893\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 560.216293\n",
      "\tTesting Loss: 562.964132\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 560.690389\n",
      "\tTesting Loss: 562.396505\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 560.918757\n",
      "\tTesting Loss: 560.420878\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 561.183489\n",
      "\tTesting Loss: 557.997589\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 561.449771\n",
      "\tTesting Loss: 556.920451\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 560.519669\n",
      "\tTesting Loss: 562.408061\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 560.255732\n",
      "\tTesting Loss: 563.952474\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 560.743073\n",
      "\tTesting Loss: 562.826213\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 560.926188\n",
      "\tTesting Loss: 560.459819\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 561.421611\n",
      "\tTesting Loss: 557.386108\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 561.749069\n",
      "\tTesting Loss: 556.044454\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 560.249583\n",
      "\tTesting Loss: 564.915690\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 560.393422\n",
      "\tTesting Loss: 564.120626\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 560.882182\n",
      "\tTesting Loss: 562.345764\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 561.166262\n",
      "\tTesting Loss: 557.825378\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 561.385264\n",
      "\tTesting Loss: 556.898112\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 560.740855\n",
      "\tTesting Loss: 561.076986\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 560.039635\n",
      "\tTesting Loss: 562.852620\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 560.619214\n",
      "\tTesting Loss: 561.773071\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 561.083539\n",
      "\tTesting Loss: 557.507141\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 561.220871\n",
      "\tTesting Loss: 556.580811\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 560.355006\n",
      "\tTesting Loss: 562.220042\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 560.087456\n",
      "\tTesting Loss: 563.083822\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 560.675598\n",
      "\tTesting Loss: 561.752502\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 560.922948\n",
      "\tTesting Loss: 557.962260\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 561.177994\n",
      "\tTesting Loss: 556.713064\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 560.662829\n",
      "\tTesting Loss: 560.246440\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 559.809390\n",
      "\tTesting Loss: 563.324015\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 560.497884\n",
      "\tTesting Loss: 562.155029\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 560.705098\n",
      "\tTesting Loss: 558.961121\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 560.983729\n",
      "\tTesting Loss: 557.025696\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 561.003240\n",
      "\tTesting Loss: 557.662394\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 559.846863\n",
      "\tTesting Loss: 563.385437\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 560.316742\n",
      "\tTesting Loss: 562.983256\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 560.383265\n",
      "\tTesting Loss: 561.667562\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 560.808647\n",
      "\tTesting Loss: 558.906840\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 561.476359\n",
      "\tTesting Loss: 555.595256\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 561.047440\n",
      "\tTesting Loss: 556.891642\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 559.359642\n",
      "\tTesting Loss: 565.086365\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 560.471901\n",
      "\tTesting Loss: 563.574615\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 560.360280\n",
      "\tTesting Loss: 561.018626\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 560.816948\n",
      "\tTesting Loss: 558.247996\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 561.543213\n",
      "\tTesting Loss: 556.297607\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 560.601484\n",
      "\tTesting Loss: 561.288269\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 559.274590\n",
      "\tTesting Loss: 563.378103\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 560.344472\n",
      "\tTesting Loss: 562.217611\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 560.787150\n",
      "\tTesting Loss: 556.868835\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 560.637075\n",
      "\tTesting Loss: 556.643860\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 559.828990\n",
      "\tTesting Loss: 562.035929\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 559.845871\n",
      "\tTesting Loss: 561.475891\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 560.330447\n",
      "\tTesting Loss: 559.902903\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 560.856865\n",
      "\tTesting Loss: 556.561910\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 560.773158\n",
      "\tTesting Loss: 556.365377\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 559.301020\n",
      "\tTesting Loss: 564.478495\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 560.049891\n",
      "\tTesting Loss: 562.887370\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 560.172473\n",
      "\tTesting Loss: 560.831563\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 560.646108\n",
      "\tTesting Loss: 557.538086\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 561.128031\n",
      "\tTesting Loss: 556.248494\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 559.938441\n",
      "\tTesting Loss: 562.410787\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 559.383792\n",
      "\tTesting Loss: 562.648519\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 560.121277\n",
      "\tTesting Loss: 561.155518\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 560.591754\n",
      "\tTesting Loss: 556.782471\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 560.558459\n",
      "\tTesting Loss: 556.490112\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 559.457265\n",
      "\tTesting Loss: 562.913676\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 559.631073\n",
      "\tTesting Loss: 561.706177\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 560.157288\n",
      "\tTesting Loss: 559.473083\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 560.584717\n",
      "\tTesting Loss: 556.460907\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 560.488907\n",
      "\tTesting Loss: 557.041524\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 559.082840\n",
      "\tTesting Loss: 563.822489\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 559.786016\n",
      "\tTesting Loss: 562.048503\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 559.953117\n",
      "\tTesting Loss: 559.670431\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 560.324285\n",
      "\tTesting Loss: 557.280802\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 560.712036\n",
      "\tTesting Loss: 556.511312\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 559.462074\n",
      "\tTesting Loss: 562.535461\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 559.310572\n",
      "\tTesting Loss: 561.998088\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 559.890483\n",
      "\tTesting Loss: 560.117584\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 560.189260\n",
      "\tTesting Loss: 557.107198\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 560.253189\n",
      "\tTesting Loss: 556.927714\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 559.316088\n",
      "\tTesting Loss: 561.655263\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 559.259893\n",
      "\tTesting Loss: 561.419474\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 559.779508\n",
      "\tTesting Loss: 559.479797\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 560.099553\n",
      "\tTesting Loss: 557.195577\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 560.251714\n",
      "\tTesting Loss: 556.817261\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 559.242376\n",
      "\tTesting Loss: 561.386627\n",
      "\tLearning Rate: 0.000147809\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 559.119647\n",
      "\tTesting Loss: 561.825216\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 559.592893\n",
      "\tTesting Loss: 560.024984\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 559.968979\n",
      "\tTesting Loss: 557.647115\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 560.131704\n",
      "\tTesting Loss: 557.199249\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 559.243744\n",
      "\tTesting Loss: 560.259471\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 558.978734\n",
      "\tTesting Loss: 560.640289\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 559.561261\n",
      "\tTesting Loss: 559.187744\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 559.949638\n",
      "\tTesting Loss: 556.932780\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 559.818446\n",
      "\tTesting Loss: 557.320221\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 558.702098\n",
      "\tTesting Loss: 561.139242\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 559.087420\n",
      "\tTesting Loss: 560.586324\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 559.575119\n",
      "\tTesting Loss: 558.553324\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 559.841624\n",
      "\tTesting Loss: 556.898071\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 559.574844\n",
      "\tTesting Loss: 557.813212\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 558.607071\n",
      "\tTesting Loss: 560.884338\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 559.061701\n",
      "\tTesting Loss: 560.304047\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 559.461283\n",
      "\tTesting Loss: 558.458252\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 559.768639\n",
      "\tTesting Loss: 556.793803\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 559.641307\n",
      "\tTesting Loss: 557.359446\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 558.439311\n",
      "\tTesting Loss: 561.140615\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 558.909714\n",
      "\tTesting Loss: 560.663839\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 559.313464\n",
      "\tTesting Loss: 558.659648\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 559.553599\n",
      "\tTesting Loss: 556.943176\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 559.567396\n",
      "\tTesting Loss: 557.016581\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 558.588994\n",
      "\tTesting Loss: 559.836873\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 558.604207\n",
      "\tTesting Loss: 560.375214\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 559.059527\n",
      "\tTesting Loss: 559.233978\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 559.529902\n",
      "\tTesting Loss: 557.043935\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 559.733727\n",
      "\tTesting Loss: 555.479482\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 558.717555\n",
      "\tTesting Loss: 559.759410\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 558.154439\n",
      "\tTesting Loss: 561.340271\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 559.026174\n",
      "\tTesting Loss: 559.625936\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 559.318911\n",
      "\tTesting Loss: 557.002177\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 559.543749\n",
      "\tTesting Loss: 556.129293\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 558.691243\n",
      "\tTesting Loss: 559.442657\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 558.202934\n",
      "\tTesting Loss: 560.516368\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 558.911769\n",
      "\tTesting Loss: 559.073059\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 559.303192\n",
      "\tTesting Loss: 556.832357\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 559.466970\n",
      "\tTesting Loss: 556.110392\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 558.414200\n",
      "\tTesting Loss: 559.931986\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 558.191419\n",
      "\tTesting Loss: 560.608541\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 558.833964\n",
      "\tTesting Loss: 559.043467\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 559.146016\n",
      "\tTesting Loss: 556.963338\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 559.424622\n",
      "\tTesting Loss: 555.878387\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 558.496936\n",
      "\tTesting Loss: 559.364034\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 557.913109\n",
      "\tTesting Loss: 560.654348\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 558.704208\n",
      "\tTesting Loss: 559.068909\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 559.049713\n",
      "\tTesting Loss: 556.763468\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 559.261152\n",
      "\tTesting Loss: 555.836650\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 558.223508\n",
      "\tTesting Loss: 559.606181\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 557.957652\n",
      "\tTesting Loss: 560.857340\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 558.611481\n",
      "\tTesting Loss: 559.019318\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 558.971494\n",
      "\tTesting Loss: 556.939677\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 559.351123\n",
      "\tTesting Loss: 555.649740\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 558.330465\n",
      "\tTesting Loss: 559.343842\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 557.641098\n",
      "\tTesting Loss: 560.788452\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 558.494657\n",
      "\tTesting Loss: 559.210754\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 558.858546\n",
      "\tTesting Loss: 556.826721\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 559.150640\n",
      "\tTesting Loss: 555.794637\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 558.126918\n",
      "\tTesting Loss: 559.483927\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 557.685842\n",
      "\tTesting Loss: 560.319041\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 558.460640\n",
      "\tTesting Loss: 558.651825\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 558.893735\n",
      "\tTesting Loss: 556.149953\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 559.007248\n",
      "\tTesting Loss: 555.494690\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 557.607173\n",
      "\tTesting Loss: 561.563578\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 557.838153\n",
      "\tTesting Loss: 560.567525\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 558.314026\n",
      "\tTesting Loss: 558.738057\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 558.795095\n",
      "\tTesting Loss: 556.417867\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 559.085803\n",
      "\tTesting Loss: 555.157166\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 557.742981\n",
      "\tTesting Loss: 561.350149\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 557.512355\n",
      "\tTesting Loss: 559.972117\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 558.273407\n",
      "\tTesting Loss: 558.347941\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 558.765816\n",
      "\tTesting Loss: 555.584564\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 558.408895\n",
      "\tTesting Loss: 556.735514\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 557.023448\n",
      "\tTesting Loss: 560.907979\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 557.863113\n",
      "\tTesting Loss: 558.965210\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 558.350548\n",
      "\tTesting Loss: 556.970876\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 558.626048\n",
      "\tTesting Loss: 555.395650\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 557.703644\n",
      "\tTesting Loss: 559.068197\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 557.068665\n",
      "\tTesting Loss: 559.434143\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 558.011098\n",
      "\tTesting Loss: 558.287557\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 558.431157\n",
      "\tTesting Loss: 555.725769\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 558.129252\n",
      "\tTesting Loss: 556.286051\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 556.849841\n",
      "\tTesting Loss: 560.186503\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 557.531876\n",
      "\tTesting Loss: 558.740448\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 557.937620\n",
      "\tTesting Loss: 557.551493\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 558.370249\n",
      "\tTesting Loss: 555.519226\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 557.999082\n",
      "\tTesting Loss: 556.955414\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 556.582919\n",
      "\tTesting Loss: 560.032817\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 557.469772\n",
      "\tTesting Loss: 558.687907\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 557.921677\n",
      "\tTesting Loss: 556.995494\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 558.133947\n",
      "\tTesting Loss: 555.455688\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 557.513657\n",
      "\tTesting Loss: 557.947510\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 556.633451\n",
      "\tTesting Loss: 558.940430\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 557.479390\n",
      "\tTesting Loss: 558.205180\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 557.870076\n",
      "\tTesting Loss: 556.322520\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 557.904002\n",
      "\tTesting Loss: 555.776123\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 556.960431\n",
      "\tTesting Loss: 558.743001\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 556.810313\n",
      "\tTesting Loss: 558.727743\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 557.449483\n",
      "\tTesting Loss: 557.831492\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 557.839877\n",
      "\tTesting Loss: 555.906820\n",
      "\tLearning Rate: 0.000133028\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 557.777791\n",
      "\tTesting Loss: 555.856567\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 556.564240\n",
      "\tTesting Loss: 558.875997\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 556.836751\n",
      "\tTesting Loss: 558.285868\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 557.436427\n",
      "\tTesting Loss: 557.208384\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 557.756694\n",
      "\tTesting Loss: 555.502319\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 557.074600\n",
      "\tTesting Loss: 557.589681\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 556.300120\n",
      "\tTesting Loss: 558.397766\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 557.107788\n",
      "\tTesting Loss: 557.731384\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 557.513875\n",
      "\tTesting Loss: 556.258179\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 557.194478\n",
      "\tTesting Loss: 556.498169\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 556.395963\n",
      "\tTesting Loss: 558.287760\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 556.715317\n",
      "\tTesting Loss: 558.047587\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 557.130030\n",
      "\tTesting Loss: 557.179952\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 557.329554\n",
      "\tTesting Loss: 556.091624\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 556.972387\n",
      "\tTesting Loss: 556.798248\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 556.308304\n",
      "\tTesting Loss: 558.193298\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 556.606562\n",
      "\tTesting Loss: 558.005992\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 556.946899\n",
      "\tTesting Loss: 557.139323\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 557.123266\n",
      "\tTesting Loss: 556.152303\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 557.010803\n",
      "\tTesting Loss: 556.182129\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 556.306351\n",
      "\tTesting Loss: 557.842183\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 556.207883\n",
      "\tTesting Loss: 558.114543\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 556.636922\n",
      "\tTesting Loss: 557.496785\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 556.851410\n",
      "\tTesting Loss: 556.511861\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 556.990473\n",
      "\tTesting Loss: 555.821533\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 556.705577\n",
      "\tTesting Loss: 556.617513\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 555.919126\n",
      "\tTesting Loss: 558.075928\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 556.180364\n",
      "\tTesting Loss: 557.912496\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 556.500259\n",
      "\tTesting Loss: 557.188904\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 556.702733\n",
      "\tTesting Loss: 556.237793\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 556.898254\n",
      "\tTesting Loss: 555.256470\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 556.597608\n",
      "\tTesting Loss: 556.567647\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 555.678879\n",
      "\tTesting Loss: 557.780680\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 555.885152\n",
      "\tTesting Loss: 557.586558\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 556.304428\n",
      "\tTesting Loss: 556.822032\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 556.477351\n",
      "\tTesting Loss: 555.762767\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 556.543081\n",
      "\tTesting Loss: 555.614115\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 556.112406\n",
      "\tTesting Loss: 557.048757\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 555.625595\n",
      "\tTesting Loss: 557.446208\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 555.832499\n",
      "\tTesting Loss: 557.240092\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 556.160962\n",
      "\tTesting Loss: 556.545909\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 556.359273\n",
      "\tTesting Loss: 555.624644\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 556.361641\n",
      "\tTesting Loss: 555.169617\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 555.937490\n",
      "\tTesting Loss: 557.208903\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 555.482961\n",
      "\tTesting Loss: 557.474406\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 555.581899\n",
      "\tTesting Loss: 557.020213\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 555.787043\n",
      "\tTesting Loss: 556.471720\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 556.121724\n",
      "\tTesting Loss: 555.248494\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 556.242711\n",
      "\tTesting Loss: 554.358887\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 555.794205\n",
      "\tTesting Loss: 557.460704\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 555.328878\n",
      "\tTesting Loss: 557.170451\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 555.350128\n",
      "\tTesting Loss: 556.192556\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 555.547407\n",
      "\tTesting Loss: 555.704559\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 555.679326\n",
      "\tTesting Loss: 555.346822\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 555.500605\n",
      "\tTesting Loss: 556.161072\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 555.269061\n",
      "\tTesting Loss: 556.258830\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 555.156837\n",
      "\tTesting Loss: 556.082723\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 555.192469\n",
      "\tTesting Loss: 555.749125\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 555.415962\n",
      "\tTesting Loss: 554.935842\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 555.598345\n",
      "\tTesting Loss: 555.192627\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 555.305232\n",
      "\tTesting Loss: 555.947428\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 555.037135\n",
      "\tTesting Loss: 555.941793\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 554.944277\n",
      "\tTesting Loss: 555.419322\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 555.049601\n",
      "\tTesting Loss: 554.615621\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 555.355158\n",
      "\tTesting Loss: 553.489665\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 555.445564\n",
      "\tTesting Loss: 555.030243\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 554.949626\n",
      "\tTesting Loss: 555.130168\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 554.716812\n",
      "\tTesting Loss: 554.845774\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 554.765673\n",
      "\tTesting Loss: 554.142761\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 555.007217\n",
      "\tTesting Loss: 552.748515\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 555.241038\n",
      "\tTesting Loss: 552.505025\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 555.055298\n",
      "\tTesting Loss: 554.212657\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 554.581324\n",
      "\tTesting Loss: 554.540710\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 554.496287\n",
      "\tTesting Loss: 553.586243\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 554.618744\n",
      "\tTesting Loss: 552.666077\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 554.909276\n",
      "\tTesting Loss: 551.342550\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 555.029556\n",
      "\tTesting Loss: 552.176900\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 554.664591\n",
      "\tTesting Loss: 553.824514\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 554.388753\n",
      "\tTesting Loss: 553.600016\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 554.354340\n",
      "\tTesting Loss: 552.646444\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 554.515900\n",
      "\tTesting Loss: 551.566203\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 554.770035\n",
      "\tTesting Loss: 550.981842\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 554.680776\n",
      "\tTesting Loss: 551.805542\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 554.375740\n",
      "\tTesting Loss: 553.342509\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 554.261993\n",
      "\tTesting Loss: 553.053630\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 554.223760\n",
      "\tTesting Loss: 552.099284\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 554.326253\n",
      "\tTesting Loss: 551.146790\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 554.595304\n",
      "\tTesting Loss: 550.752726\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 554.604683\n",
      "\tTesting Loss: 550.940511\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 554.247414\n",
      "\tTesting Loss: 553.266785\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 554.070287\n",
      "\tTesting Loss: 553.591909\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 554.055751\n",
      "\tTesting Loss: 552.294678\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 554.121999\n",
      "\tTesting Loss: 551.364421\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 554.351329\n",
      "\tTesting Loss: 550.644460\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 554.481237\n",
      "\tTesting Loss: 550.438965\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 554.261510\n",
      "\tTesting Loss: 552.756083\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 553.925016\n",
      "\tTesting Loss: 553.773824\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 553.942266\n",
      "\tTesting Loss: 552.702006\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 553.972794\n",
      "\tTesting Loss: 551.519674\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 554.141291\n",
      "\tTesting Loss: 550.602498\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 554.415637\n",
      "\tTesting Loss: 550.293660\n",
      "\tLearning Rate: 0.000119725\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 554.350011\n",
      "\tTesting Loss: 552.076335\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 553.787020\n",
      "\tTesting Loss: 552.761719\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 553.734698\n",
      "\tTesting Loss: 551.440592\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 553.993881\n",
      "\tTesting Loss: 550.569956\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 554.190323\n",
      "\tTesting Loss: 550.337321\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 553.951490\n",
      "\tTesting Loss: 551.253774\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 553.720306\n",
      "\tTesting Loss: 551.499776\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 553.746346\n",
      "\tTesting Loss: 550.948629\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 553.815974\n",
      "\tTesting Loss: 550.439646\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 553.882584\n",
      "\tTesting Loss: 550.452220\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 553.849711\n",
      "\tTesting Loss: 550.317576\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 553.743818\n",
      "\tTesting Loss: 551.175781\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 553.691516\n",
      "\tTesting Loss: 550.880005\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 553.614405\n",
      "\tTesting Loss: 550.723450\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 553.618566\n",
      "\tTesting Loss: 550.570160\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 553.635707\n",
      "\tTesting Loss: 550.334106\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 553.677409\n",
      "\tTesting Loss: 550.227936\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 553.703893\n",
      "\tTesting Loss: 550.185608\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 553.637767\n",
      "\tTesting Loss: 550.314351\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 553.534617\n",
      "\tTesting Loss: 551.167338\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 553.507599\n",
      "\tTesting Loss: 550.737488\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 553.431615\n",
      "\tTesting Loss: 550.533885\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 553.412338\n",
      "\tTesting Loss: 550.462870\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 553.506284\n",
      "\tTesting Loss: 550.051086\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 553.799334\n",
      "\tTesting Loss: 549.975830\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 553.783422\n",
      "\tTesting Loss: 549.785604\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 553.460836\n",
      "\tTesting Loss: 552.051615\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 553.324351\n",
      "\tTesting Loss: 551.523244\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 553.280563\n",
      "\tTesting Loss: 550.423482\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 553.419434\n",
      "\tTesting Loss: 550.075948\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 553.784475\n",
      "\tTesting Loss: 549.696452\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 553.821467\n",
      "\tTesting Loss: 549.712199\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 553.396289\n",
      "\tTesting Loss: 551.290914\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 553.187078\n",
      "\tTesting Loss: 551.225708\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 553.240855\n",
      "\tTesting Loss: 550.291016\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 553.331909\n",
      "\tTesting Loss: 549.825338\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 553.603282\n",
      "\tTesting Loss: 549.838949\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 553.587728\n",
      "\tTesting Loss: 549.273000\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 553.307602\n",
      "\tTesting Loss: 551.014140\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 553.179077\n",
      "\tTesting Loss: 550.488749\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 553.151543\n",
      "\tTesting Loss: 550.043304\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 553.234985\n",
      "\tTesting Loss: 549.847961\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 553.335342\n",
      "\tTesting Loss: 549.592672\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 553.385750\n",
      "\tTesting Loss: 549.569407\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 553.230952\n",
      "\tTesting Loss: 550.092875\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 553.057887\n",
      "\tTesting Loss: 550.426717\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 553.011653\n",
      "\tTesting Loss: 550.019379\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 552.974345\n",
      "\tTesting Loss: 549.753367\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 553.170743\n",
      "\tTesting Loss: 549.679454\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 553.440976\n",
      "\tTesting Loss: 548.988230\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 553.409170\n",
      "\tTesting Loss: 550.402079\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 553.063543\n",
      "\tTesting Loss: 550.986186\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 552.945969\n",
      "\tTesting Loss: 550.488525\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 552.976013\n",
      "\tTesting Loss: 549.863169\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 553.010358\n",
      "\tTesting Loss: 549.515422\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 553.276637\n",
      "\tTesting Loss: 549.393819\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 553.298314\n",
      "\tTesting Loss: 548.834351\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 553.032547\n",
      "\tTesting Loss: 550.897441\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 552.860413\n",
      "\tTesting Loss: 550.183634\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 552.825648\n",
      "\tTesting Loss: 549.777181\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 552.915611\n",
      "\tTesting Loss: 549.448730\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 553.137522\n",
      "\tTesting Loss: 549.263102\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 553.169484\n",
      "\tTesting Loss: 548.847900\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 552.960302\n",
      "\tTesting Loss: 550.092163\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 552.789551\n",
      "\tTesting Loss: 549.370524\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 552.725601\n",
      "\tTesting Loss: 549.677999\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 552.787694\n",
      "\tTesting Loss: 549.185425\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 553.041748\n",
      "\tTesting Loss: 549.209717\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 553.060501\n",
      "\tTesting Loss: 548.564311\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 552.837580\n",
      "\tTesting Loss: 550.289775\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 552.687200\n",
      "\tTesting Loss: 549.477763\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 552.614372\n",
      "\tTesting Loss: 549.510284\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 552.676900\n",
      "\tTesting Loss: 549.229299\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 552.899480\n",
      "\tTesting Loss: 549.073161\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 552.981944\n",
      "\tTesting Loss: 548.488861\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 552.880381\n",
      "\tTesting Loss: 549.795329\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 552.638532\n",
      "\tTesting Loss: 549.231710\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 552.524714\n",
      "\tTesting Loss: 549.315938\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 552.602849\n",
      "\tTesting Loss: 549.080516\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 552.782064\n",
      "\tTesting Loss: 548.917592\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 552.804301\n",
      "\tTesting Loss: 548.487020\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 552.731239\n",
      "\tTesting Loss: 549.242452\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 552.565048\n",
      "\tTesting Loss: 548.802144\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 552.464457\n",
      "\tTesting Loss: 549.335775\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 552.433327\n",
      "\tTesting Loss: 548.831543\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 552.589734\n",
      "\tTesting Loss: 549.045898\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 552.710518\n",
      "\tTesting Loss: 548.218313\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 552.723796\n",
      "\tTesting Loss: 548.935089\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 552.553932\n",
      "\tTesting Loss: 548.846273\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 552.419378\n",
      "\tTesting Loss: 549.118022\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 552.401446\n",
      "\tTesting Loss: 548.770213\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 552.444077\n",
      "\tTesting Loss: 548.899974\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 552.529943\n",
      "\tTesting Loss: 548.404134\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 552.643458\n",
      "\tTesting Loss: 548.694865\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 552.492071\n",
      "\tTesting Loss: 548.294596\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 552.343547\n",
      "\tTesting Loss: 549.321025\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 552.270518\n",
      "\tTesting Loss: 548.674438\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 552.207425\n",
      "\tTesting Loss: 548.962341\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 552.222000\n",
      "\tTesting Loss: 548.402598\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 552.645027\n",
      "\tTesting Loss: 548.603414\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 552.674932\n",
      "\tTesting Loss: 547.728597\n",
      "\tLearning Rate: 0.000107753\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 552.478775\n",
      "\tTesting Loss: 550.792094\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 552.222761\n",
      "\tTesting Loss: 549.513774\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 552.200729\n",
      "\tTesting Loss: 548.791982\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 552.461319\n",
      "\tTesting Loss: 548.396322\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 552.598317\n",
      "\tTesting Loss: 548.251953\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 552.311450\n",
      "\tTesting Loss: 548.071075\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 552.199498\n",
      "\tTesting Loss: 548.687317\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 552.184062\n",
      "\tTesting Loss: 548.337606\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 552.314331\n",
      "\tTesting Loss: 548.470469\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 552.250102\n",
      "\tTesting Loss: 547.956217\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 552.223892\n",
      "\tTesting Loss: 548.557383\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 552.155538\n",
      "\tTesting Loss: 548.112925\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 552.139669\n",
      "\tTesting Loss: 548.526632\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 552.096321\n",
      "\tTesting Loss: 548.108175\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 552.128438\n",
      "\tTesting Loss: 548.458944\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 552.133003\n",
      "\tTesting Loss: 548.004720\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 552.131556\n",
      "\tTesting Loss: 548.401388\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 552.058497\n",
      "\tTesting Loss: 547.935384\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 552.037239\n",
      "\tTesting Loss: 548.430745\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 551.988897\n",
      "\tTesting Loss: 547.943075\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 552.068682\n",
      "\tTesting Loss: 548.315592\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 552.072515\n",
      "\tTesting Loss: 547.747050\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 552.111160\n",
      "\tTesting Loss: 548.295064\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 552.025874\n",
      "\tTesting Loss: 547.795695\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 551.949916\n",
      "\tTesting Loss: 548.354624\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 551.877747\n",
      "\tTesting Loss: 547.988332\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 551.962212\n",
      "\tTesting Loss: 548.261800\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 552.008878\n",
      "\tTesting Loss: 547.648661\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 552.145335\n",
      "\tTesting Loss: 548.093058\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 552.020152\n",
      "\tTesting Loss: 547.494904\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 551.961782\n",
      "\tTesting Loss: 548.210175\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 551.867015\n",
      "\tTesting Loss: 547.873210\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 551.843475\n",
      "\tTesting Loss: 548.219716\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 551.787735\n",
      "\tTesting Loss: 547.740346\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 552.034655\n",
      "\tTesting Loss: 547.992371\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 552.051804\n",
      "\tTesting Loss: 547.301229\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 552.032572\n",
      "\tTesting Loss: 548.083252\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 551.860438\n",
      "\tTesting Loss: 547.666402\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 551.794210\n",
      "\tTesting Loss: 548.232503\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 551.689667\n",
      "\tTesting Loss: 547.804250\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 551.847295\n",
      "\tTesting Loss: 547.863068\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 551.995880\n",
      "\tTesting Loss: 547.259440\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 552.101611\n",
      "\tTesting Loss: 547.801554\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 551.868347\n",
      "\tTesting Loss: 547.560425\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 551.759399\n",
      "\tTesting Loss: 548.198975\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 551.595342\n",
      "\tTesting Loss: 547.638672\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 551.835108\n",
      "\tTesting Loss: 547.759806\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 551.928083\n",
      "\tTesting Loss: 547.103353\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 551.993718\n",
      "\tTesting Loss: 547.725199\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 551.766637\n",
      "\tTesting Loss: 547.417542\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 551.702596\n",
      "\tTesting Loss: 547.962728\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 551.559784\n",
      "\tTesting Loss: 547.454142\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 551.786830\n",
      "\tTesting Loss: 547.622742\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 551.828206\n",
      "\tTesting Loss: 546.989197\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 551.893509\n",
      "\tTesting Loss: 547.574270\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 551.665807\n",
      "\tTesting Loss: 547.293905\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 551.632009\n",
      "\tTesting Loss: 547.877289\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 551.483330\n",
      "\tTesting Loss: 547.305827\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 551.696325\n",
      "\tTesting Loss: 547.467570\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 551.678658\n",
      "\tTesting Loss: 546.914978\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 551.797442\n",
      "\tTesting Loss: 547.469981\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 551.586914\n",
      "\tTesting Loss: 547.057292\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 551.560562\n",
      "\tTesting Loss: 547.737671\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 551.368924\n",
      "\tTesting Loss: 547.108398\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 551.637695\n",
      "\tTesting Loss: 547.340963\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 551.592100\n",
      "\tTesting Loss: 546.758464\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 551.693085\n",
      "\tTesting Loss: 547.317749\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 551.513626\n",
      "\tTesting Loss: 546.953491\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 551.508840\n",
      "\tTesting Loss: 547.503499\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 551.295619\n",
      "\tTesting Loss: 546.988861\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 551.532458\n",
      "\tTesting Loss: 547.273865\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 551.445953\n",
      "\tTesting Loss: 546.632812\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 551.636866\n",
      "\tTesting Loss: 547.103994\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 551.448130\n",
      "\tTesting Loss: 546.757304\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 551.474182\n",
      "\tTesting Loss: 547.442118\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 551.224673\n",
      "\tTesting Loss: 546.820618\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 551.441872\n",
      "\tTesting Loss: 547.128601\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 551.334735\n",
      "\tTesting Loss: 546.556163\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 551.543752\n",
      "\tTesting Loss: 547.032288\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 551.368296\n",
      "\tTesting Loss: 546.671061\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 551.421079\n",
      "\tTesting Loss: 547.370280\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 551.179291\n",
      "\tTesting Loss: 546.739807\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 551.343587\n",
      "\tTesting Loss: 547.087748\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 551.194069\n",
      "\tTesting Loss: 546.507172\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 551.451274\n",
      "\tTesting Loss: 546.881358\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 551.309161\n",
      "\tTesting Loss: 546.479197\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 551.401019\n",
      "\tTesting Loss: 547.089803\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 551.150210\n",
      "\tTesting Loss: 546.690633\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 551.312943\n",
      "\tTesting Loss: 547.247660\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 550.966774\n",
      "\tTesting Loss: 546.479146\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 551.289378\n",
      "\tTesting Loss: 546.834086\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 551.191861\n",
      "\tTesting Loss: 546.277496\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 551.367284\n",
      "\tTesting Loss: 546.904521\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 551.149923\n",
      "\tTesting Loss: 546.663534\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 551.246160\n",
      "\tTesting Loss: 547.275360\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 550.909625\n",
      "\tTesting Loss: 546.559428\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 551.210164\n",
      "\tTesting Loss: 546.804952\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 551.099609\n",
      "\tTesting Loss: 546.221385\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 551.275604\n",
      "\tTesting Loss: 546.656657\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 551.078690\n",
      "\tTesting Loss: 546.487345\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 551.201024\n",
      "\tTesting Loss: 547.263784\n",
      "\tLearning Rate: 0.000096977\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 550.856305\n",
      "\tTesting Loss: 546.362457\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 551.159342\n",
      "\tTesting Loss: 546.632731\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 550.967087\n",
      "\tTesting Loss: 546.153422\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 551.143951\n",
      "\tTesting Loss: 546.688782\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 550.973460\n",
      "\tTesting Loss: 546.371704\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 551.076111\n",
      "\tTesting Loss: 546.737091\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 550.850927\n",
      "\tTesting Loss: 546.199209\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 551.086377\n",
      "\tTesting Loss: 546.578593\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 550.881304\n",
      "\tTesting Loss: 546.112915\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 551.059298\n",
      "\tTesting Loss: 546.591105\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 550.839961\n",
      "\tTesting Loss: 546.246338\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 551.023476\n",
      "\tTesting Loss: 546.671916\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 550.808988\n",
      "\tTesting Loss: 546.132853\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 550.976329\n",
      "\tTesting Loss: 546.479431\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 550.845779\n",
      "\tTesting Loss: 546.092611\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 550.982908\n",
      "\tTesting Loss: 546.446126\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 550.737905\n",
      "\tTesting Loss: 546.048106\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 550.975731\n",
      "\tTesting Loss: 546.606496\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 550.731216\n",
      "\tTesting Loss: 545.962056\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 550.924062\n",
      "\tTesting Loss: 546.477661\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 550.755819\n",
      "\tTesting Loss: 546.008372\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 550.881482\n",
      "\tTesting Loss: 546.363424\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 550.730087\n",
      "\tTesting Loss: 546.017466\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 550.897583\n",
      "\tTesting Loss: 546.401377\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 550.645162\n",
      "\tTesting Loss: 545.813578\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 550.884369\n",
      "\tTesting Loss: 546.312368\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 550.662064\n",
      "\tTesting Loss: 545.949870\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 550.861389\n",
      "\tTesting Loss: 546.525167\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 550.640116\n",
      "\tTesting Loss: 545.880636\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 550.781039\n",
      "\tTesting Loss: 546.173401\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 550.631948\n",
      "\tTesting Loss: 545.814433\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 550.821841\n",
      "\tTesting Loss: 546.159353\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 550.569257\n",
      "\tTesting Loss: 545.761627\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 550.783244\n",
      "\tTesting Loss: 546.449483\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 550.544975\n",
      "\tTesting Loss: 545.755249\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 550.748767\n",
      "\tTesting Loss: 546.058726\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 550.546694\n",
      "\tTesting Loss: 545.627187\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 550.741847\n",
      "\tTesting Loss: 546.079671\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 550.534365\n",
      "\tTesting Loss: 545.869588\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 550.720965\n",
      "\tTesting Loss: 546.458333\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 550.439517\n",
      "\tTesting Loss: 545.613037\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 550.690882\n",
      "\tTesting Loss: 546.015228\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 550.510735\n",
      "\tTesting Loss: 545.621328\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 550.673604\n",
      "\tTesting Loss: 546.073364\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 550.469022\n",
      "\tTesting Loss: 545.660248\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 550.648359\n",
      "\tTesting Loss: 546.140828\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 550.412277\n",
      "\tTesting Loss: 545.523275\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 550.640386\n",
      "\tTesting Loss: 545.977142\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 550.397273\n",
      "\tTesting Loss: 545.395264\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 550.626574\n",
      "\tTesting Loss: 546.013468\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 550.360591\n",
      "\tTesting Loss: 545.480489\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 550.617200\n",
      "\tTesting Loss: 546.119202\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 550.371020\n",
      "\tTesting Loss: 545.397196\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 550.571991\n",
      "\tTesting Loss: 545.883097\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 550.411845\n",
      "\tTesting Loss: 545.478190\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 550.523303\n",
      "\tTesting Loss: 545.882263\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 550.323026\n",
      "\tTesting Loss: 545.358053\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 550.517965\n",
      "\tTesting Loss: 545.774272\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 550.304474\n",
      "\tTesting Loss: 545.268575\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 550.522769\n",
      "\tTesting Loss: 545.780263\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 550.256902\n",
      "\tTesting Loss: 545.299601\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 550.512469\n",
      "\tTesting Loss: 545.936198\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 550.237493\n",
      "\tTesting Loss: 545.311076\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 550.495710\n",
      "\tTesting Loss: 545.878591\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 550.242788\n",
      "\tTesting Loss: 545.251119\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 550.450083\n",
      "\tTesting Loss: 545.606272\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 550.274691\n",
      "\tTesting Loss: 545.491069\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 550.470840\n",
      "\tTesting Loss: 546.057027\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 550.235456\n",
      "\tTesting Loss: 545.552002\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 550.451803\n",
      "\tTesting Loss: 546.074504\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 550.253649\n",
      "\tTesting Loss: 545.799530\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 550.401499\n",
      "\tTesting Loss: 546.152730\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 550.243983\n",
      "\tTesting Loss: 545.595876\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 550.395439\n",
      "\tTesting Loss: 545.906403\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 550.254862\n",
      "\tTesting Loss: 545.803497\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 550.369298\n",
      "\tTesting Loss: 546.068807\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 550.196981\n",
      "\tTesting Loss: 545.677633\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 550.349820\n",
      "\tTesting Loss: 545.925435\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 550.179977\n",
      "\tTesting Loss: 545.702087\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 550.352341\n",
      "\tTesting Loss: 546.173492\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 550.088348\n",
      "\tTesting Loss: 546.087687\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 550.380272\n",
      "\tTesting Loss: 546.713674\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 550.129486\n",
      "\tTesting Loss: 546.458567\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 550.354716\n",
      "\tTesting Loss: 546.849172\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 550.136775\n",
      "\tTesting Loss: 546.506002\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 550.332446\n",
      "\tTesting Loss: 546.773488\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 550.223544\n",
      "\tTesting Loss: 546.705302\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 550.254608\n",
      "\tTesting Loss: 546.685852\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 550.116216\n",
      "\tTesting Loss: 546.384928\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 550.234652\n",
      "\tTesting Loss: 546.231018\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 550.054436\n",
      "\tTesting Loss: 546.401449\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 550.300321\n",
      "\tTesting Loss: 546.733582\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 549.996470\n",
      "\tTesting Loss: 546.385010\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 550.253342\n",
      "\tTesting Loss: 546.616374\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 549.978607\n",
      "\tTesting Loss: 546.135315\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 550.228322\n",
      "\tTesting Loss: 546.418121\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 550.044907\n",
      "\tTesting Loss: 546.463328\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 550.244034\n",
      "\tTesting Loss: 546.700012\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 550.041550\n",
      "\tTesting Loss: 546.276031\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 550.157908\n",
      "\tTesting Loss: 546.382955\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 550.110339\n",
      "\tTesting Loss: 546.357117\n",
      "\tLearning Rate: 0.000087280\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 550.138186\n",
      "\tTesting Loss: 546.238851\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 550.017115\n",
      "\tTesting Loss: 546.313660\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 550.173027\n",
      "\tTesting Loss: 546.326497\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 549.913610\n",
      "\tTesting Loss: 546.310445\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 550.160477\n",
      "\tTesting Loss: 546.383158\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 549.936005\n",
      "\tTesting Loss: 546.314667\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 550.152095\n",
      "\tTesting Loss: 546.474141\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 549.953674\n",
      "\tTesting Loss: 546.374084\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 550.133865\n",
      "\tTesting Loss: 546.382670\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 549.977343\n",
      "\tTesting Loss: 546.446808\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 550.106544\n",
      "\tTesting Loss: 546.349731\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 549.944214\n",
      "\tTesting Loss: 546.400411\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 550.125819\n",
      "\tTesting Loss: 546.460917\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 549.957176\n",
      "\tTesting Loss: 546.359619\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 550.083605\n",
      "\tTesting Loss: 546.274272\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 549.915502\n",
      "\tTesting Loss: 546.454793\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 550.099335\n",
      "\tTesting Loss: 546.380168\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 549.856567\n",
      "\tTesting Loss: 546.385600\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 550.101354\n",
      "\tTesting Loss: 546.512838\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 549.883718\n",
      "\tTesting Loss: 546.511088\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 550.077087\n",
      "\tTesting Loss: 546.557902\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 549.960973\n",
      "\tTesting Loss: 546.490194\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 550.042274\n",
      "\tTesting Loss: 546.468872\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 549.963776\n",
      "\tTesting Loss: 546.556844\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 550.036916\n",
      "\tTesting Loss: 546.389872\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 549.920041\n",
      "\tTesting Loss: 546.595479\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 550.033951\n",
      "\tTesting Loss: 546.469096\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 549.866366\n",
      "\tTesting Loss: 546.486552\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 550.028486\n",
      "\tTesting Loss: 546.486491\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 549.837067\n",
      "\tTesting Loss: 546.582072\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 550.005852\n",
      "\tTesting Loss: 546.502268\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 549.794454\n",
      "\tTesting Loss: 546.430278\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 550.000427\n",
      "\tTesting Loss: 546.520640\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 549.853480\n",
      "\tTesting Loss: 546.691813\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 549.995811\n",
      "\tTesting Loss: 546.581665\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 549.840495\n",
      "\tTesting Loss: 546.583740\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 549.956871\n",
      "\tTesting Loss: 546.458598\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 549.836970\n",
      "\tTesting Loss: 546.675629\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 549.952103\n",
      "\tTesting Loss: 546.663167\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 549.805428\n",
      "\tTesting Loss: 546.652618\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 549.930585\n",
      "\tTesting Loss: 546.794596\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 549.771413\n",
      "\tTesting Loss: 546.652598\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 549.929423\n",
      "\tTesting Loss: 546.709717\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 549.774261\n",
      "\tTesting Loss: 546.571706\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 549.897975\n",
      "\tTesting Loss: 546.781453\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 549.799489\n",
      "\tTesting Loss: 546.748637\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 549.864426\n",
      "\tTesting Loss: 547.054301\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 549.710070\n",
      "\tTesting Loss: 546.677734\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 549.876740\n",
      "\tTesting Loss: 547.207072\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 549.679189\n",
      "\tTesting Loss: 546.633016\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 549.877477\n",
      "\tTesting Loss: 547.190643\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 549.710141\n",
      "\tTesting Loss: 546.638743\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 549.859619\n",
      "\tTesting Loss: 547.200684\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 549.749173\n",
      "\tTesting Loss: 546.664327\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 549.827922\n",
      "\tTesting Loss: 546.860596\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 549.740705\n",
      "\tTesting Loss: 546.586294\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 549.828522\n",
      "\tTesting Loss: 547.171875\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 549.780065\n",
      "\tTesting Loss: 546.757100\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 549.803240\n",
      "\tTesting Loss: 546.919556\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 549.735087\n",
      "\tTesting Loss: 546.599833\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 549.802607\n",
      "\tTesting Loss: 547.394928\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 549.738948\n",
      "\tTesting Loss: 546.784688\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 549.790871\n",
      "\tTesting Loss: 547.209483\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 549.684809\n",
      "\tTesting Loss: 546.592061\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 549.772769\n",
      "\tTesting Loss: 547.461507\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 549.664187\n",
      "\tTesting Loss: 546.789795\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 549.790344\n",
      "\tTesting Loss: 547.584513\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 549.580671\n",
      "\tTesting Loss: 546.671590\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 549.791196\n",
      "\tTesting Loss: 547.797516\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 549.663849\n",
      "\tTesting Loss: 546.904867\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 549.770500\n",
      "\tTesting Loss: 547.594910\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 549.634588\n",
      "\tTesting Loss: 546.695496\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 549.751539\n",
      "\tTesting Loss: 547.683757\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 549.676389\n",
      "\tTesting Loss: 547.005035\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 549.761019\n",
      "\tTesting Loss: 547.634460\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 549.612350\n",
      "\tTesting Loss: 546.713582\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 549.734756\n",
      "\tTesting Loss: 547.784302\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 549.650436\n",
      "\tTesting Loss: 547.111348\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 549.752340\n",
      "\tTesting Loss: 547.747437\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 549.614924\n",
      "\tTesting Loss: 546.762980\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 549.737475\n",
      "\tTesting Loss: 547.876363\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 549.644803\n",
      "\tTesting Loss: 547.099711\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 549.754730\n",
      "\tTesting Loss: 547.777323\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 549.630620\n",
      "\tTesting Loss: 546.953918\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 549.700556\n",
      "\tTesting Loss: 547.743144\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 549.659571\n",
      "\tTesting Loss: 547.307454\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 549.710978\n",
      "\tTesting Loss: 547.598531\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 549.637365\n",
      "\tTesting Loss: 547.076996\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 549.679532\n",
      "\tTesting Loss: 547.829610\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 549.643112\n",
      "\tTesting Loss: 547.414042\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 549.676259\n",
      "\tTesting Loss: 547.640686\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 549.555519\n",
      "\tTesting Loss: 546.905640\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 549.635396\n",
      "\tTesting Loss: 547.935791\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 549.578568\n",
      "\tTesting Loss: 547.402720\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 549.662649\n",
      "\tTesting Loss: 547.775248\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 549.464432\n",
      "\tTesting Loss: 546.679372\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 549.628466\n",
      "\tTesting Loss: 548.121460\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 549.521372\n",
      "\tTesting Loss: 547.400411\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 549.649114\n",
      "\tTesting Loss: 547.811412\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 549.441193\n",
      "\tTesting Loss: 546.767558\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 549.611081\n",
      "\tTesting Loss: 548.051239\n",
      "\tLearning Rate: 0.000078552\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 549.575439\n",
      "\tTesting Loss: 547.561391\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 549.614080\n",
      "\tTesting Loss: 547.502482\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 549.530039\n",
      "\tTesting Loss: 547.414734\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 549.593750\n",
      "\tTesting Loss: 547.850281\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 549.586555\n",
      "\tTesting Loss: 547.417501\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 549.587423\n",
      "\tTesting Loss: 547.499797\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 549.587906\n",
      "\tTesting Loss: 547.778310\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 549.592896\n",
      "\tTesting Loss: 547.546346\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 549.580190\n",
      "\tTesting Loss: 547.596680\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 549.543076\n",
      "\tTesting Loss: 547.593486\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 549.570999\n",
      "\tTesting Loss: 547.801371\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 549.544678\n",
      "\tTesting Loss: 547.466146\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 549.551498\n",
      "\tTesting Loss: 547.841329\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 549.440196\n",
      "\tTesting Loss: 547.252777\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 549.547699\n",
      "\tTesting Loss: 548.002848\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 549.399790\n",
      "\tTesting Loss: 547.083354\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 549.530390\n",
      "\tTesting Loss: 547.967916\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 549.396901\n",
      "\tTesting Loss: 547.368754\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 549.511780\n",
      "\tTesting Loss: 547.787994\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 549.407130\n",
      "\tTesting Loss: 547.334991\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 549.478307\n",
      "\tTesting Loss: 547.855408\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 549.388616\n",
      "\tTesting Loss: 547.225159\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 549.472305\n",
      "\tTesting Loss: 547.863485\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 549.328008\n",
      "\tTesting Loss: 547.197103\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 549.444855\n",
      "\tTesting Loss: 547.873657\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 549.296549\n",
      "\tTesting Loss: 547.225820\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 549.388685\n",
      "\tTesting Loss: 547.780304\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 549.273641\n",
      "\tTesting Loss: 547.258972\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 549.336136\n",
      "\tTesting Loss: 547.740987\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 549.225324\n",
      "\tTesting Loss: 547.157817\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 549.275721\n",
      "\tTesting Loss: 547.731222\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 549.161385\n",
      "\tTesting Loss: 547.013936\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 549.190603\n",
      "\tTesting Loss: 547.647725\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 549.014206\n",
      "\tTesting Loss: 546.676483\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 549.005173\n",
      "\tTesting Loss: 547.420959\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 548.730789\n",
      "\tTesting Loss: 546.325073\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 548.887985\n",
      "\tTesting Loss: 546.556305\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 548.714055\n",
      "\tTesting Loss: 546.413940\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 549.127797\n",
      "\tTesting Loss: 546.508240\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 548.984647\n",
      "\tTesting Loss: 546.725159\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 548.850802\n",
      "\tTesting Loss: 546.198995\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 549.246071\n",
      "\tTesting Loss: 546.890381\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 549.199066\n",
      "\tTesting Loss: 547.279541\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 548.815633\n",
      "\tTesting Loss: 546.086273\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 549.028379\n",
      "\tTesting Loss: 544.261332\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 549.026967\n",
      "\tTesting Loss: 546.210714\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 548.830856\n",
      "\tTesting Loss: 547.716604\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 548.904444\n",
      "\tTesting Loss: 545.434408\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 549.119191\n",
      "\tTesting Loss: 547.721608\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 548.868902\n",
      "\tTesting Loss: 545.845622\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 549.122083\n",
      "\tTesting Loss: 547.589050\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 548.717677\n",
      "\tTesting Loss: 545.764984\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 549.138672\n",
      "\tTesting Loss: 547.570211\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 548.714076\n",
      "\tTesting Loss: 545.838114\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 549.155101\n",
      "\tTesting Loss: 547.592163\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 548.661204\n",
      "\tTesting Loss: 545.577515\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 549.128428\n",
      "\tTesting Loss: 547.639048\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 548.688019\n",
      "\tTesting Loss: 545.744110\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 549.164246\n",
      "\tTesting Loss: 547.691874\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 548.661667\n",
      "\tTesting Loss: 545.236776\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 549.112872\n",
      "\tTesting Loss: 547.684306\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 548.699753\n",
      "\tTesting Loss: 545.395335\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 549.151082\n",
      "\tTesting Loss: 547.754191\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 548.661728\n",
      "\tTesting Loss: 544.769735\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 549.115295\n",
      "\tTesting Loss: 547.792409\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 548.731547\n",
      "\tTesting Loss: 545.095805\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 549.167603\n",
      "\tTesting Loss: 547.916738\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 548.810598\n",
      "\tTesting Loss: 544.669851\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 549.133519\n",
      "\tTesting Loss: 547.961650\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 548.868342\n",
      "\tTesting Loss: 544.809448\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 549.137665\n",
      "\tTesting Loss: 547.926270\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 548.962423\n",
      "\tTesting Loss: 545.529215\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 549.078644\n",
      "\tTesting Loss: 547.630066\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 548.990362\n",
      "\tTesting Loss: 546.051127\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 549.001633\n",
      "\tTesting Loss: 547.199788\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 548.892054\n",
      "\tTesting Loss: 545.845011\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 549.009852\n",
      "\tTesting Loss: 547.119486\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 548.809341\n",
      "\tTesting Loss: 545.565318\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 549.045385\n",
      "\tTesting Loss: 547.266012\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 548.736028\n",
      "\tTesting Loss: 545.213928\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 549.056946\n",
      "\tTesting Loss: 547.450663\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 548.668401\n",
      "\tTesting Loss: 544.968486\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 549.062342\n",
      "\tTesting Loss: 547.547262\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 548.717112\n",
      "\tTesting Loss: 544.918193\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 549.054123\n",
      "\tTesting Loss: 547.578573\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 548.769104\n",
      "\tTesting Loss: 545.028442\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 549.050913\n",
      "\tTesting Loss: 547.549093\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 548.828166\n",
      "\tTesting Loss: 545.240967\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 549.021088\n",
      "\tTesting Loss: 547.436371\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 548.830877\n",
      "\tTesting Loss: 545.378052\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 548.997106\n",
      "\tTesting Loss: 547.337769\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 548.815974\n",
      "\tTesting Loss: 545.490214\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 548.980591\n",
      "\tTesting Loss: 547.195526\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 548.791962\n",
      "\tTesting Loss: 545.545797\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 548.971664\n",
      "\tTesting Loss: 547.110606\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 548.769735\n",
      "\tTesting Loss: 545.469137\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 548.965314\n",
      "\tTesting Loss: 547.119659\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 548.705737\n",
      "\tTesting Loss: 545.271220\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 548.972402\n",
      "\tTesting Loss: 547.184285\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 548.677526\n",
      "\tTesting Loss: 545.171112\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 548.972961\n",
      "\tTesting Loss: 547.221842\n",
      "\tLearning Rate: 0.000070697\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 548.682256\n",
      "\tTesting Loss: 545.164480\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 548.957235\n",
      "\tTesting Loss: 547.230448\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 548.698675\n",
      "\tTesting Loss: 545.145131\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 548.946528\n",
      "\tTesting Loss: 547.197062\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 548.758174\n",
      "\tTesting Loss: 545.367106\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 548.911558\n",
      "\tTesting Loss: 547.079468\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 548.754298\n",
      "\tTesting Loss: 545.365224\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 548.892624\n",
      "\tTesting Loss: 547.004303\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 548.723618\n",
      "\tTesting Loss: 545.371785\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 548.892543\n",
      "\tTesting Loss: 546.954244\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 548.716232\n",
      "\tTesting Loss: 545.392924\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 548.887217\n",
      "\tTesting Loss: 546.900909\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 548.717791\n",
      "\tTesting Loss: 545.506948\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 548.867610\n",
      "\tTesting Loss: 546.826579\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 548.703766\n",
      "\tTesting Loss: 545.413381\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 548.858836\n",
      "\tTesting Loss: 546.849996\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 548.656886\n",
      "\tTesting Loss: 545.229248\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 548.856176\n",
      "\tTesting Loss: 546.888418\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 548.615153\n",
      "\tTesting Loss: 545.142802\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 548.851255\n",
      "\tTesting Loss: 546.896891\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 548.620089\n",
      "\tTesting Loss: 545.095123\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 548.839422\n",
      "\tTesting Loss: 546.906799\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 548.609756\n",
      "\tTesting Loss: 545.032094\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 548.828328\n",
      "\tTesting Loss: 546.893514\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 548.600021\n",
      "\tTesting Loss: 545.032247\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 548.820775\n",
      "\tTesting Loss: 546.885600\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 548.611008\n",
      "\tTesting Loss: 545.033366\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 548.805654\n",
      "\tTesting Loss: 546.845072\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 548.617439\n",
      "\tTesting Loss: 545.078440\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 548.798721\n",
      "\tTesting Loss: 546.817688\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 548.614034\n",
      "\tTesting Loss: 545.118896\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 548.789075\n",
      "\tTesting Loss: 546.749878\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 548.634115\n",
      "\tTesting Loss: 545.219767\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 548.776024\n",
      "\tTesting Loss: 546.667257\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 548.630740\n",
      "\tTesting Loss: 545.231110\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 548.758423\n",
      "\tTesting Loss: 546.623596\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 548.603317\n",
      "\tTesting Loss: 545.192149\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 548.749430\n",
      "\tTesting Loss: 546.598165\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 548.588730\n",
      "\tTesting Loss: 545.165853\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 548.743556\n",
      "\tTesting Loss: 546.569814\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 548.569366\n",
      "\tTesting Loss: 545.146484\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 548.739950\n",
      "\tTesting Loss: 546.531657\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 548.585548\n",
      "\tTesting Loss: 545.178914\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 548.724546\n",
      "\tTesting Loss: 546.491597\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 548.570002\n",
      "\tTesting Loss: 545.151245\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 548.702204\n",
      "\tTesting Loss: 546.456024\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 548.533076\n",
      "\tTesting Loss: 545.036438\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 548.696391\n",
      "\tTesting Loss: 546.471944\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 548.504130\n",
      "\tTesting Loss: 544.979329\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 548.692797\n",
      "\tTesting Loss: 546.458832\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 548.501892\n",
      "\tTesting Loss: 544.980011\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 548.682200\n",
      "\tTesting Loss: 546.431844\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 548.503403\n",
      "\tTesting Loss: 544.959534\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 548.672740\n",
      "\tTesting Loss: 546.424540\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 548.492037\n",
      "\tTesting Loss: 544.936859\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 548.652184\n",
      "\tTesting Loss: 546.395253\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 548.464450\n",
      "\tTesting Loss: 544.770671\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 548.639371\n",
      "\tTesting Loss: 546.359762\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 548.416835\n",
      "\tTesting Loss: 544.712362\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 548.634048\n",
      "\tTesting Loss: 546.362752\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 548.418442\n",
      "\tTesting Loss: 544.749247\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 548.635564\n",
      "\tTesting Loss: 546.351461\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 548.449125\n",
      "\tTesting Loss: 544.810262\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 548.619342\n",
      "\tTesting Loss: 546.319499\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 548.462072\n",
      "\tTesting Loss: 544.813558\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 548.598145\n",
      "\tTesting Loss: 546.253784\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 548.428510\n",
      "\tTesting Loss: 544.789449\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 548.606267\n",
      "\tTesting Loss: 546.255534\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 548.446757\n",
      "\tTesting Loss: 544.891907\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 548.595790\n",
      "\tTesting Loss: 546.190084\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 548.459958\n",
      "\tTesting Loss: 544.928782\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 548.577174\n",
      "\tTesting Loss: 546.115641\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 548.443832\n",
      "\tTesting Loss: 544.910970\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 548.563268\n",
      "\tTesting Loss: 546.071452\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 548.417521\n",
      "\tTesting Loss: 544.872559\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 548.552910\n",
      "\tTesting Loss: 546.033569\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 548.415243\n",
      "\tTesting Loss: 544.886943\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 548.549047\n",
      "\tTesting Loss: 545.991536\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 548.411972\n",
      "\tTesting Loss: 544.909322\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 548.536184\n",
      "\tTesting Loss: 545.945343\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 548.404500\n",
      "\tTesting Loss: 544.868917\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 548.521062\n",
      "\tTesting Loss: 545.903687\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 548.394524\n",
      "\tTesting Loss: 544.865224\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 548.506826\n",
      "\tTesting Loss: 545.870707\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 548.355896\n",
      "\tTesting Loss: 544.769145\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 548.499944\n",
      "\tTesting Loss: 545.869914\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 548.350789\n",
      "\tTesting Loss: 544.763570\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 548.493830\n",
      "\tTesting Loss: 545.839640\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 548.342590\n",
      "\tTesting Loss: 544.729574\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 548.487434\n",
      "\tTesting Loss: 545.815867\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 548.336426\n",
      "\tTesting Loss: 544.728312\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 548.476679\n",
      "\tTesting Loss: 545.779887\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 548.338834\n",
      "\tTesting Loss: 544.716634\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 548.458084\n",
      "\tTesting Loss: 545.736593\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 548.324539\n",
      "\tTesting Loss: 544.664978\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 548.440595\n",
      "\tTesting Loss: 545.707489\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 548.296176\n",
      "\tTesting Loss: 544.604553\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 548.434097\n",
      "\tTesting Loss: 545.693990\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 548.266764\n",
      "\tTesting Loss: 544.481323\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 548.419795\n",
      "\tTesting Loss: 545.664205\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 548.246404\n",
      "\tTesting Loss: 544.522156\n",
      "\tLearning Rate: 0.000063627\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 548.431300\n",
      "\tTesting Loss: 545.641408\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 548.293248\n",
      "\tTesting Loss: 544.583110\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 548.419006\n",
      "\tTesting Loss: 545.604919\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 548.324524\n",
      "\tTesting Loss: 544.686462\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 548.406606\n",
      "\tTesting Loss: 545.493754\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 548.337135\n",
      "\tTesting Loss: 544.761373\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 548.398026\n",
      "\tTesting Loss: 545.418447\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 548.342951\n",
      "\tTesting Loss: 544.807617\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 548.379545\n",
      "\tTesting Loss: 545.341024\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 548.326256\n",
      "\tTesting Loss: 544.785756\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 548.366778\n",
      "\tTesting Loss: 545.284505\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 548.312892\n",
      "\tTesting Loss: 544.716949\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 548.360987\n",
      "\tTesting Loss: 545.268026\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 548.286196\n",
      "\tTesting Loss: 544.619609\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 548.351084\n",
      "\tTesting Loss: 545.233948\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 548.269999\n",
      "\tTesting Loss: 544.584229\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 548.347941\n",
      "\tTesting Loss: 545.224833\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 548.256261\n",
      "\tTesting Loss: 544.507212\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 548.341652\n",
      "\tTesting Loss: 545.202840\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 548.253759\n",
      "\tTesting Loss: 544.484151\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 548.333974\n",
      "\tTesting Loss: 545.166921\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 548.252060\n",
      "\tTesting Loss: 544.465963\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 548.324610\n",
      "\tTesting Loss: 545.139303\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 548.234596\n",
      "\tTesting Loss: 544.402649\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 548.316610\n",
      "\tTesting Loss: 545.120209\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 548.223495\n",
      "\tTesting Loss: 544.345530\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 548.310318\n",
      "\tTesting Loss: 545.100016\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 548.204269\n",
      "\tTesting Loss: 544.307556\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 548.311122\n",
      "\tTesting Loss: 545.083761\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 548.205617\n",
      "\tTesting Loss: 544.288788\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 548.302195\n",
      "\tTesting Loss: 545.054108\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 548.210653\n",
      "\tTesting Loss: 544.299733\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 548.301036\n",
      "\tTesting Loss: 545.032878\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 548.204992\n",
      "\tTesting Loss: 544.274556\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 548.289032\n",
      "\tTesting Loss: 544.994019\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 548.196162\n",
      "\tTesting Loss: 544.242493\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 548.284449\n",
      "\tTesting Loss: 544.964071\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 548.192454\n",
      "\tTesting Loss: 544.222890\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 548.280726\n",
      "\tTesting Loss: 544.931610\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 548.188807\n",
      "\tTesting Loss: 544.211497\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 548.271042\n",
      "\tTesting Loss: 544.905853\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 548.189400\n",
      "\tTesting Loss: 544.204203\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 548.261937\n",
      "\tTesting Loss: 544.872630\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 548.175247\n",
      "\tTesting Loss: 544.135417\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 548.251574\n",
      "\tTesting Loss: 544.857951\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 548.142481\n",
      "\tTesting Loss: 544.014018\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 548.244222\n",
      "\tTesting Loss: 544.830699\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 548.098333\n",
      "\tTesting Loss: 543.924784\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 548.246877\n",
      "\tTesting Loss: 544.822662\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 548.101947\n",
      "\tTesting Loss: 543.938822\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 548.246155\n",
      "\tTesting Loss: 544.802999\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 548.119217\n",
      "\tTesting Loss: 543.954610\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 548.250738\n",
      "\tTesting Loss: 544.792948\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 548.121796\n",
      "\tTesting Loss: 543.978241\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 548.239349\n",
      "\tTesting Loss: 544.750264\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 548.131872\n",
      "\tTesting Loss: 543.996318\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 548.231379\n",
      "\tTesting Loss: 544.699198\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 548.149661\n",
      "\tTesting Loss: 544.032491\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 548.218920\n",
      "\tTesting Loss: 544.646729\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 548.147374\n",
      "\tTesting Loss: 544.013641\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 548.203206\n",
      "\tTesting Loss: 544.613037\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 548.141253\n",
      "\tTesting Loss: 543.997253\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 548.189430\n",
      "\tTesting Loss: 544.574677\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 548.106801\n",
      "\tTesting Loss: 543.903809\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 548.181023\n",
      "\tTesting Loss: 544.546061\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 548.070343\n",
      "\tTesting Loss: 543.818451\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 548.191910\n",
      "\tTesting Loss: 544.544607\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 548.050522\n",
      "\tTesting Loss: 543.772746\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 548.192284\n",
      "\tTesting Loss: 544.522909\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 548.076332\n",
      "\tTesting Loss: 543.809143\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 548.182729\n",
      "\tTesting Loss: 544.492859\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 548.075384\n",
      "\tTesting Loss: 543.800903\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 548.168767\n",
      "\tTesting Loss: 544.449992\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 548.067220\n",
      "\tTesting Loss: 543.766988\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 548.163844\n",
      "\tTesting Loss: 544.420522\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 548.044022\n",
      "\tTesting Loss: 543.724843\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 548.164261\n",
      "\tTesting Loss: 544.415751\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 548.038282\n",
      "\tTesting Loss: 543.687093\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 548.162526\n",
      "\tTesting Loss: 544.393178\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 548.039612\n",
      "\tTesting Loss: 543.692485\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 548.153124\n",
      "\tTesting Loss: 544.347005\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 548.046265\n",
      "\tTesting Loss: 543.712504\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 548.144114\n",
      "\tTesting Loss: 544.309377\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 548.047333\n",
      "\tTesting Loss: 543.697245\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 548.135040\n",
      "\tTesting Loss: 544.271983\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 548.038681\n",
      "\tTesting Loss: 543.677989\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 548.118612\n",
      "\tTesting Loss: 544.245310\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 548.018560\n",
      "\tTesting Loss: 543.591777\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 548.112144\n",
      "\tTesting Loss: 544.205516\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.984993\n",
      "\tTesting Loss: 543.529856\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 548.113767\n",
      "\tTesting Loss: 544.191081\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.975632\n",
      "\tTesting Loss: 543.516632\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 548.126185\n",
      "\tTesting Loss: 544.183380\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 548.002920\n",
      "\tTesting Loss: 543.577281\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 548.115135\n",
      "\tTesting Loss: 544.116760\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 548.016047\n",
      "\tTesting Loss: 543.639933\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 548.108287\n",
      "\tTesting Loss: 544.072581\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 548.028117\n",
      "\tTesting Loss: 543.658610\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 548.085500\n",
      "\tTesting Loss: 544.020640\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 548.033229\n",
      "\tTesting Loss: 543.677795\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 548.076159\n",
      "\tTesting Loss: 543.955241\n",
      "\tLearning Rate: 0.000057264\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 548.023417\n",
      "\tTesting Loss: 543.675639\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 548.067337\n",
      "\tTesting Loss: 543.920736\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 548.027578\n",
      "\tTesting Loss: 543.669678\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 548.053884\n",
      "\tTesting Loss: 543.877187\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 548.019986\n",
      "\tTesting Loss: 543.667857\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 548.038671\n",
      "\tTesting Loss: 543.825358\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 548.012705\n",
      "\tTesting Loss: 543.661662\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 548.029485\n",
      "\tTesting Loss: 543.769491\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 548.007426\n",
      "\tTesting Loss: 543.659505\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 548.019353\n",
      "\tTesting Loss: 543.703206\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 548.004735\n",
      "\tTesting Loss: 543.665100\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.991557\n",
      "\tTesting Loss: 543.623830\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 548.005544\n",
      "\tTesting Loss: 543.651998\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.988406\n",
      "\tTesting Loss: 543.581340\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.999079\n",
      "\tTesting Loss: 543.632599\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.972448\n",
      "\tTesting Loss: 543.528280\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 548.001109\n",
      "\tTesting Loss: 543.620585\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.968211\n",
      "\tTesting Loss: 543.479909\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 548.002314\n",
      "\tTesting Loss: 543.584473\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.964610\n",
      "\tTesting Loss: 543.473796\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.994069\n",
      "\tTesting Loss: 543.554362\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.963303\n",
      "\tTesting Loss: 543.459615\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.985382\n",
      "\tTesting Loss: 543.512207\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.957377\n",
      "\tTesting Loss: 543.434102\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.969994\n",
      "\tTesting Loss: 543.461914\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.947459\n",
      "\tTesting Loss: 543.403666\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.958171\n",
      "\tTesting Loss: 543.428284\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.939713\n",
      "\tTesting Loss: 543.385000\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.949000\n",
      "\tTesting Loss: 543.376994\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.927485\n",
      "\tTesting Loss: 543.340291\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.949290\n",
      "\tTesting Loss: 543.357371\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.927699\n",
      "\tTesting Loss: 543.327382\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.930283\n",
      "\tTesting Loss: 543.303507\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.935888\n",
      "\tTesting Loss: 543.329569\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.910543\n",
      "\tTesting Loss: 543.222127\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.934143\n",
      "\tTesting Loss: 543.297770\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.894666\n",
      "\tTesting Loss: 543.185140\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.947736\n",
      "\tTesting Loss: 543.301819\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.874751\n",
      "\tTesting Loss: 543.102966\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.953440\n",
      "\tTesting Loss: 543.288859\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.880117\n",
      "\tTesting Loss: 543.123128\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.945897\n",
      "\tTesting Loss: 543.240641\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.876129\n",
      "\tTesting Loss: 543.095286\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.928655\n",
      "\tTesting Loss: 543.185699\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.881246\n",
      "\tTesting Loss: 543.093374\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.919474\n",
      "\tTesting Loss: 543.149038\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.872040\n",
      "\tTesting Loss: 543.072774\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.902845\n",
      "\tTesting Loss: 543.090953\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.876015\n",
      "\tTesting Loss: 543.063049\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.885938\n",
      "\tTesting Loss: 543.024394\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.880280\n",
      "\tTesting Loss: 543.030334\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.857686\n",
      "\tTesting Loss: 542.977325\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.890004\n",
      "\tTesting Loss: 543.033193\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.843274\n",
      "\tTesting Loss: 542.925618\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.896210\n",
      "\tTesting Loss: 543.000224\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.822355\n",
      "\tTesting Loss: 542.879974\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.891520\n",
      "\tTesting Loss: 542.979838\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.820043\n",
      "\tTesting Loss: 542.861267\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.895391\n",
      "\tTesting Loss: 542.950887\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.812927\n",
      "\tTesting Loss: 542.831665\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.893539\n",
      "\tTesting Loss: 542.930522\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.810415\n",
      "\tTesting Loss: 542.807556\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.887243\n",
      "\tTesting Loss: 542.891347\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.805623\n",
      "\tTesting Loss: 542.785919\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.883306\n",
      "\tTesting Loss: 542.866180\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.793940\n",
      "\tTesting Loss: 542.738668\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.878743\n",
      "\tTesting Loss: 542.829020\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.811267\n",
      "\tTesting Loss: 542.753947\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.861821\n",
      "\tTesting Loss: 542.772705\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.820806\n",
      "\tTesting Loss: 542.733704\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.838882\n",
      "\tTesting Loss: 542.740936\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.827566\n",
      "\tTesting Loss: 542.712667\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.816874\n",
      "\tTesting Loss: 542.660278\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.824122\n",
      "\tTesting Loss: 542.681366\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.799683\n",
      "\tTesting Loss: 542.642100\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.826876\n",
      "\tTesting Loss: 542.640462\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.762436\n",
      "\tTesting Loss: 542.550171\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.827817\n",
      "\tTesting Loss: 542.602854\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.758433\n",
      "\tTesting Loss: 542.552134\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.830007\n",
      "\tTesting Loss: 542.574880\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.738920\n",
      "\tTesting Loss: 542.508687\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.834351\n",
      "\tTesting Loss: 542.545369\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.740636\n",
      "\tTesting Loss: 542.506215\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.839422\n",
      "\tTesting Loss: 542.537028\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.747228\n",
      "\tTesting Loss: 542.466187\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.825353\n",
      "\tTesting Loss: 542.492655\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.756518\n",
      "\tTesting Loss: 542.466634\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.816864\n",
      "\tTesting Loss: 542.445333\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.764666\n",
      "\tTesting Loss: 542.430013\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.805077\n",
      "\tTesting Loss: 542.421407\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.775614\n",
      "\tTesting Loss: 542.400981\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.786074\n",
      "\tTesting Loss: 542.371073\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.774760\n",
      "\tTesting Loss: 542.367503\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.778392\n",
      "\tTesting Loss: 542.342244\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.774297\n",
      "\tTesting Loss: 542.324982\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.765780\n",
      "\tTesting Loss: 542.309204\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.769501\n",
      "\tTesting Loss: 542.286174\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.762314\n",
      "\tTesting Loss: 542.265523\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.760941\n",
      "\tTesting Loss: 542.247019\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.758616\n",
      "\tTesting Loss: 542.237305\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.759432\n",
      "\tTesting Loss: 542.216227\n",
      "\tLearning Rate: 0.000051538\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.756460\n",
      "\tTesting Loss: 542.221151\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.749283\n",
      "\tTesting Loss: 542.180145\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.745506\n",
      "\tTesting Loss: 542.154043\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.745229\n",
      "\tTesting Loss: 542.162496\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.744212\n",
      "\tTesting Loss: 542.132304\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.748057\n",
      "\tTesting Loss: 542.123515\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.735390\n",
      "\tTesting Loss: 542.106954\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.742643\n",
      "\tTesting Loss: 542.095581\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.720164\n",
      "\tTesting Loss: 542.042348\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.736796\n",
      "\tTesting Loss: 542.079580\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.734909\n",
      "\tTesting Loss: 542.021810\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.722710\n",
      "\tTesting Loss: 542.045268\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.753199\n",
      "\tTesting Loss: 542.006999\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.695325\n",
      "\tTesting Loss: 541.981934\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.764511\n",
      "\tTesting Loss: 541.980143\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.668798\n",
      "\tTesting Loss: 541.963959\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.771973\n",
      "\tTesting Loss: 541.927063\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.653076\n",
      "\tTesting Loss: 541.922109\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.770068\n",
      "\tTesting Loss: 541.904643\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.654083\n",
      "\tTesting Loss: 541.903839\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.774551\n",
      "\tTesting Loss: 541.885600\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.664062\n",
      "\tTesting Loss: 541.872640\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.763112\n",
      "\tTesting Loss: 541.847260\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.679461\n",
      "\tTesting Loss: 541.854126\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.750847\n",
      "\tTesting Loss: 541.815043\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.683286\n",
      "\tTesting Loss: 541.799123\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.744146\n",
      "\tTesting Loss: 541.774597\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.697418\n",
      "\tTesting Loss: 541.777232\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.720072\n",
      "\tTesting Loss: 541.731425\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.711136\n",
      "\tTesting Loss: 541.730042\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.710932\n",
      "\tTesting Loss: 541.704702\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.708984\n",
      "\tTesting Loss: 541.685669\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.709742\n",
      "\tTesting Loss: 541.674062\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.708313\n",
      "\tTesting Loss: 541.658651\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.710719\n",
      "\tTesting Loss: 541.628479\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.702260\n",
      "\tTesting Loss: 541.611959\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.710510\n",
      "\tTesting Loss: 541.609355\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.701124\n",
      "\tTesting Loss: 541.569448\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.701777\n",
      "\tTesting Loss: 541.567871\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.691844\n",
      "\tTesting Loss: 541.537496\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.704198\n",
      "\tTesting Loss: 541.529399\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.694364\n",
      "\tTesting Loss: 541.511587\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.702459\n",
      "\tTesting Loss: 541.489105\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.697805\n",
      "\tTesting Loss: 541.489441\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.698395\n",
      "\tTesting Loss: 541.461019\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.697937\n",
      "\tTesting Loss: 541.446757\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.686579\n",
      "\tTesting Loss: 541.414836\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.695470\n",
      "\tTesting Loss: 541.410075\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.688309\n",
      "\tTesting Loss: 541.360107\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.670542\n",
      "\tTesting Loss: 541.399028\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.706757\n",
      "\tTesting Loss: 541.317688\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.634176\n",
      "\tTesting Loss: 541.362142\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.716827\n",
      "\tTesting Loss: 541.285807\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.591568\n",
      "\tTesting Loss: 541.309408\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.724365\n",
      "\tTesting Loss: 541.203389\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.598796\n",
      "\tTesting Loss: 541.338776\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.721400\n",
      "\tTesting Loss: 541.212779\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.624756\n",
      "\tTesting Loss: 541.269572\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.707916\n",
      "\tTesting Loss: 541.146454\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.617175\n",
      "\tTesting Loss: 541.253092\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.708689\n",
      "\tTesting Loss: 541.136108\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.596331\n",
      "\tTesting Loss: 541.166911\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.721863\n",
      "\tTesting Loss: 541.116964\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.590322\n",
      "\tTesting Loss: 541.175761\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.716812\n",
      "\tTesting Loss: 541.073079\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.608131\n",
      "\tTesting Loss: 541.156453\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.701350\n",
      "\tTesting Loss: 541.026052\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.605436\n",
      "\tTesting Loss: 541.098958\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.707418\n",
      "\tTesting Loss: 541.004232\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.605708\n",
      "\tTesting Loss: 541.073568\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.702215\n",
      "\tTesting Loss: 540.985097\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.608266\n",
      "\tTesting Loss: 541.035756\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.698746\n",
      "\tTesting Loss: 540.962056\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.618202\n",
      "\tTesting Loss: 540.987081\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.692708\n",
      "\tTesting Loss: 540.940755\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.642883\n",
      "\tTesting Loss: 540.970652\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.671046\n",
      "\tTesting Loss: 540.923665\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.669065\n",
      "\tTesting Loss: 540.916951\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.655060\n",
      "\tTesting Loss: 540.900045\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.669622\n",
      "\tTesting Loss: 540.897990\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.656738\n",
      "\tTesting Loss: 540.864451\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.660344\n",
      "\tTesting Loss: 540.862122\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.657272\n",
      "\tTesting Loss: 540.836629\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.663162\n",
      "\tTesting Loss: 540.824320\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.660024\n",
      "\tTesting Loss: 540.815023\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.651652\n",
      "\tTesting Loss: 540.793599\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.652105\n",
      "\tTesting Loss: 540.770955\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.658203\n",
      "\tTesting Loss: 540.774638\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.657010\n",
      "\tTesting Loss: 540.747467\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.646881\n",
      "\tTesting Loss: 540.717021\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.652740\n",
      "\tTesting Loss: 540.741211\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.646296\n",
      "\tTesting Loss: 540.708964\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.655454\n",
      "\tTesting Loss: 540.681071\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.632790\n",
      "\tTesting Loss: 540.669525\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.644007\n",
      "\tTesting Loss: 540.658488\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.647392\n",
      "\tTesting Loss: 540.656718\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.633530\n",
      "\tTesting Loss: 540.625203\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.636297\n",
      "\tTesting Loss: 540.620646\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.639519\n",
      "\tTesting Loss: 540.611267\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.623484\n",
      "\tTesting Loss: 540.596415\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.632673\n",
      "\tTesting Loss: 540.558065\n",
      "\tLearning Rate: 0.000046384\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.623044\n",
      "\tTesting Loss: 540.558105\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.627047\n",
      "\tTesting Loss: 540.547017\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.624672\n",
      "\tTesting Loss: 540.527557\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.621953\n",
      "\tTesting Loss: 540.509532\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.622653\n",
      "\tTesting Loss: 540.503306\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.620239\n",
      "\tTesting Loss: 540.465495\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.620834\n",
      "\tTesting Loss: 540.480774\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.619199\n",
      "\tTesting Loss: 540.459951\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.625254\n",
      "\tTesting Loss: 540.436320\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.592634\n",
      "\tTesting Loss: 540.405029\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.610893\n",
      "\tTesting Loss: 540.430339\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.616590\n",
      "\tTesting Loss: 540.393565\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.614746\n",
      "\tTesting Loss: 540.392517\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.592837\n",
      "\tTesting Loss: 540.348745\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.608988\n",
      "\tTesting Loss: 540.368347\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.609319\n",
      "\tTesting Loss: 540.346415\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.608210\n",
      "\tTesting Loss: 540.334829\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.587377\n",
      "\tTesting Loss: 540.296407\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.598689\n",
      "\tTesting Loss: 540.304932\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.601237\n",
      "\tTesting Loss: 540.293355\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.600337\n",
      "\tTesting Loss: 540.277832\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.599711\n",
      "\tTesting Loss: 540.263509\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.596934\n",
      "\tTesting Loss: 540.246755\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.578578\n",
      "\tTesting Loss: 540.222249\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.595408\n",
      "\tTesting Loss: 540.233836\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.587102\n",
      "\tTesting Loss: 540.200582\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.582860\n",
      "\tTesting Loss: 540.189799\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.583369\n",
      "\tTesting Loss: 540.202026\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.591390\n",
      "\tTesting Loss: 540.162476\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.588710\n",
      "\tTesting Loss: 540.146667\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.556760\n",
      "\tTesting Loss: 540.123189\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.589966\n",
      "\tTesting Loss: 540.141215\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.575531\n",
      "\tTesting Loss: 540.098043\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.582329\n",
      "\tTesting Loss: 540.100342\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.563520\n",
      "\tTesting Loss: 540.052053\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.579046\n",
      "\tTesting Loss: 540.079397\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.567937\n",
      "\tTesting Loss: 540.016622\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.578753\n",
      "\tTesting Loss: 540.053569\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.555934\n",
      "\tTesting Loss: 539.975057\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.564016\n",
      "\tTesting Loss: 540.020813\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.566297\n",
      "\tTesting Loss: 539.954244\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.561010\n",
      "\tTesting Loss: 540.006256\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.558792\n",
      "\tTesting Loss: 539.913554\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.559092\n",
      "\tTesting Loss: 539.956472\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.559329\n",
      "\tTesting Loss: 539.881226\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.559441\n",
      "\tTesting Loss: 539.938700\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.559034\n",
      "\tTesting Loss: 539.850159\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.557892\n",
      "\tTesting Loss: 539.885620\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.548648\n",
      "\tTesting Loss: 539.825236\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.549428\n",
      "\tTesting Loss: 539.861847\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.568507\n",
      "\tTesting Loss: 539.797445\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.552984\n",
      "\tTesting Loss: 539.827047\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.545186\n",
      "\tTesting Loss: 539.777344\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.553655\n",
      "\tTesting Loss: 539.796458\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.551788\n",
      "\tTesting Loss: 539.746613\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.553797\n",
      "\tTesting Loss: 539.765605\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.550555\n",
      "\tTesting Loss: 539.725037\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.551987\n",
      "\tTesting Loss: 539.716492\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.549601\n",
      "\tTesting Loss: 539.710032\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.531497\n",
      "\tTesting Loss: 539.674296\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.562037\n",
      "\tTesting Loss: 539.700317\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.534515\n",
      "\tTesting Loss: 539.656616\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.553594\n",
      "\tTesting Loss: 539.646779\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.520289\n",
      "\tTesting Loss: 539.602539\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.540771\n",
      "\tTesting Loss: 539.648336\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.532967\n",
      "\tTesting Loss: 539.581868\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.540695\n",
      "\tTesting Loss: 539.596008\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.512271\n",
      "\tTesting Loss: 539.551514\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.536977\n",
      "\tTesting Loss: 539.574046\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.525775\n",
      "\tTesting Loss: 539.518087\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.535233\n",
      "\tTesting Loss: 539.539337\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.505763\n",
      "\tTesting Loss: 539.471741\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.533966\n",
      "\tTesting Loss: 539.543030\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.512207\n",
      "\tTesting Loss: 539.458720\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.526800\n",
      "\tTesting Loss: 539.478129\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.508703\n",
      "\tTesting Loss: 539.427714\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.509816\n",
      "\tTesting Loss: 539.475616\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.528470\n",
      "\tTesting Loss: 539.406189\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.519933\n",
      "\tTesting Loss: 539.426076\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.500994\n",
      "\tTesting Loss: 539.386637\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.523427\n",
      "\tTesting Loss: 539.415527\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.502604\n",
      "\tTesting Loss: 539.350586\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.511126\n",
      "\tTesting Loss: 539.378337\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.507970\n",
      "\tTesting Loss: 539.317790\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.512543\n",
      "\tTesting Loss: 539.359172\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.497447\n",
      "\tTesting Loss: 539.302399\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.517919\n",
      "\tTesting Loss: 539.329061\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.505564\n",
      "\tTesting Loss: 539.276245\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.510691\n",
      "\tTesting Loss: 539.287191\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.510145\n",
      "\tTesting Loss: 539.260925\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.511264\n",
      "\tTesting Loss: 539.268229\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.494029\n",
      "\tTesting Loss: 539.221924\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.510363\n",
      "\tTesting Loss: 539.244161\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.501411\n",
      "\tTesting Loss: 539.203857\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.517059\n",
      "\tTesting Loss: 539.221171\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.502670\n",
      "\tTesting Loss: 539.189056\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.510620\n",
      "\tTesting Loss: 539.174998\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.489705\n",
      "\tTesting Loss: 539.158925\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.495089\n",
      "\tTesting Loss: 539.149841\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.513346\n",
      "\tTesting Loss: 539.159037\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.498042\n",
      "\tTesting Loss: 539.120127\n",
      "\tLearning Rate: 0.000041746\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.504951\n",
      "\tTesting Loss: 539.099955\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.491994\n",
      "\tTesting Loss: 539.101827\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.485438\n",
      "\tTesting Loss: 539.079000\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.504100\n",
      "\tTesting Loss: 539.079295\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.501358\n",
      "\tTesting Loss: 539.070211\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.498245\n",
      "\tTesting Loss: 539.044535\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.489929\n",
      "\tTesting Loss: 539.033223\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.477941\n",
      "\tTesting Loss: 539.015279\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.497556\n",
      "\tTesting Loss: 539.030792\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.492793\n",
      "\tTesting Loss: 538.993958\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.496407\n",
      "\tTesting Loss: 538.989929\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.494921\n",
      "\tTesting Loss: 538.995778\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.478508\n",
      "\tTesting Loss: 538.954915\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.481054\n",
      "\tTesting Loss: 538.958516\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.486521\n",
      "\tTesting Loss: 538.958934\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.493429\n",
      "\tTesting Loss: 538.939087\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.485794\n",
      "\tTesting Loss: 538.929850\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.469775\n",
      "\tTesting Loss: 538.899170\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.495855\n",
      "\tTesting Loss: 538.921712\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.471268\n",
      "\tTesting Loss: 538.881897\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.490514\n",
      "\tTesting Loss: 538.912170\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.472682\n",
      "\tTesting Loss: 538.866180\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.482437\n",
      "\tTesting Loss: 538.870117\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.475164\n",
      "\tTesting Loss: 538.854665\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.486654\n",
      "\tTesting Loss: 538.863546\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.476624\n",
      "\tTesting Loss: 538.837728\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.467524\n",
      "\tTesting Loss: 538.817861\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.464406\n",
      "\tTesting Loss: 538.814453\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.483635\n",
      "\tTesting Loss: 538.818970\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.479337\n",
      "\tTesting Loss: 538.810140\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.465123\n",
      "\tTesting Loss: 538.795593\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.463638\n",
      "\tTesting Loss: 538.774302\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.474739\n",
      "\tTesting Loss: 538.802450\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.476473\n",
      "\tTesting Loss: 538.776794\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.469772\n",
      "\tTesting Loss: 538.757853\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.458590\n",
      "\tTesting Loss: 538.772237\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.458618\n",
      "\tTesting Loss: 538.754710\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.474737\n",
      "\tTesting Loss: 538.735870\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.466166\n",
      "\tTesting Loss: 538.747111\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.449376\n",
      "\tTesting Loss: 538.732951\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.464376\n",
      "\tTesting Loss: 538.729085\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.449745\n",
      "\tTesting Loss: 538.712097\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.453183\n",
      "\tTesting Loss: 538.703715\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.450244\n",
      "\tTesting Loss: 538.706014\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.449214\n",
      "\tTesting Loss: 538.693146\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.466304\n",
      "\tTesting Loss: 538.713481\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.436417\n",
      "\tTesting Loss: 538.669657\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.447896\n",
      "\tTesting Loss: 538.671326\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.457423\n",
      "\tTesting Loss: 538.669352\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.433604\n",
      "\tTesting Loss: 538.634796\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.438726\n",
      "\tTesting Loss: 538.652934\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.443405\n",
      "\tTesting Loss: 538.644307\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.454346\n",
      "\tTesting Loss: 538.627035\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.440130\n",
      "\tTesting Loss: 538.608978\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.421694\n",
      "\tTesting Loss: 538.596883\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.431117\n",
      "\tTesting Loss: 538.602743\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.446528\n",
      "\tTesting Loss: 538.597514\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.445709\n",
      "\tTesting Loss: 538.601420\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.434156\n",
      "\tTesting Loss: 538.567027\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.421305\n",
      "\tTesting Loss: 538.538534\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.429265\n",
      "\tTesting Loss: 538.570658\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.435949\n",
      "\tTesting Loss: 538.547373\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.434667\n",
      "\tTesting Loss: 538.545695\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.420919\n",
      "\tTesting Loss: 538.537272\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.422185\n",
      "\tTesting Loss: 538.516215\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.435379\n",
      "\tTesting Loss: 538.510417\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.433207\n",
      "\tTesting Loss: 538.506551\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.418363\n",
      "\tTesting Loss: 538.505025\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.424062\n",
      "\tTesting Loss: 538.488159\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.438306\n",
      "\tTesting Loss: 538.496663\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.430326\n",
      "\tTesting Loss: 538.464976\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.417892\n",
      "\tTesting Loss: 538.448537\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.416951\n",
      "\tTesting Loss: 538.456909\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.430916\n",
      "\tTesting Loss: 538.442057\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.424497\n",
      "\tTesting Loss: 538.409841\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.408506\n",
      "\tTesting Loss: 538.418864\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.420115\n",
      "\tTesting Loss: 538.410136\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.429677\n",
      "\tTesting Loss: 538.387899\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.414902\n",
      "\tTesting Loss: 538.370870\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.393799\n",
      "\tTesting Loss: 538.331909\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.424650\n",
      "\tTesting Loss: 538.380778\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.414581\n",
      "\tTesting Loss: 538.347677\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.407888\n",
      "\tTesting Loss: 538.318258\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.407756\n",
      "\tTesting Loss: 538.320608\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.415655\n",
      "\tTesting Loss: 538.310730\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.413061\n",
      "\tTesting Loss: 538.287415\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.394618\n",
      "\tTesting Loss: 538.286194\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.401820\n",
      "\tTesting Loss: 538.275614\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.409429\n",
      "\tTesting Loss: 538.265004\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.417470\n",
      "\tTesting Loss: 538.265350\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.395528\n",
      "\tTesting Loss: 538.246887\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.404978\n",
      "\tTesting Loss: 538.231934\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.412496\n",
      "\tTesting Loss: 538.228058\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.411514\n",
      "\tTesting Loss: 538.212362\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.405177\n",
      "\tTesting Loss: 538.190348\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.398422\n",
      "\tTesting Loss: 538.192647\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.394320\n",
      "\tTesting Loss: 538.173248\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.412369\n",
      "\tTesting Loss: 538.170461\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.408432\n",
      "\tTesting Loss: 538.151611\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.391225\n",
      "\tTesting Loss: 538.158407\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.399424\n",
      "\tTesting Loss: 538.132670\n",
      "\tLearning Rate: 0.000037571\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.396327\n",
      "\tTesting Loss: 538.123474\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.416239\n",
      "\tTesting Loss: 538.125834\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.385569\n",
      "\tTesting Loss: 538.097107\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.403531\n",
      "\tTesting Loss: 538.104411\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.407758\n",
      "\tTesting Loss: 538.109049\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.404922\n",
      "\tTesting Loss: 538.071411\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.396947\n",
      "\tTesting Loss: 538.070953\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.388247\n",
      "\tTesting Loss: 538.061066\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.411870\n",
      "\tTesting Loss: 538.060669\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.405523\n",
      "\tTesting Loss: 538.026855\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.388880\n",
      "\tTesting Loss: 538.033254\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.397753\n",
      "\tTesting Loss: 538.023478\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.412771\n",
      "\tTesting Loss: 538.013865\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.402761\n",
      "\tTesting Loss: 538.002401\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.394811\n",
      "\tTesting Loss: 537.994609\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.393133\n",
      "\tTesting Loss: 537.969604\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.395348\n",
      "\tTesting Loss: 537.981649\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.412900\n",
      "\tTesting Loss: 537.976807\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.395187\n",
      "\tTesting Loss: 537.953674\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.400645\n",
      "\tTesting Loss: 537.951803\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.398471\n",
      "\tTesting Loss: 537.948191\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.407216\n",
      "\tTesting Loss: 537.919413\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.393560\n",
      "\tTesting Loss: 537.920186\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.393163\n",
      "\tTesting Loss: 537.899150\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.398603\n",
      "\tTesting Loss: 537.900330\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.406286\n",
      "\tTesting Loss: 537.890137\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.387955\n",
      "\tTesting Loss: 537.873383\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.394061\n",
      "\tTesting Loss: 537.872620\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.397715\n",
      "\tTesting Loss: 537.875285\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.411992\n",
      "\tTesting Loss: 537.845083\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.398916\n",
      "\tTesting Loss: 537.832499\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.389435\n",
      "\tTesting Loss: 537.831777\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.396729\n",
      "\tTesting Loss: 537.820241\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.405795\n",
      "\tTesting Loss: 537.808390\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.389086\n",
      "\tTesting Loss: 537.811910\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.395304\n",
      "\tTesting Loss: 537.785716\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.397990\n",
      "\tTesting Loss: 537.786112\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.392810\n",
      "\tTesting Loss: 537.780396\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.403681\n",
      "\tTesting Loss: 537.756236\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.391586\n",
      "\tTesting Loss: 537.757609\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.389765\n",
      "\tTesting Loss: 537.737061\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.401049\n",
      "\tTesting Loss: 537.754649\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.407504\n",
      "\tTesting Loss: 537.715352\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.393061\n",
      "\tTesting Loss: 537.714457\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.388824\n",
      "\tTesting Loss: 537.710449\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.401693\n",
      "\tTesting Loss: 537.684143\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.400864\n",
      "\tTesting Loss: 537.678141\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.382919\n",
      "\tTesting Loss: 537.679372\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.398402\n",
      "\tTesting Loss: 537.683675\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.405306\n",
      "\tTesting Loss: 537.637675\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.388428\n",
      "\tTesting Loss: 537.655986\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.391446\n",
      "\tTesting Loss: 537.651143\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.403122\n",
      "\tTesting Loss: 537.621379\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.400790\n",
      "\tTesting Loss: 537.618988\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.387909\n",
      "\tTesting Loss: 537.624980\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.394559\n",
      "\tTesting Loss: 537.602356\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.401962\n",
      "\tTesting Loss: 537.580709\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.394780\n",
      "\tTesting Loss: 537.592936\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.383708\n",
      "\tTesting Loss: 537.571859\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.402435\n",
      "\tTesting Loss: 537.579264\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.401408\n",
      "\tTesting Loss: 537.560150\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.389491\n",
      "\tTesting Loss: 537.542399\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.389648\n",
      "\tTesting Loss: 537.545064\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.396286\n",
      "\tTesting Loss: 537.532867\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.397957\n",
      "\tTesting Loss: 537.505554\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.395411\n",
      "\tTesting Loss: 537.539632\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.388957\n",
      "\tTesting Loss: 537.512492\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.400909\n",
      "\tTesting Loss: 537.486410\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.399094\n",
      "\tTesting Loss: 537.481639\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.386731\n",
      "\tTesting Loss: 537.490784\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.400093\n",
      "\tTesting Loss: 537.472839\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.397690\n",
      "\tTesting Loss: 537.439463\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.387060\n",
      "\tTesting Loss: 537.448792\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.387110\n",
      "\tTesting Loss: 537.445292\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.392558\n",
      "\tTesting Loss: 537.442037\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.397685\n",
      "\tTesting Loss: 537.408498\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.390986\n",
      "\tTesting Loss: 537.406779\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.387688\n",
      "\tTesting Loss: 537.431681\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.392080\n",
      "\tTesting Loss: 537.406372\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.400258\n",
      "\tTesting Loss: 537.369588\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.392581\n",
      "\tTesting Loss: 537.384135\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.390386\n",
      "\tTesting Loss: 537.388458\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.388621\n",
      "\tTesting Loss: 537.356283\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.399175\n",
      "\tTesting Loss: 537.353048\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.391891\n",
      "\tTesting Loss: 537.357422\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.386836\n",
      "\tTesting Loss: 537.349843\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.397878\n",
      "\tTesting Loss: 537.356954\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.400518\n",
      "\tTesting Loss: 537.318532\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.403107\n",
      "\tTesting Loss: 537.310720\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.392410\n",
      "\tTesting Loss: 537.333740\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.393738\n",
      "\tTesting Loss: 537.299489\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.398565\n",
      "\tTesting Loss: 537.288462\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.396845\n",
      "\tTesting Loss: 537.283427\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.388443\n",
      "\tTesting Loss: 537.290771\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.390757\n",
      "\tTesting Loss: 537.287974\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.401586\n",
      "\tTesting Loss: 537.253438\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.398041\n",
      "\tTesting Loss: 537.255391\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.386261\n",
      "\tTesting Loss: 537.258667\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.398636\n",
      "\tTesting Loss: 537.246887\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.403102\n",
      "\tTesting Loss: 537.227946\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.403058\n",
      "\tTesting Loss: 537.223460\n",
      "\tLearning Rate: 0.000033814\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.392268\n",
      "\tTesting Loss: 537.240550\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.392487\n",
      "\tTesting Loss: 537.199300\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.390986\n",
      "\tTesting Loss: 537.227091\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.403865\n",
      "\tTesting Loss: 537.203003\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.396322\n",
      "\tTesting Loss: 537.205098\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.391721\n",
      "\tTesting Loss: 537.195953\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.408463\n",
      "\tTesting Loss: 537.182922\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.386808\n",
      "\tTesting Loss: 537.161804\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.384799\n",
      "\tTesting Loss: 537.157145\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.399366\n",
      "\tTesting Loss: 537.203135\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.409322\n",
      "\tTesting Loss: 537.152456\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.403140\n",
      "\tTesting Loss: 537.148661\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.383324\n",
      "\tTesting Loss: 537.135173\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.402120\n",
      "\tTesting Loss: 537.142131\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.394994\n",
      "\tTesting Loss: 537.114797\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.385928\n",
      "\tTesting Loss: 537.111410\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.396983\n",
      "\tTesting Loss: 537.130188\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.398000\n",
      "\tTesting Loss: 537.099386\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.388064\n",
      "\tTesting Loss: 537.090922\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.393539\n",
      "\tTesting Loss: 537.120015\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.406398\n",
      "\tTesting Loss: 537.093608\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.396098\n",
      "\tTesting Loss: 537.078918\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.396179\n",
      "\tTesting Loss: 537.099833\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.399508\n",
      "\tTesting Loss: 537.077637\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.406237\n",
      "\tTesting Loss: 537.043213\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.389638\n",
      "\tTesting Loss: 537.073771\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.398333\n",
      "\tTesting Loss: 537.064006\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.406972\n",
      "\tTesting Loss: 537.047323\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.391098\n",
      "\tTesting Loss: 537.018880\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.389592\n",
      "\tTesting Loss: 537.029856\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.387522\n",
      "\tTesting Loss: 537.036713\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.403035\n",
      "\tTesting Loss: 537.029846\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.394135\n",
      "\tTesting Loss: 536.992330\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.391240\n",
      "\tTesting Loss: 536.992218\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.392604\n",
      "\tTesting Loss: 537.029063\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.409765\n",
      "\tTesting Loss: 536.992757\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.406291\n",
      "\tTesting Loss: 536.962311\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.392995\n",
      "\tTesting Loss: 537.003479\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.393494\n",
      "\tTesting Loss: 536.964691\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.404963\n",
      "\tTesting Loss: 536.961995\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.394168\n",
      "\tTesting Loss: 536.957967\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.395416\n",
      "\tTesting Loss: 536.948751\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.400459\n",
      "\tTesting Loss: 536.968770\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.409251\n",
      "\tTesting Loss: 536.921549\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.395131\n",
      "\tTesting Loss: 536.921488\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.385938\n",
      "\tTesting Loss: 536.941854\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.403717\n",
      "\tTesting Loss: 536.926381\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.404053\n",
      "\tTesting Loss: 536.906230\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.399745\n",
      "\tTesting Loss: 536.928446\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.401245\n",
      "\tTesting Loss: 536.900441\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.407692\n",
      "\tTesting Loss: 536.875356\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.396820\n",
      "\tTesting Loss: 536.895304\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.402964\n",
      "\tTesting Loss: 536.892232\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.395266\n",
      "\tTesting Loss: 536.859823\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.404154\n",
      "\tTesting Loss: 536.880229\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.405533\n",
      "\tTesting Loss: 536.866760\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.405177\n",
      "\tTesting Loss: 536.833771\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.392980\n",
      "\tTesting Loss: 536.864095\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.400940\n",
      "\tTesting Loss: 536.850240\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.407842\n",
      "\tTesting Loss: 536.809509\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.394165\n",
      "\tTesting Loss: 536.812663\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.393583\n",
      "\tTesting Loss: 536.840912\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.403674\n",
      "\tTesting Loss: 536.817657\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.408351\n",
      "\tTesting Loss: 536.809184\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.401006\n",
      "\tTesting Loss: 536.811178\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.400828\n",
      "\tTesting Loss: 536.810221\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.400543\n",
      "\tTesting Loss: 536.783549\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.402008\n",
      "\tTesting Loss: 536.793294\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.400599\n",
      "\tTesting Loss: 536.779134\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.402039\n",
      "\tTesting Loss: 536.777852\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.394938\n",
      "\tTesting Loss: 536.773966\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.410769\n",
      "\tTesting Loss: 536.778829\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.406464\n",
      "\tTesting Loss: 536.742503\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.398758\n",
      "\tTesting Loss: 536.754801\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.398066\n",
      "\tTesting Loss: 536.766195\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.410123\n",
      "\tTesting Loss: 536.723348\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.400897\n",
      "\tTesting Loss: 536.716899\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.392420\n",
      "\tTesting Loss: 536.745188\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.409841\n",
      "\tTesting Loss: 536.729553\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.413829\n",
      "\tTesting Loss: 536.713582\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.399419\n",
      "\tTesting Loss: 536.702942\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.402913\n",
      "\tTesting Loss: 536.718160\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.406459\n",
      "\tTesting Loss: 536.713786\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.407483\n",
      "\tTesting Loss: 536.690542\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.408234\n",
      "\tTesting Loss: 536.683421\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.404292\n",
      "\tTesting Loss: 536.673767\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.400416\n",
      "\tTesting Loss: 536.694946\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.407557\n",
      "\tTesting Loss: 536.672058\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.416295\n",
      "\tTesting Loss: 536.658315\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.405746\n",
      "\tTesting Loss: 536.642080\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.401662\n",
      "\tTesting Loss: 536.663869\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.407697\n",
      "\tTesting Loss: 536.656637\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.411926\n",
      "\tTesting Loss: 536.635091\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.414083\n",
      "\tTesting Loss: 536.622213\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.401794\n",
      "\tTesting Loss: 536.633942\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.403152\n",
      "\tTesting Loss: 536.613220\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.406591\n",
      "\tTesting Loss: 536.623942\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.421000\n",
      "\tTesting Loss: 536.602610\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.409467\n",
      "\tTesting Loss: 536.596090\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.403325\n",
      "\tTesting Loss: 536.613566\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.407735\n",
      "\tTesting Loss: 536.598755\n",
      "\tLearning Rate: 0.000030433\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.416725\n",
      "\tTesting Loss: 536.582479\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.409726\n",
      "\tTesting Loss: 536.579651\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.406972\n",
      "\tTesting Loss: 536.575256\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.407956\n",
      "\tTesting Loss: 536.575419\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.420461\n",
      "\tTesting Loss: 536.565857\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.413712\n",
      "\tTesting Loss: 536.559662\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.405838\n",
      "\tTesting Loss: 536.558889\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.413467\n",
      "\tTesting Loss: 536.553538\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.411753\n",
      "\tTesting Loss: 536.545593\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.420776\n",
      "\tTesting Loss: 536.521352\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.416112\n",
      "\tTesting Loss: 536.538239\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.410276\n",
      "\tTesting Loss: 536.534200\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.415146\n",
      "\tTesting Loss: 536.520508\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.413325\n",
      "\tTesting Loss: 536.523397\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.411980\n",
      "\tTesting Loss: 536.520253\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.418391\n",
      "\tTesting Loss: 536.501872\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.411636\n",
      "\tTesting Loss: 536.520538\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.413325\n",
      "\tTesting Loss: 536.498851\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.416580\n",
      "\tTesting Loss: 536.479858\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.403971\n",
      "\tTesting Loss: 536.506714\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.419088\n",
      "\tTesting Loss: 536.490885\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.414841\n",
      "\tTesting Loss: 536.468028\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.418226\n",
      "\tTesting Loss: 536.462728\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.416779\n",
      "\tTesting Loss: 536.466248\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.406237\n",
      "\tTesting Loss: 536.464213\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.416148\n",
      "\tTesting Loss: 536.457886\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.415731\n",
      "\tTesting Loss: 536.449178\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.418920\n",
      "\tTesting Loss: 536.428375\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.421300\n",
      "\tTesting Loss: 536.435221\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.408669\n",
      "\tTesting Loss: 536.438761\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.415766\n",
      "\tTesting Loss: 536.428365\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.416166\n",
      "\tTesting Loss: 536.416829\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.407850\n",
      "\tTesting Loss: 536.423147\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.409681\n",
      "\tTesting Loss: 536.427999\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.425507\n",
      "\tTesting Loss: 536.408407\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.421293\n",
      "\tTesting Loss: 536.396942\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.408274\n",
      "\tTesting Loss: 536.398478\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.417259\n",
      "\tTesting Loss: 536.399740\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.416110\n",
      "\tTesting Loss: 536.389343\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.427368\n",
      "\tTesting Loss: 536.359416\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.418691\n",
      "\tTesting Loss: 536.356099\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.403465\n",
      "\tTesting Loss: 536.383494\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.418849\n",
      "\tTesting Loss: 536.360006\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.426448\n",
      "\tTesting Loss: 536.341288\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.406448\n",
      "\tTesting Loss: 536.357463\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.416351\n",
      "\tTesting Loss: 536.349630\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.421575\n",
      "\tTesting Loss: 536.335988\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.421318\n",
      "\tTesting Loss: 536.321309\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.408508\n",
      "\tTesting Loss: 536.330790\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.410894\n",
      "\tTesting Loss: 536.343353\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.420095\n",
      "\tTesting Loss: 536.313049\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.418264\n",
      "\tTesting Loss: 536.309611\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.411021\n",
      "\tTesting Loss: 536.320099\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.413620\n",
      "\tTesting Loss: 536.314718\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.417297\n",
      "\tTesting Loss: 536.289917\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.415639\n",
      "\tTesting Loss: 536.287679\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.417697\n",
      "\tTesting Loss: 536.288635\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.416748\n",
      "\tTesting Loss: 536.286346\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.410004\n",
      "\tTesting Loss: 536.281830\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.409892\n",
      "\tTesting Loss: 536.287028\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.412959\n",
      "\tTesting Loss: 536.283366\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.435069\n",
      "\tTesting Loss: 536.250712\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.420972\n",
      "\tTesting Loss: 536.234924\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.402044\n",
      "\tTesting Loss: 536.261047\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.416616\n",
      "\tTesting Loss: 536.244507\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.431137\n",
      "\tTesting Loss: 536.232931\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.419749\n",
      "\tTesting Loss: 536.218984\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.415888\n",
      "\tTesting Loss: 536.243683\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.417508\n",
      "\tTesting Loss: 536.235077\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.434921\n",
      "\tTesting Loss: 536.215332\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.429291\n",
      "\tTesting Loss: 536.195699\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.419446\n",
      "\tTesting Loss: 536.199646\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.418424\n",
      "\tTesting Loss: 536.214193\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.413411\n",
      "\tTesting Loss: 536.198344\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.428599\n",
      "\tTesting Loss: 536.194946\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.423986\n",
      "\tTesting Loss: 536.186646\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.420036\n",
      "\tTesting Loss: 536.192424\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.426793\n",
      "\tTesting Loss: 536.175781\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.432157\n",
      "\tTesting Loss: 536.168538\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.435557\n",
      "\tTesting Loss: 536.154175\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.425018\n",
      "\tTesting Loss: 536.155762\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.403498\n",
      "\tTesting Loss: 536.165812\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.421112\n",
      "\tTesting Loss: 536.170329\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.445028\n",
      "\tTesting Loss: 536.124583\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.428327\n",
      "\tTesting Loss: 536.120850\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.416687\n",
      "\tTesting Loss: 536.134644\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.413747\n",
      "\tTesting Loss: 536.134033\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.424988\n",
      "\tTesting Loss: 536.126526\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.432134\n",
      "\tTesting Loss: 536.103821\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.436440\n",
      "\tTesting Loss: 536.094991\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.429016\n",
      "\tTesting Loss: 536.101267\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.418004\n",
      "\tTesting Loss: 536.118652\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.426908\n",
      "\tTesting Loss: 536.090708\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.427953\n",
      "\tTesting Loss: 536.074992\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.429161\n",
      "\tTesting Loss: 536.071554\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.426231\n",
      "\tTesting Loss: 536.081207\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.420987\n",
      "\tTesting Loss: 536.074910\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.418808\n",
      "\tTesting Loss: 536.066630\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.416540\n",
      "\tTesting Loss: 536.060689\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.432810\n",
      "\tTesting Loss: 536.056152\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.437119\n",
      "\tTesting Loss: 536.030467\n",
      "\tLearning Rate: 0.000027389\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.429998\n",
      "\tTesting Loss: 536.034149\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.410940\n",
      "\tTesting Loss: 536.051412\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.422328\n",
      "\tTesting Loss: 536.036448\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.442866\n",
      "\tTesting Loss: 536.016571\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.425913\n",
      "\tTesting Loss: 536.028056\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.414708\n",
      "\tTesting Loss: 536.046173\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.436930\n",
      "\tTesting Loss: 536.018901\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.428197\n",
      "\tTesting Loss: 536.012024\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.420883\n",
      "\tTesting Loss: 536.018738\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.419657\n",
      "\tTesting Loss: 536.020406\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.435745\n",
      "\tTesting Loss: 536.001689\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.431994\n",
      "\tTesting Loss: 535.987956\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.425440\n",
      "\tTesting Loss: 535.991516\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.420896\n",
      "\tTesting Loss: 536.004242\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.419688\n",
      "\tTesting Loss: 535.998596\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.436902\n",
      "\tTesting Loss: 535.991425\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.431239\n",
      "\tTesting Loss: 535.975708\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.417226\n",
      "\tTesting Loss: 535.985718\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.416382\n",
      "\tTesting Loss: 535.988332\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.440465\n",
      "\tTesting Loss: 535.972209\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.438939\n",
      "\tTesting Loss: 535.968903\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.426427\n",
      "\tTesting Loss: 535.971822\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.425339\n",
      "\tTesting Loss: 535.967428\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.435422\n",
      "\tTesting Loss: 535.962097\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.441076\n",
      "\tTesting Loss: 535.936574\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.430018\n",
      "\tTesting Loss: 535.947428\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.425217\n",
      "\tTesting Loss: 535.959066\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.427165\n",
      "\tTesting Loss: 535.952372\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.448260\n",
      "\tTesting Loss: 535.922496\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.431641\n",
      "\tTesting Loss: 535.928507\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.424001\n",
      "\tTesting Loss: 535.937907\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.426259\n",
      "\tTesting Loss: 535.942342\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.445096\n",
      "\tTesting Loss: 535.914693\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.438700\n",
      "\tTesting Loss: 535.917918\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.427007\n",
      "\tTesting Loss: 535.919698\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.425074\n",
      "\tTesting Loss: 535.929586\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.430738\n",
      "\tTesting Loss: 535.919495\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.448881\n",
      "\tTesting Loss: 535.893270\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.437406\n",
      "\tTesting Loss: 535.899231\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.432073\n",
      "\tTesting Loss: 535.902262\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.426280\n",
      "\tTesting Loss: 535.911875\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.438474\n",
      "\tTesting Loss: 535.897298\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.445847\n",
      "\tTesting Loss: 535.891418\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.445516\n",
      "\tTesting Loss: 535.871704\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.437729\n",
      "\tTesting Loss: 535.880412\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.435763\n",
      "\tTesting Loss: 535.883423\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.434105\n",
      "\tTesting Loss: 535.877981\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.443237\n",
      "\tTesting Loss: 535.866750\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.440417\n",
      "\tTesting Loss: 535.868876\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.439560\n",
      "\tTesting Loss: 535.869934\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.440847\n",
      "\tTesting Loss: 535.860392\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.451009\n",
      "\tTesting Loss: 535.843323\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.441645\n",
      "\tTesting Loss: 535.838786\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.436485\n",
      "\tTesting Loss: 535.843241\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.428528\n",
      "\tTesting Loss: 535.852285\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.446991\n",
      "\tTesting Loss: 535.838521\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.455541\n",
      "\tTesting Loss: 535.815125\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.442144\n",
      "\tTesting Loss: 535.825785\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.431249\n",
      "\tTesting Loss: 535.839620\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.451365\n",
      "\tTesting Loss: 535.821248\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.449570\n",
      "\tTesting Loss: 535.798757\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.437302\n",
      "\tTesting Loss: 535.807292\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.442691\n",
      "\tTesting Loss: 535.821248\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.442078\n",
      "\tTesting Loss: 535.811503\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.450269\n",
      "\tTesting Loss: 535.805105\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.451436\n",
      "\tTesting Loss: 535.786621\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.445048\n",
      "\tTesting Loss: 535.798462\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.439735\n",
      "\tTesting Loss: 535.795003\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.443008\n",
      "\tTesting Loss: 535.795390\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.445104\n",
      "\tTesting Loss: 535.771077\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.454969\n",
      "\tTesting Loss: 535.756978\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.438238\n",
      "\tTesting Loss: 535.774251\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.432765\n",
      "\tTesting Loss: 535.782501\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.453578\n",
      "\tTesting Loss: 535.765035\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.456947\n",
      "\tTesting Loss: 535.745717\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.444400\n",
      "\tTesting Loss: 535.755676\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.433838\n",
      "\tTesting Loss: 535.765544\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.446938\n",
      "\tTesting Loss: 535.745850\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.458181\n",
      "\tTesting Loss: 535.715546\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.442561\n",
      "\tTesting Loss: 535.733459\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.436844\n",
      "\tTesting Loss: 535.749298\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.456032\n",
      "\tTesting Loss: 535.726562\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.456014\n",
      "\tTesting Loss: 535.708018\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.447721\n",
      "\tTesting Loss: 535.707621\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.440928\n",
      "\tTesting Loss: 535.725057\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.440585\n",
      "\tTesting Loss: 535.717672\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.451330\n",
      "\tTesting Loss: 535.700195\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.456985\n",
      "\tTesting Loss: 535.676982\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.443225\n",
      "\tTesting Loss: 535.697998\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.434316\n",
      "\tTesting Loss: 535.703623\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.449036\n",
      "\tTesting Loss: 535.691874\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.464630\n",
      "\tTesting Loss: 535.655457\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.448486\n",
      "\tTesting Loss: 535.679688\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.441096\n",
      "\tTesting Loss: 535.690725\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.445897\n",
      "\tTesting Loss: 535.683350\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.451650\n",
      "\tTesting Loss: 535.665344\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.457957\n",
      "\tTesting Loss: 535.648031\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.448130\n",
      "\tTesting Loss: 535.651489\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.447001\n",
      "\tTesting Loss: 535.657227\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.450083\n",
      "\tTesting Loss: 535.641378\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.459582\n",
      "\tTesting Loss: 535.631571\n",
      "\tLearning Rate: 0.000024650\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.449463\n",
      "\tTesting Loss: 535.638224\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.443906\n",
      "\tTesting Loss: 535.641836\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.449704\n",
      "\tTesting Loss: 535.618510\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.452530\n",
      "\tTesting Loss: 535.613759\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.449473\n",
      "\tTesting Loss: 535.621033\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.438283\n",
      "\tTesting Loss: 535.624329\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.462179\n",
      "\tTesting Loss: 535.602163\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.452118\n",
      "\tTesting Loss: 535.598267\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.438583\n",
      "\tTesting Loss: 535.611125\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.464043\n",
      "\tTesting Loss: 535.577311\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.448196\n",
      "\tTesting Loss: 535.593760\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.432371\n",
      "\tTesting Loss: 535.596761\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.449788\n",
      "\tTesting Loss: 535.576070\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.460520\n",
      "\tTesting Loss: 535.560404\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.433606\n",
      "\tTesting Loss: 535.591227\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.444809\n",
      "\tTesting Loss: 535.572774\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.467730\n",
      "\tTesting Loss: 535.549601\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.436040\n",
      "\tTesting Loss: 535.566895\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.441762\n",
      "\tTesting Loss: 535.571025\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.471532\n",
      "\tTesting Loss: 535.540059\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.443387\n",
      "\tTesting Loss: 535.564392\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.437057\n",
      "\tTesting Loss: 535.566366\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.459396\n",
      "\tTesting Loss: 535.541168\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.448687\n",
      "\tTesting Loss: 535.546173\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.441406\n",
      "\tTesting Loss: 535.552277\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.460973\n",
      "\tTesting Loss: 535.529541\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.446406\n",
      "\tTesting Loss: 535.534139\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.437729\n",
      "\tTesting Loss: 535.539052\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.457743\n",
      "\tTesting Loss: 535.521139\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.452871\n",
      "\tTesting Loss: 535.523702\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.441706\n",
      "\tTesting Loss: 535.530579\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.440186\n",
      "\tTesting Loss: 535.528025\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.451955\n",
      "\tTesting Loss: 535.510763\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.455129\n",
      "\tTesting Loss: 535.488017\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.441666\n",
      "\tTesting Loss: 535.518300\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.436056\n",
      "\tTesting Loss: 535.517232\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.460892\n",
      "\tTesting Loss: 535.480286\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.447070\n",
      "\tTesting Loss: 535.487325\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.434453\n",
      "\tTesting Loss: 535.506266\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.448687\n",
      "\tTesting Loss: 535.492483\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.467911\n",
      "\tTesting Loss: 535.462382\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.441711\n",
      "\tTesting Loss: 535.480042\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.436996\n",
      "\tTesting Loss: 535.485718\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.456726\n",
      "\tTesting Loss: 535.464396\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.459859\n",
      "\tTesting Loss: 535.457194\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.440575\n",
      "\tTesting Loss: 535.468933\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.451243\n",
      "\tTesting Loss: 535.467611\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.457387\n",
      "\tTesting Loss: 535.450480\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.450551\n",
      "\tTesting Loss: 535.454803\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.446050\n",
      "\tTesting Loss: 535.453186\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.448908\n",
      "\tTesting Loss: 535.463745\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.448082\n",
      "\tTesting Loss: 535.444010\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.457586\n",
      "\tTesting Loss: 535.422760\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.451447\n",
      "\tTesting Loss: 535.431681\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.442935\n",
      "\tTesting Loss: 535.444071\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.446157\n",
      "\tTesting Loss: 535.436442\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.458781\n",
      "\tTesting Loss: 535.415181\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.448461\n",
      "\tTesting Loss: 535.416382\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.445760\n",
      "\tTesting Loss: 535.428955\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.446625\n",
      "\tTesting Loss: 535.424296\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.453934\n",
      "\tTesting Loss: 535.402507\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.452576\n",
      "\tTesting Loss: 535.398641\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.444784\n",
      "\tTesting Loss: 535.411051\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.446752\n",
      "\tTesting Loss: 535.406596\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.457263\n",
      "\tTesting Loss: 535.383728\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.451726\n",
      "\tTesting Loss: 535.399577\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.445404\n",
      "\tTesting Loss: 535.408671\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.452784\n",
      "\tTesting Loss: 535.396077\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.452840\n",
      "\tTesting Loss: 535.380554\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.447606\n",
      "\tTesting Loss: 535.389506\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.443136\n",
      "\tTesting Loss: 535.401296\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.440913\n",
      "\tTesting Loss: 535.390279\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.461100\n",
      "\tTesting Loss: 535.346700\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.439468\n",
      "\tTesting Loss: 535.384583\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.435226\n",
      "\tTesting Loss: 535.377797\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.457003\n",
      "\tTesting Loss: 535.346171\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.448486\n",
      "\tTesting Loss: 535.359741\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.425138\n",
      "\tTesting Loss: 535.383860\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.441226\n",
      "\tTesting Loss: 535.349202\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.457815\n",
      "\tTesting Loss: 535.324504\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.431735\n",
      "\tTesting Loss: 535.358541\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.430219\n",
      "\tTesting Loss: 535.355855\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.463158\n",
      "\tTesting Loss: 535.325989\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.436864\n",
      "\tTesting Loss: 535.344503\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.426356\n",
      "\tTesting Loss: 535.344279\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.463117\n",
      "\tTesting Loss: 535.303162\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.435916\n",
      "\tTesting Loss: 535.342265\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.425608\n",
      "\tTesting Loss: 535.334747\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.459905\n",
      "\tTesting Loss: 535.293640\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.442042\n",
      "\tTesting Loss: 535.336619\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.419459\n",
      "\tTesting Loss: 535.323771\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.473368\n",
      "\tTesting Loss: 535.248698\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.423228\n",
      "\tTesting Loss: 535.327901\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.408875\n",
      "\tTesting Loss: 535.315206\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.470078\n",
      "\tTesting Loss: 535.252350\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.424113\n",
      "\tTesting Loss: 535.334534\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.395170\n",
      "\tTesting Loss: 535.321798\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.467819\n",
      "\tTesting Loss: 535.249634\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.409945\n",
      "\tTesting Loss: 535.326304\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.381363\n",
      "\tTesting Loss: 535.320780\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.448924\n",
      "\tTesting Loss: 535.257690\n",
      "\tLearning Rate: 0.000022185\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.416751\n",
      "\tTesting Loss: 535.300008\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.358826\n",
      "\tTesting Loss: 535.324076\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.451330\n",
      "\tTesting Loss: 535.288717\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.404739\n",
      "\tTesting Loss: 535.279928\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.391271\n",
      "\tTesting Loss: 535.288350\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.447261\n",
      "\tTesting Loss: 535.280334\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.407639\n",
      "\tTesting Loss: 535.263255\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.432348\n",
      "\tTesting Loss: 535.258962\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.416489\n",
      "\tTesting Loss: 535.251312\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.424535\n",
      "\tTesting Loss: 535.232402\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.434087\n",
      "\tTesting Loss: 535.249695\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.425120\n",
      "\tTesting Loss: 535.233195\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.428439\n",
      "\tTesting Loss: 535.232920\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.427862\n",
      "\tTesting Loss: 535.238495\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.427615\n",
      "\tTesting Loss: 535.235484\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.431417\n",
      "\tTesting Loss: 535.228149\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.427770\n",
      "\tTesting Loss: 535.227325\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.423686\n",
      "\tTesting Loss: 535.223765\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.435303\n",
      "\tTesting Loss: 535.208700\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.427653\n",
      "\tTesting Loss: 535.217641\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.423592\n",
      "\tTesting Loss: 535.217407\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.435204\n",
      "\tTesting Loss: 535.201477\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.427577\n",
      "\tTesting Loss: 535.204163\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.425817\n",
      "\tTesting Loss: 535.205037\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.437848\n",
      "\tTesting Loss: 535.192546\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.427831\n",
      "\tTesting Loss: 535.201925\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.428533\n",
      "\tTesting Loss: 535.199219\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.433624\n",
      "\tTesting Loss: 535.188527\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.432053\n",
      "\tTesting Loss: 535.187327\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.426203\n",
      "\tTesting Loss: 535.189067\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.438957\n",
      "\tTesting Loss: 535.170329\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.431900\n",
      "\tTesting Loss: 535.175680\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.423871\n",
      "\tTesting Loss: 535.187297\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.438784\n",
      "\tTesting Loss: 535.163696\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.432810\n",
      "\tTesting Loss: 535.168742\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.425880\n",
      "\tTesting Loss: 535.171611\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.440013\n",
      "\tTesting Loss: 535.151123\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.432658\n",
      "\tTesting Loss: 535.161591\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.425456\n",
      "\tTesting Loss: 535.165975\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.440903\n",
      "\tTesting Loss: 535.146444\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.432805\n",
      "\tTesting Loss: 535.150879\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.427989\n",
      "\tTesting Loss: 535.151459\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.442294\n",
      "\tTesting Loss: 535.139638\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.435343\n",
      "\tTesting Loss: 535.139964\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.429080\n",
      "\tTesting Loss: 535.139832\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.441676\n",
      "\tTesting Loss: 535.126658\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.434362\n",
      "\tTesting Loss: 535.127116\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.433296\n",
      "\tTesting Loss: 535.130798\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.439125\n",
      "\tTesting Loss: 535.122447\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.435008\n",
      "\tTesting Loss: 535.124105\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.440058\n",
      "\tTesting Loss: 535.114431\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.435689\n",
      "\tTesting Loss: 535.127970\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.428604\n",
      "\tTesting Loss: 535.121257\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.446846\n",
      "\tTesting Loss: 535.086975\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.433050\n",
      "\tTesting Loss: 535.113831\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.431742\n",
      "\tTesting Loss: 535.102397\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.444974\n",
      "\tTesting Loss: 535.089600\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.438182\n",
      "\tTesting Loss: 535.102234\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.435666\n",
      "\tTesting Loss: 535.086782\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.442866\n",
      "\tTesting Loss: 535.089701\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.438034\n",
      "\tTesting Loss: 535.086426\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.438978\n",
      "\tTesting Loss: 535.087260\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.441757\n",
      "\tTesting Loss: 535.089325\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.436269\n",
      "\tTesting Loss: 535.074697\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.444885\n",
      "\tTesting Loss: 535.058858\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.438555\n",
      "\tTesting Loss: 535.070292\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.440577\n",
      "\tTesting Loss: 535.064697\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.446869\n",
      "\tTesting Loss: 535.049540\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.441411\n",
      "\tTesting Loss: 535.055145\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.438024\n",
      "\tTesting Loss: 535.055227\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.447327\n",
      "\tTesting Loss: 535.045980\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.441076\n",
      "\tTesting Loss: 535.048197\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.438723\n",
      "\tTesting Loss: 535.051921\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.444712\n",
      "\tTesting Loss: 535.027486\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.443204\n",
      "\tTesting Loss: 535.032705\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.440277\n",
      "\tTesting Loss: 535.037567\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.444786\n",
      "\tTesting Loss: 535.026204\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.445887\n",
      "\tTesting Loss: 535.020976\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.446091\n",
      "\tTesting Loss: 535.026998\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.440618\n",
      "\tTesting Loss: 535.033122\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.442627\n",
      "\tTesting Loss: 535.020711\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.447144\n",
      "\tTesting Loss: 535.006846\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.443741\n",
      "\tTesting Loss: 535.004720\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.443003\n",
      "\tTesting Loss: 535.014038\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.443176\n",
      "\tTesting Loss: 535.015035\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.451085\n",
      "\tTesting Loss: 534.986735\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.445819\n",
      "\tTesting Loss: 534.999491\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.442774\n",
      "\tTesting Loss: 535.000814\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.448522\n",
      "\tTesting Loss: 534.990509\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.446894\n",
      "\tTesting Loss: 534.991170\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.447215\n",
      "\tTesting Loss: 534.983938\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.447581\n",
      "\tTesting Loss: 534.972209\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.446899\n",
      "\tTesting Loss: 534.990143\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.445424\n",
      "\tTesting Loss: 534.981588\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.447983\n",
      "\tTesting Loss: 534.959554\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.450203\n",
      "\tTesting Loss: 534.969147\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.446716\n",
      "\tTesting Loss: 534.975911\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.451019\n",
      "\tTesting Loss: 534.960693\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.454519\n",
      "\tTesting Loss: 534.949219\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.449443\n",
      "\tTesting Loss: 534.951742\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.446096\n",
      "\tTesting Loss: 534.954488\n",
      "\tLearning Rate: 0.000019967\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.449992\n",
      "\tTesting Loss: 534.947479\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.451220\n",
      "\tTesting Loss: 534.944010\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.447993\n",
      "\tTesting Loss: 534.941671\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.451116\n",
      "\tTesting Loss: 534.948446\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.453568\n",
      "\tTesting Loss: 534.943278\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.456762\n",
      "\tTesting Loss: 534.931864\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.454005\n",
      "\tTesting Loss: 534.931620\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.450175\n",
      "\tTesting Loss: 534.935913\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.454671\n",
      "\tTesting Loss: 534.922607\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.455907\n",
      "\tTesting Loss: 534.921936\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.454147\n",
      "\tTesting Loss: 534.922577\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.453883\n",
      "\tTesting Loss: 534.926168\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.451795\n",
      "\tTesting Loss: 534.930216\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.453395\n",
      "\tTesting Loss: 534.921224\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.455963\n",
      "\tTesting Loss: 534.908122\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.458405\n",
      "\tTesting Loss: 534.901286\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.452807\n",
      "\tTesting Loss: 534.910166\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.453949\n",
      "\tTesting Loss: 534.898794\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.457799\n",
      "\tTesting Loss: 534.898885\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.460592\n",
      "\tTesting Loss: 534.887512\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.459928\n",
      "\tTesting Loss: 534.899801\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.457209\n",
      "\tTesting Loss: 534.902883\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.456930\n",
      "\tTesting Loss: 534.889994\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.460419\n",
      "\tTesting Loss: 534.870361\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.459117\n",
      "\tTesting Loss: 534.879985\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.452621\n",
      "\tTesting Loss: 534.882365\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.464259\n",
      "\tTesting Loss: 534.861023\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.459727\n",
      "\tTesting Loss: 534.869629\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.458104\n",
      "\tTesting Loss: 534.866516\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.463623\n",
      "\tTesting Loss: 534.852641\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.460780\n",
      "\tTesting Loss: 534.870605\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.458420\n",
      "\tTesting Loss: 534.869486\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.460640\n",
      "\tTesting Loss: 534.849223\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.465780\n",
      "\tTesting Loss: 534.840149\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.456843\n",
      "\tTesting Loss: 534.847941\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.459005\n",
      "\tTesting Loss: 534.837301\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.464066\n",
      "\tTesting Loss: 534.838074\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.463387\n",
      "\tTesting Loss: 534.832072\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.460983\n",
      "\tTesting Loss: 534.830750\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.462229\n",
      "\tTesting Loss: 534.838114\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.464043\n",
      "\tTesting Loss: 534.828135\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.466009\n",
      "\tTesting Loss: 534.809896\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.461558\n",
      "\tTesting Loss: 534.818481\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.456558\n",
      "\tTesting Loss: 534.827311\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.462148\n",
      "\tTesting Loss: 534.810669\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.467074\n",
      "\tTesting Loss: 534.787882\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.459793\n",
      "\tTesting Loss: 534.808797\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.458384\n",
      "\tTesting Loss: 534.807617\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.469666\n",
      "\tTesting Loss: 534.776967\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.461751\n",
      "\tTesting Loss: 534.803955\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.459816\n",
      "\tTesting Loss: 534.797139\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.469480\n",
      "\tTesting Loss: 534.784780\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.469457\n",
      "\tTesting Loss: 534.786336\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.463760\n",
      "\tTesting Loss: 534.791616\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.467616\n",
      "\tTesting Loss: 534.781809\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.463852\n",
      "\tTesting Loss: 534.793986\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.471944\n",
      "\tTesting Loss: 534.758321\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.460403\n",
      "\tTesting Loss: 534.783518\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.461711\n",
      "\tTesting Loss: 534.777466\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.472905\n",
      "\tTesting Loss: 534.749878\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.469777\n",
      "\tTesting Loss: 534.770335\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.462468\n",
      "\tTesting Loss: 534.782817\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.471146\n",
      "\tTesting Loss: 534.748027\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.474637\n",
      "\tTesting Loss: 534.743896\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.462214\n",
      "\tTesting Loss: 534.755666\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.466507\n",
      "\tTesting Loss: 534.749674\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.475993\n",
      "\tTesting Loss: 534.747742\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.465790\n",
      "\tTesting Loss: 534.760132\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.470617\n",
      "\tTesting Loss: 534.737020\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.476552\n",
      "\tTesting Loss: 534.741089\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.469055\n",
      "\tTesting Loss: 534.754089\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.473849\n",
      "\tTesting Loss: 534.732869\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.477814\n",
      "\tTesting Loss: 534.722575\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.472346\n",
      "\tTesting Loss: 534.736572\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.471245\n",
      "\tTesting Loss: 534.745300\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.472176\n",
      "\tTesting Loss: 534.726847\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.483017\n",
      "\tTesting Loss: 534.702067\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.467387\n",
      "\tTesting Loss: 534.737722\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.472115\n",
      "\tTesting Loss: 534.702443\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.482168\n",
      "\tTesting Loss: 534.707662\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.477946\n",
      "\tTesting Loss: 534.714091\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.471085\n",
      "\tTesting Loss: 534.700450\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.478869\n",
      "\tTesting Loss: 534.693695\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.480164\n",
      "\tTesting Loss: 534.698059\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.473925\n",
      "\tTesting Loss: 534.707296\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.473546\n",
      "\tTesting Loss: 534.697083\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.485580\n",
      "\tTesting Loss: 534.675334\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.478816\n",
      "\tTesting Loss: 534.685384\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.471934\n",
      "\tTesting Loss: 534.697896\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.483022\n",
      "\tTesting Loss: 534.675232\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.481191\n",
      "\tTesting Loss: 534.684408\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.483231\n",
      "\tTesting Loss: 534.677511\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.479266\n",
      "\tTesting Loss: 534.675232\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.476151\n",
      "\tTesting Loss: 534.694824\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.485380\n",
      "\tTesting Loss: 534.645406\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.486323\n",
      "\tTesting Loss: 534.655457\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.473696\n",
      "\tTesting Loss: 534.673706\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.483088\n",
      "\tTesting Loss: 534.646525\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.493113\n",
      "\tTesting Loss: 534.645009\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.478017\n",
      "\tTesting Loss: 534.667033\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.479991\n",
      "\tTesting Loss: 534.640625\n",
      "\tLearning Rate: 0.000017970\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.489647\n",
      "\tTesting Loss: 534.639099\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.486267\n",
      "\tTesting Loss: 534.645813\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.481929\n",
      "\tTesting Loss: 534.632253\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.484177\n",
      "\tTesting Loss: 534.650726\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.486572\n",
      "\tTesting Loss: 534.628591\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.484772\n",
      "\tTesting Loss: 534.627502\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.480845\n",
      "\tTesting Loss: 534.634786\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.491048\n",
      "\tTesting Loss: 534.624390\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.488668\n",
      "\tTesting Loss: 534.625905\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.482015\n",
      "\tTesting Loss: 534.631419\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.492277\n",
      "\tTesting Loss: 534.608582\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.487518\n",
      "\tTesting Loss: 534.619609\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.483597\n",
      "\tTesting Loss: 534.627126\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.490598\n",
      "\tTesting Loss: 534.601501\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.493680\n",
      "\tTesting Loss: 534.598063\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.483765\n",
      "\tTesting Loss: 534.624339\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.488759\n",
      "\tTesting Loss: 534.597717\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.496134\n",
      "\tTesting Loss: 534.588572\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.484858\n",
      "\tTesting Loss: 534.610779\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.490362\n",
      "\tTesting Loss: 534.583954\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.493451\n",
      "\tTesting Loss: 534.585032\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.490540\n",
      "\tTesting Loss: 534.592692\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.482559\n",
      "\tTesting Loss: 534.594767\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.493637\n",
      "\tTesting Loss: 534.578186\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.493004\n",
      "\tTesting Loss: 534.588501\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.487778\n",
      "\tTesting Loss: 534.584595\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.491150\n",
      "\tTesting Loss: 534.569417\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.492432\n",
      "\tTesting Loss: 534.577962\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.493749\n",
      "\tTesting Loss: 534.571747\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.491020\n",
      "\tTesting Loss: 534.566772\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.489644\n",
      "\tTesting Loss: 534.575663\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.499730\n",
      "\tTesting Loss: 534.547262\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.493698\n",
      "\tTesting Loss: 534.562358\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.486206\n",
      "\tTesting Loss: 534.571493\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.494746\n",
      "\tTesting Loss: 534.548218\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.500407\n",
      "\tTesting Loss: 534.541423\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.490199\n",
      "\tTesting Loss: 534.559987\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.490616\n",
      "\tTesting Loss: 534.544027\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.497320\n",
      "\tTesting Loss: 534.536845\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.493134\n",
      "\tTesting Loss: 534.545654\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.491404\n",
      "\tTesting Loss: 534.542948\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.494415\n",
      "\tTesting Loss: 534.534485\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.491992\n",
      "\tTesting Loss: 534.544556\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.495911\n",
      "\tTesting Loss: 534.527374\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.500895\n",
      "\tTesting Loss: 534.508077\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.485001\n",
      "\tTesting Loss: 534.540934\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.491191\n",
      "\tTesting Loss: 534.511820\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.501350\n",
      "\tTesting Loss: 534.502096\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.493200\n",
      "\tTesting Loss: 534.523041\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.493123\n",
      "\tTesting Loss: 534.509440\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.502482\n",
      "\tTesting Loss: 534.490194\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.490196\n",
      "\tTesting Loss: 534.521088\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.492030\n",
      "\tTesting Loss: 534.493805\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.500051\n",
      "\tTesting Loss: 534.478353\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.494151\n",
      "\tTesting Loss: 534.502380\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.493083\n",
      "\tTesting Loss: 534.494019\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.497787\n",
      "\tTesting Loss: 534.480570\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.498896\n",
      "\tTesting Loss: 534.485565\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.492724\n",
      "\tTesting Loss: 534.490011\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.492119\n",
      "\tTesting Loss: 534.482890\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.497826\n",
      "\tTesting Loss: 534.468018\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.498103\n",
      "\tTesting Loss: 534.462992\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.495697\n",
      "\tTesting Loss: 534.468058\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.487434\n",
      "\tTesting Loss: 534.492157\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.499863\n",
      "\tTesting Loss: 534.432556\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.501684\n",
      "\tTesting Loss: 534.446838\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.486201\n",
      "\tTesting Loss: 534.467183\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.498970\n",
      "\tTesting Loss: 534.427480\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.504476\n",
      "\tTesting Loss: 534.452067\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.486465\n",
      "\tTesting Loss: 534.469859\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.496946\n",
      "\tTesting Loss: 534.411509\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.503459\n",
      "\tTesting Loss: 534.447774\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.486267\n",
      "\tTesting Loss: 534.474915\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.496882\n",
      "\tTesting Loss: 534.399556\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.501709\n",
      "\tTesting Loss: 534.432170\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.488693\n",
      "\tTesting Loss: 534.452901\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.493248\n",
      "\tTesting Loss: 534.409882\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.501750\n",
      "\tTesting Loss: 534.427999\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.489230\n",
      "\tTesting Loss: 534.450012\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.495499\n",
      "\tTesting Loss: 534.390015\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.498238\n",
      "\tTesting Loss: 534.418437\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.491249\n",
      "\tTesting Loss: 534.433919\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.497121\n",
      "\tTesting Loss: 534.392578\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.499034\n",
      "\tTesting Loss: 534.415914\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.492284\n",
      "\tTesting Loss: 534.424276\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.498179\n",
      "\tTesting Loss: 534.377258\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.497203\n",
      "\tTesting Loss: 534.405904\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.488724\n",
      "\tTesting Loss: 534.420451\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.498947\n",
      "\tTesting Loss: 534.370626\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.498856\n",
      "\tTesting Loss: 534.394531\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.490067\n",
      "\tTesting Loss: 534.408569\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.493134\n",
      "\tTesting Loss: 534.383494\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.498784\n",
      "\tTesting Loss: 534.377655\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.499382\n",
      "\tTesting Loss: 534.373210\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.488363\n",
      "\tTesting Loss: 534.396139\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.496490\n",
      "\tTesting Loss: 534.370565\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.502731\n",
      "\tTesting Loss: 534.359314\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.495316\n",
      "\tTesting Loss: 534.380676\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.491714\n",
      "\tTesting Loss: 534.383311\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.493721\n",
      "\tTesting Loss: 534.374471\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.508072\n",
      "\tTesting Loss: 534.332733\n",
      "\tLearning Rate: 0.000016173\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.491404\n",
      "\tTesting Loss: 534.378164\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.489939\n",
      "\tTesting Loss: 534.360758\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.505872\n",
      "\tTesting Loss: 534.327972\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.497409\n",
      "\tTesting Loss: 534.374003\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.490448\n",
      "\tTesting Loss: 534.354899\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.502080\n",
      "\tTesting Loss: 534.311849\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.497332\n",
      "\tTesting Loss: 534.358866\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.487183\n",
      "\tTesting Loss: 534.360087\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.503794\n",
      "\tTesting Loss: 534.292989\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.498062\n",
      "\tTesting Loss: 534.356791\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.484258\n",
      "\tTesting Loss: 534.358378\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.503057\n",
      "\tTesting Loss: 534.280782\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.500183\n",
      "\tTesting Loss: 534.353984\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.482422\n",
      "\tTesting Loss: 534.372447\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.501452\n",
      "\tTesting Loss: 534.259338\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.498189\n",
      "\tTesting Loss: 534.347748\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.480026\n",
      "\tTesting Loss: 534.381266\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.499430\n",
      "\tTesting Loss: 534.258240\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.497650\n",
      "\tTesting Loss: 534.336680\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.484907\n",
      "\tTesting Loss: 534.375132\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.492625\n",
      "\tTesting Loss: 534.269877\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.504361\n",
      "\tTesting Loss: 534.297933\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.487417\n",
      "\tTesting Loss: 534.383260\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.485654\n",
      "\tTesting Loss: 534.285604\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.504939\n",
      "\tTesting Loss: 534.265727\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.489815\n",
      "\tTesting Loss: 534.381938\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.482862\n",
      "\tTesting Loss: 534.299947\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.504303\n",
      "\tTesting Loss: 534.241414\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.488856\n",
      "\tTesting Loss: 534.370219\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.480047\n",
      "\tTesting Loss: 534.310018\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.497424\n",
      "\tTesting Loss: 534.232829\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.491908\n",
      "\tTesting Loss: 534.337748\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.474162\n",
      "\tTesting Loss: 534.342336\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.500966\n",
      "\tTesting Loss: 534.222168\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.493968\n",
      "\tTesting Loss: 534.321950\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.473745\n",
      "\tTesting Loss: 534.362762\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.495178\n",
      "\tTesting Loss: 534.227498\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.496854\n",
      "\tTesting Loss: 534.290568\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.469927\n",
      "\tTesting Loss: 534.376017\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.487010\n",
      "\tTesting Loss: 534.232127\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.493228\n",
      "\tTesting Loss: 534.279256\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.476123\n",
      "\tTesting Loss: 534.385885\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.479741\n",
      "\tTesting Loss: 534.257202\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.496986\n",
      "\tTesting Loss: 534.249349\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.475899\n",
      "\tTesting Loss: 534.381134\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.476583\n",
      "\tTesting Loss: 534.265849\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.497213\n",
      "\tTesting Loss: 534.242869\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.476217\n",
      "\tTesting Loss: 534.369364\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.471720\n",
      "\tTesting Loss: 534.270467\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.499054\n",
      "\tTesting Loss: 534.227539\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.475433\n",
      "\tTesting Loss: 534.354960\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.470317\n",
      "\tTesting Loss: 534.277527\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.500290\n",
      "\tTesting Loss: 534.220337\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.463178\n",
      "\tTesting Loss: 534.366943\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.465805\n",
      "\tTesting Loss: 534.254191\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.496526\n",
      "\tTesting Loss: 534.235555\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.459427\n",
      "\tTesting Loss: 534.373494\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.467890\n",
      "\tTesting Loss: 534.253794\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.497416\n",
      "\tTesting Loss: 534.247843\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.449760\n",
      "\tTesting Loss: 534.367757\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.464353\n",
      "\tTesting Loss: 534.241384\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.492640\n",
      "\tTesting Loss: 534.267639\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.446294\n",
      "\tTesting Loss: 534.368937\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.467623\n",
      "\tTesting Loss: 534.240438\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.483195\n",
      "\tTesting Loss: 534.293620\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.450953\n",
      "\tTesting Loss: 534.334534\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.471100\n",
      "\tTesting Loss: 534.248047\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.479050\n",
      "\tTesting Loss: 534.290588\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.452967\n",
      "\tTesting Loss: 534.305888\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.476730\n",
      "\tTesting Loss: 534.237478\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.470378\n",
      "\tTesting Loss: 534.289714\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.463587\n",
      "\tTesting Loss: 534.265096\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.480858\n",
      "\tTesting Loss: 534.229747\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.471408\n",
      "\tTesting Loss: 534.267049\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.471102\n",
      "\tTesting Loss: 534.249451\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.480596\n",
      "\tTesting Loss: 534.229095\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.472529\n",
      "\tTesting Loss: 534.252950\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.470154\n",
      "\tTesting Loss: 534.234975\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.483363\n",
      "\tTesting Loss: 534.210490\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.469460\n",
      "\tTesting Loss: 534.239115\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.472056\n",
      "\tTesting Loss: 534.213175\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.483526\n",
      "\tTesting Loss: 534.202850\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.470617\n",
      "\tTesting Loss: 534.230937\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.472585\n",
      "\tTesting Loss: 534.199432\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.481837\n",
      "\tTesting Loss: 534.187164\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.468170\n",
      "\tTesting Loss: 534.228597\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.471634\n",
      "\tTesting Loss: 534.187297\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.483086\n",
      "\tTesting Loss: 534.175252\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.465322\n",
      "\tTesting Loss: 534.226542\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.472504\n",
      "\tTesting Loss: 534.174357\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.484495\n",
      "\tTesting Loss: 534.172709\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.465266\n",
      "\tTesting Loss: 534.228302\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.470172\n",
      "\tTesting Loss: 534.164825\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.485046\n",
      "\tTesting Loss: 534.162730\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.462087\n",
      "\tTesting Loss: 534.238607\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.468341\n",
      "\tTesting Loss: 534.160706\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.486699\n",
      "\tTesting Loss: 534.151784\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.457876\n",
      "\tTesting Loss: 534.243846\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.467204\n",
      "\tTesting Loss: 534.139567\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.480657\n",
      "\tTesting Loss: 534.158803\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.456904\n",
      "\tTesting Loss: 534.249878\n",
      "\tLearning Rate: 0.000014556\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.467595\n",
      "\tTesting Loss: 534.146973\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.478373\n",
      "\tTesting Loss: 534.169820\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.459249\n",
      "\tTesting Loss: 534.218486\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.470093\n",
      "\tTesting Loss: 534.139109\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.475838\n",
      "\tTesting Loss: 534.181152\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.461398\n",
      "\tTesting Loss: 534.184519\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.472702\n",
      "\tTesting Loss: 534.140015\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.473846\n",
      "\tTesting Loss: 534.172607\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.460902\n",
      "\tTesting Loss: 534.170451\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.475609\n",
      "\tTesting Loss: 534.136068\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.470856\n",
      "\tTesting Loss: 534.168620\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.465703\n",
      "\tTesting Loss: 534.150330\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.474424\n",
      "\tTesting Loss: 534.130727\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.470479\n",
      "\tTesting Loss: 534.152761\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.467707\n",
      "\tTesting Loss: 534.141805\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.471029\n",
      "\tTesting Loss: 534.126434\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.473216\n",
      "\tTesting Loss: 534.137472\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.468999\n",
      "\tTesting Loss: 534.131093\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.473325\n",
      "\tTesting Loss: 534.124410\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.471390\n",
      "\tTesting Loss: 534.129761\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.467616\n",
      "\tTesting Loss: 534.115987\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.471591\n",
      "\tTesting Loss: 534.108480\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.469391\n",
      "\tTesting Loss: 534.127401\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.468015\n",
      "\tTesting Loss: 534.110413\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.470212\n",
      "\tTesting Loss: 534.104919\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.471130\n",
      "\tTesting Loss: 534.112508\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.468394\n",
      "\tTesting Loss: 534.102437\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.470983\n",
      "\tTesting Loss: 534.099518\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.472412\n",
      "\tTesting Loss: 534.099752\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.468602\n",
      "\tTesting Loss: 534.098368\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.467557\n",
      "\tTesting Loss: 534.103414\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.468816\n",
      "\tTesting Loss: 534.092763\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.469549\n",
      "\tTesting Loss: 534.086446\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.469594\n",
      "\tTesting Loss: 534.091441\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.468002\n",
      "\tTesting Loss: 534.096263\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.468732\n",
      "\tTesting Loss: 534.083557\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.472427\n",
      "\tTesting Loss: 534.075226\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.470408\n",
      "\tTesting Loss: 534.081278\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.464472\n",
      "\tTesting Loss: 534.093526\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.470317\n",
      "\tTesting Loss: 534.066020\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.469999\n",
      "\tTesting Loss: 534.078217\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.466125\n",
      "\tTesting Loss: 534.083466\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.468122\n",
      "\tTesting Loss: 534.059611\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.470220\n",
      "\tTesting Loss: 534.068278\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.467784\n",
      "\tTesting Loss: 534.074361\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.467153\n",
      "\tTesting Loss: 534.061320\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.467623\n",
      "\tTesting Loss: 534.073334\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.471110\n",
      "\tTesting Loss: 534.052327\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.470441\n",
      "\tTesting Loss: 534.056274\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.472417\n",
      "\tTesting Loss: 534.056905\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.470205\n",
      "\tTesting Loss: 534.058411\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.468048\n",
      "\tTesting Loss: 534.056569\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.471494\n",
      "\tTesting Loss: 534.043193\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.470088\n",
      "\tTesting Loss: 534.050069\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.467936\n",
      "\tTesting Loss: 534.054606\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.468369\n",
      "\tTesting Loss: 534.053345\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.468353\n",
      "\tTesting Loss: 534.042226\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.475474\n",
      "\tTesting Loss: 534.026845\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.467224\n",
      "\tTesting Loss: 534.056274\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.472346\n",
      "\tTesting Loss: 534.035868\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.476329\n",
      "\tTesting Loss: 534.018250\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.464859\n",
      "\tTesting Loss: 534.052856\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.469699\n",
      "\tTesting Loss: 534.029378\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.474299\n",
      "\tTesting Loss: 534.017507\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.465169\n",
      "\tTesting Loss: 534.044037\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.468925\n",
      "\tTesting Loss: 534.013082\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.475581\n",
      "\tTesting Loss: 534.005961\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.465408\n",
      "\tTesting Loss: 534.043986\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.467222\n",
      "\tTesting Loss: 534.012756\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.478836\n",
      "\tTesting Loss: 533.985819\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.466741\n",
      "\tTesting Loss: 534.042908\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.469615\n",
      "\tTesting Loss: 533.999532\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.477575\n",
      "\tTesting Loss: 533.978149\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.468419\n",
      "\tTesting Loss: 534.042643\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.463908\n",
      "\tTesting Loss: 534.012553\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.476003\n",
      "\tTesting Loss: 533.960632\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.469828\n",
      "\tTesting Loss: 534.028951\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.459178\n",
      "\tTesting Loss: 534.027608\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.471840\n",
      "\tTesting Loss: 533.943197\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.467827\n",
      "\tTesting Loss: 534.008484\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.460887\n",
      "\tTesting Loss: 534.039103\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.467295\n",
      "\tTesting Loss: 533.960551\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.475006\n",
      "\tTesting Loss: 533.973633\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.467435\n",
      "\tTesting Loss: 534.042480\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.465719\n",
      "\tTesting Loss: 533.978892\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.472928\n",
      "\tTesting Loss: 533.952942\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.470840\n",
      "\tTesting Loss: 534.016388\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.460704\n",
      "\tTesting Loss: 534.013306\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.468997\n",
      "\tTesting Loss: 533.944804\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.471278\n",
      "\tTesting Loss: 533.975505\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.459198\n",
      "\tTesting Loss: 534.030497\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.460704\n",
      "\tTesting Loss: 533.965729\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.477951\n",
      "\tTesting Loss: 533.933350\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.469360\n",
      "\tTesting Loss: 534.018494\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.457575\n",
      "\tTesting Loss: 533.990784\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.471878\n",
      "\tTesting Loss: 533.917887\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.472003\n",
      "\tTesting Loss: 533.985616\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.449384\n",
      "\tTesting Loss: 534.048523\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.462911\n",
      "\tTesting Loss: 533.905823\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.473969\n",
      "\tTesting Loss: 533.928050\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.454712\n",
      "\tTesting Loss: 534.074127\n",
      "\tLearning Rate: 0.000013100\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.452459\n",
      "\tTesting Loss: 533.961985\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.473620\n",
      "\tTesting Loss: 533.905924\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.455338\n",
      "\tTesting Loss: 534.051086\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.450272\n",
      "\tTesting Loss: 533.974935\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.473984\n",
      "\tTesting Loss: 533.910685\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.450587\n",
      "\tTesting Loss: 534.022217\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.449183\n",
      "\tTesting Loss: 533.975749\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.472397\n",
      "\tTesting Loss: 533.921651\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.449214\n",
      "\tTesting Loss: 534.015991\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.450185\n",
      "\tTesting Loss: 533.969991\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.469655\n",
      "\tTesting Loss: 533.914388\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.446818\n",
      "\tTesting Loss: 534.002726\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.454285\n",
      "\tTesting Loss: 533.955017\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.468755\n",
      "\tTesting Loss: 533.912760\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.448751\n",
      "\tTesting Loss: 533.994609\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.453018\n",
      "\tTesting Loss: 533.948527\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.463628\n",
      "\tTesting Loss: 533.920553\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.446676\n",
      "\tTesting Loss: 533.986603\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.452479\n",
      "\tTesting Loss: 533.940887\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.461197\n",
      "\tTesting Loss: 533.916056\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.449046\n",
      "\tTesting Loss: 533.978658\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.452970\n",
      "\tTesting Loss: 533.936768\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.465708\n",
      "\tTesting Loss: 533.901855\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.446609\n",
      "\tTesting Loss: 533.977804\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.450592\n",
      "\tTesting Loss: 533.927490\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.461151\n",
      "\tTesting Loss: 533.899007\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.445719\n",
      "\tTesting Loss: 533.963369\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.447830\n",
      "\tTesting Loss: 533.929403\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.462168\n",
      "\tTesting Loss: 533.890828\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.448578\n",
      "\tTesting Loss: 533.955760\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.447985\n",
      "\tTesting Loss: 533.923462\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.459015\n",
      "\tTesting Loss: 533.891856\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.447879\n",
      "\tTesting Loss: 533.959025\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.445404\n",
      "\tTesting Loss: 533.930074\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.461563\n",
      "\tTesting Loss: 533.873515\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.446096\n",
      "\tTesting Loss: 533.947164\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.448827\n",
      "\tTesting Loss: 533.917358\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.459058\n",
      "\tTesting Loss: 533.871257\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.446691\n",
      "\tTesting Loss: 533.938456\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.448339\n",
      "\tTesting Loss: 533.920451\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.455666\n",
      "\tTesting Loss: 533.877065\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.450027\n",
      "\tTesting Loss: 533.922821\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.446968\n",
      "\tTesting Loss: 533.917969\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.458956\n",
      "\tTesting Loss: 533.855723\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.448761\n",
      "\tTesting Loss: 533.907288\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.443944\n",
      "\tTesting Loss: 533.926860\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.454870\n",
      "\tTesting Loss: 533.850789\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.447718\n",
      "\tTesting Loss: 533.895732\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.443481\n",
      "\tTesting Loss: 533.925771\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.455892\n",
      "\tTesting Loss: 533.852397\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.448792\n",
      "\tTesting Loss: 533.891622\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.444178\n",
      "\tTesting Loss: 533.923645\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.449412\n",
      "\tTesting Loss: 533.857707\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.450343\n",
      "\tTesting Loss: 533.873108\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.446576\n",
      "\tTesting Loss: 533.914062\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.448263\n",
      "\tTesting Loss: 533.861593\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.451505\n",
      "\tTesting Loss: 533.854797\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.443647\n",
      "\tTesting Loss: 533.909220\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.442060\n",
      "\tTesting Loss: 533.879150\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.452616\n",
      "\tTesting Loss: 533.836975\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.446686\n",
      "\tTesting Loss: 533.886149\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.441071\n",
      "\tTesting Loss: 533.884481\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.451696\n",
      "\tTesting Loss: 533.827220\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.446177\n",
      "\tTesting Loss: 533.874868\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.440303\n",
      "\tTesting Loss: 533.899292\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.447291\n",
      "\tTesting Loss: 533.822734\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.448324\n",
      "\tTesting Loss: 533.848623\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.440859\n",
      "\tTesting Loss: 533.901733\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.442268\n",
      "\tTesting Loss: 533.840627\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.448451\n",
      "\tTesting Loss: 533.823405\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.443113\n",
      "\tTesting Loss: 533.889201\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.440196\n",
      "\tTesting Loss: 533.865204\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.451248\n",
      "\tTesting Loss: 533.800527\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.441579\n",
      "\tTesting Loss: 533.863871\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.432658\n",
      "\tTesting Loss: 533.886353\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.447299\n",
      "\tTesting Loss: 533.787791\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.446716\n",
      "\tTesting Loss: 533.832377\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.436910\n",
      "\tTesting Loss: 533.903849\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.440297\n",
      "\tTesting Loss: 533.810120\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.447281\n",
      "\tTesting Loss: 533.798828\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.439311\n",
      "\tTesting Loss: 533.893148\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.427335\n",
      "\tTesting Loss: 533.865285\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.444824\n",
      "\tTesting Loss: 533.764526\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.441940\n",
      "\tTesting Loss: 533.841319\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.424230\n",
      "\tTesting Loss: 533.911540\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.442835\n",
      "\tTesting Loss: 533.771362\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.449290\n",
      "\tTesting Loss: 533.778666\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.420431\n",
      "\tTesting Loss: 533.959757\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.427567\n",
      "\tTesting Loss: 533.809387\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.449361\n",
      "\tTesting Loss: 533.750753\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.418073\n",
      "\tTesting Loss: 533.937754\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.416746\n",
      "\tTesting Loss: 533.853617\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.453456\n",
      "\tTesting Loss: 533.744639\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.410355\n",
      "\tTesting Loss: 533.917643\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.418172\n",
      "\tTesting Loss: 533.856893\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.441994\n",
      "\tTesting Loss: 533.773315\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.414210\n",
      "\tTesting Loss: 533.881938\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.415761\n",
      "\tTesting Loss: 533.857941\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.435527\n",
      "\tTesting Loss: 533.774699\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.417692\n",
      "\tTesting Loss: 533.853444\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.415154\n",
      "\tTesting Loss: 533.857513\n",
      "\tLearning Rate: 0.000011790\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.428360\n",
      "\tTesting Loss: 533.786082\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.427429\n",
      "\tTesting Loss: 533.826131\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.418162\n",
      "\tTesting Loss: 533.845866\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.429263\n",
      "\tTesting Loss: 533.790019\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.428716\n",
      "\tTesting Loss: 533.822184\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.417023\n",
      "\tTesting Loss: 533.834880\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.429296\n",
      "\tTesting Loss: 533.785400\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.425316\n",
      "\tTesting Loss: 533.812480\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.418528\n",
      "\tTesting Loss: 533.815308\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.424383\n",
      "\tTesting Loss: 533.787903\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.424975\n",
      "\tTesting Loss: 533.803121\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.420359\n",
      "\tTesting Loss: 533.815247\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.427004\n",
      "\tTesting Loss: 533.779470\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.424850\n",
      "\tTesting Loss: 533.797750\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.420970\n",
      "\tTesting Loss: 533.809652\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.426229\n",
      "\tTesting Loss: 533.784546\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.426796\n",
      "\tTesting Loss: 533.778849\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.421382\n",
      "\tTesting Loss: 533.796183\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.426697\n",
      "\tTesting Loss: 533.772603\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.426488\n",
      "\tTesting Loss: 533.785177\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.424225\n",
      "\tTesting Loss: 533.785828\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.422206\n",
      "\tTesting Loss: 533.774089\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.425087\n",
      "\tTesting Loss: 533.770793\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.422724\n",
      "\tTesting Loss: 533.777873\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.423236\n",
      "\tTesting Loss: 533.769460\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.427841\n",
      "\tTesting Loss: 533.764303\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.425557\n",
      "\tTesting Loss: 533.772685\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.423869\n",
      "\tTesting Loss: 533.760081\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.425387\n",
      "\tTesting Loss: 533.759745\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.419807\n",
      "\tTesting Loss: 533.777079\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.425268\n",
      "\tTesting Loss: 533.749349\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.425240\n",
      "\tTesting Loss: 533.755269\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.423195\n",
      "\tTesting Loss: 533.763977\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.424973\n",
      "\tTesting Loss: 533.756266\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.424540\n",
      "\tTesting Loss: 533.751953\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.422852\n",
      "\tTesting Loss: 533.756866\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.421954\n",
      "\tTesting Loss: 533.756419\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.423462\n",
      "\tTesting Loss: 533.751017\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.423180\n",
      "\tTesting Loss: 533.753296\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.424072\n",
      "\tTesting Loss: 533.747142\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.424314\n",
      "\tTesting Loss: 533.739950\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.424220\n",
      "\tTesting Loss: 533.744029\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.419907\n",
      "\tTesting Loss: 533.756042\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.424077\n",
      "\tTesting Loss: 533.734945\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.423281\n",
      "\tTesting Loss: 533.732015\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.420008\n",
      "\tTesting Loss: 533.742900\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.424009\n",
      "\tTesting Loss: 533.734151\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.423678\n",
      "\tTesting Loss: 533.734517\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.418856\n",
      "\tTesting Loss: 533.736888\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.422424\n",
      "\tTesting Loss: 533.725627\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.423360\n",
      "\tTesting Loss: 533.727030\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.417582\n",
      "\tTesting Loss: 533.736511\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.421148\n",
      "\tTesting Loss: 533.717336\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.423894\n",
      "\tTesting Loss: 533.719666\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.419706\n",
      "\tTesting Loss: 533.731995\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.420748\n",
      "\tTesting Loss: 533.726196\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.423340\n",
      "\tTesting Loss: 533.718516\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.421371\n",
      "\tTesting Loss: 533.712280\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.418381\n",
      "\tTesting Loss: 533.715454\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.418460\n",
      "\tTesting Loss: 533.718129\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.422633\n",
      "\tTesting Loss: 533.710612\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.421422\n",
      "\tTesting Loss: 533.711751\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.421079\n",
      "\tTesting Loss: 533.706421\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.422892\n",
      "\tTesting Loss: 533.702616\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.419937\n",
      "\tTesting Loss: 533.707743\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.417994\n",
      "\tTesting Loss: 533.708557\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.421636\n",
      "\tTesting Loss: 533.700358\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.420492\n",
      "\tTesting Loss: 533.701457\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.419464\n",
      "\tTesting Loss: 533.702057\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.421844\n",
      "\tTesting Loss: 533.692190\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.422048\n",
      "\tTesting Loss: 533.692078\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.417875\n",
      "\tTesting Loss: 533.698171\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.417676\n",
      "\tTesting Loss: 533.696604\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.420756\n",
      "\tTesting Loss: 533.690674\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.419263\n",
      "\tTesting Loss: 533.692434\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.421982\n",
      "\tTesting Loss: 533.677673\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.424927\n",
      "\tTesting Loss: 533.677572\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.419256\n",
      "\tTesting Loss: 533.690216\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.416926\n",
      "\tTesting Loss: 533.689270\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.420680\n",
      "\tTesting Loss: 533.672821\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.421778\n",
      "\tTesting Loss: 533.674947\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.420108\n",
      "\tTesting Loss: 533.678019\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.421522\n",
      "\tTesting Loss: 533.669047\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.420700\n",
      "\tTesting Loss: 533.674174\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.416349\n",
      "\tTesting Loss: 533.681468\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.419205\n",
      "\tTesting Loss: 533.668681\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.422773\n",
      "\tTesting Loss: 533.654785\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.422145\n",
      "\tTesting Loss: 533.666158\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.421326\n",
      "\tTesting Loss: 533.671275\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.419362\n",
      "\tTesting Loss: 533.668925\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.423645\n",
      "\tTesting Loss: 533.648885\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.422699\n",
      "\tTesting Loss: 533.658773\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.417837\n",
      "\tTesting Loss: 533.668742\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.417786\n",
      "\tTesting Loss: 533.660899\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.421000\n",
      "\tTesting Loss: 533.647380\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.425532\n",
      "\tTesting Loss: 533.636332\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.422038\n",
      "\tTesting Loss: 533.651143\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.418371\n",
      "\tTesting Loss: 533.656525\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.420245\n",
      "\tTesting Loss: 533.645721\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.422160\n",
      "\tTesting Loss: 533.636047\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.418528\n",
      "\tTesting Loss: 533.639872\n",
      "\tLearning Rate: 0.000010611\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.419988\n",
      "\tTesting Loss: 533.642171\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.425278\n",
      "\tTesting Loss: 533.632273\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.422620\n",
      "\tTesting Loss: 533.639893\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.421422\n",
      "\tTesting Loss: 533.643911\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.420904\n",
      "\tTesting Loss: 533.639404\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.422114\n",
      "\tTesting Loss: 533.629781\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.423523\n",
      "\tTesting Loss: 533.624329\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.422216\n",
      "\tTesting Loss: 533.631063\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.418238\n",
      "\tTesting Loss: 533.635986\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.422104\n",
      "\tTesting Loss: 533.617391\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.426610\n",
      "\tTesting Loss: 533.616394\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.427678\n",
      "\tTesting Loss: 533.620829\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.421679\n",
      "\tTesting Loss: 533.625519\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.422307\n",
      "\tTesting Loss: 533.619781\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.423327\n",
      "\tTesting Loss: 533.612132\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.424616\n",
      "\tTesting Loss: 533.618083\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.423368\n",
      "\tTesting Loss: 533.617249\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.424026\n",
      "\tTesting Loss: 533.609131\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.425379\n",
      "\tTesting Loss: 533.607320\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.426768\n",
      "\tTesting Loss: 533.603821\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.425166\n",
      "\tTesting Loss: 533.611938\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.420344\n",
      "\tTesting Loss: 533.613790\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.423538\n",
      "\tTesting Loss: 533.595886\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.428660\n",
      "\tTesting Loss: 533.585785\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.429418\n",
      "\tTesting Loss: 533.604309\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.423210\n",
      "\tTesting Loss: 533.614522\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.420578\n",
      "\tTesting Loss: 533.600637\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.427691\n",
      "\tTesting Loss: 533.579203\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.432348\n",
      "\tTesting Loss: 533.590068\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.422994\n",
      "\tTesting Loss: 533.608297\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.426275\n",
      "\tTesting Loss: 533.584646\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.431590\n",
      "\tTesting Loss: 533.579386\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.430379\n",
      "\tTesting Loss: 533.586629\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.425156\n",
      "\tTesting Loss: 533.589783\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.422251\n",
      "\tTesting Loss: 533.595378\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.426888\n",
      "\tTesting Loss: 533.573466\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.431163\n",
      "\tTesting Loss: 533.571706\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.429189\n",
      "\tTesting Loss: 533.584015\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.425781\n",
      "\tTesting Loss: 533.581401\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.431129\n",
      "\tTesting Loss: 533.566538\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.433459\n",
      "\tTesting Loss: 533.571269\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.425102\n",
      "\tTesting Loss: 533.574158\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.424306\n",
      "\tTesting Loss: 533.569112\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.432365\n",
      "\tTesting Loss: 533.555837\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.428899\n",
      "\tTesting Loss: 533.571238\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.429253\n",
      "\tTesting Loss: 533.560343\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.431580\n",
      "\tTesting Loss: 533.552999\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.432749\n",
      "\tTesting Loss: 533.557800\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.428902\n",
      "\tTesting Loss: 533.564657\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.430333\n",
      "\tTesting Loss: 533.555379\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.430303\n",
      "\tTesting Loss: 533.547190\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.433538\n",
      "\tTesting Loss: 533.549632\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.433268\n",
      "\tTesting Loss: 533.551270\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.431920\n",
      "\tTesting Loss: 533.547801\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.430341\n",
      "\tTesting Loss: 533.549276\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.430857\n",
      "\tTesting Loss: 533.546427\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.432048\n",
      "\tTesting Loss: 533.541718\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.433060\n",
      "\tTesting Loss: 533.532613\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.432602\n",
      "\tTesting Loss: 533.535726\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.433553\n",
      "\tTesting Loss: 533.543030\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.434222\n",
      "\tTesting Loss: 533.528951\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.433848\n",
      "\tTesting Loss: 533.530741\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.431880\n",
      "\tTesting Loss: 533.530090\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.432894\n",
      "\tTesting Loss: 533.524495\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.436895\n",
      "\tTesting Loss: 533.521535\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.433823\n",
      "\tTesting Loss: 533.528432\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.432269\n",
      "\tTesting Loss: 533.527547\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.434285\n",
      "\tTesting Loss: 533.522807\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.437154\n",
      "\tTesting Loss: 533.506154\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.437352\n",
      "\tTesting Loss: 533.507589\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.436101\n",
      "\tTesting Loss: 533.524780\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.430107\n",
      "\tTesting Loss: 533.536479\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.432963\n",
      "\tTesting Loss: 533.502726\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.443370\n",
      "\tTesting Loss: 533.490356\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.438296\n",
      "\tTesting Loss: 533.519796\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.432704\n",
      "\tTesting Loss: 533.517273\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.438156\n",
      "\tTesting Loss: 533.494263\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.436930\n",
      "\tTesting Loss: 533.515096\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.432449\n",
      "\tTesting Loss: 533.513590\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.439407\n",
      "\tTesting Loss: 533.483602\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.442261\n",
      "\tTesting Loss: 533.498118\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.435499\n",
      "\tTesting Loss: 533.516825\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.437113\n",
      "\tTesting Loss: 533.485881\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.441208\n",
      "\tTesting Loss: 533.491414\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.434865\n",
      "\tTesting Loss: 533.501282\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.437327\n",
      "\tTesting Loss: 533.479594\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.438675\n",
      "\tTesting Loss: 533.487640\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.439779\n",
      "\tTesting Loss: 533.480906\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.441157\n",
      "\tTesting Loss: 533.489278\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.436111\n",
      "\tTesting Loss: 533.496877\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.437861\n",
      "\tTesting Loss: 533.475983\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.444572\n",
      "\tTesting Loss: 533.464478\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.442017\n",
      "\tTesting Loss: 533.481405\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.434448\n",
      "\tTesting Loss: 533.490356\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.433970\n",
      "\tTesting Loss: 533.484985\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.441010\n",
      "\tTesting Loss: 533.458761\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.446747\n",
      "\tTesting Loss: 533.456319\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.438950\n",
      "\tTesting Loss: 533.495850\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.433263\n",
      "\tTesting Loss: 533.482768\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.445826\n",
      "\tTesting Loss: 533.431620\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.450297\n",
      "\tTesting Loss: 533.471212\n",
      "\tLearning Rate: 0.000009550\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.432134\n",
      "\tTesting Loss: 533.498444\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.433345\n",
      "\tTesting Loss: 533.446462\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.446793\n",
      "\tTesting Loss: 533.441274\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.450244\n",
      "\tTesting Loss: 533.478017\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.432526\n",
      "\tTesting Loss: 533.495626\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.434156\n",
      "\tTesting Loss: 533.432332\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.456111\n",
      "\tTesting Loss: 533.424500\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.446294\n",
      "\tTesting Loss: 533.531982\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.416425\n",
      "\tTesting Loss: 533.482168\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.453389\n",
      "\tTesting Loss: 533.350810\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.464198\n",
      "\tTesting Loss: 533.461009\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.423024\n",
      "\tTesting Loss: 533.613647\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.410233\n",
      "\tTesting Loss: 533.364705\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.478228\n",
      "\tTesting Loss: 533.360840\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.432467\n",
      "\tTesting Loss: 533.659047\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.386164\n",
      "\tTesting Loss: 533.422078\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.462606\n",
      "\tTesting Loss: 533.404399\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.418889\n",
      "\tTesting Loss: 533.592916\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.396327\n",
      "\tTesting Loss: 533.423238\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.451467\n",
      "\tTesting Loss: 533.441243\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.414597\n",
      "\tTesting Loss: 533.544393\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.413747\n",
      "\tTesting Loss: 533.413106\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.443309\n",
      "\tTesting Loss: 533.452657\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.420532\n",
      "\tTesting Loss: 533.511414\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.419228\n",
      "\tTesting Loss: 533.422302\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.441198\n",
      "\tTesting Loss: 533.445516\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.422755\n",
      "\tTesting Loss: 533.486613\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.421366\n",
      "\tTesting Loss: 533.425018\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.440631\n",
      "\tTesting Loss: 533.441956\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.427892\n",
      "\tTesting Loss: 533.479604\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.424294\n",
      "\tTesting Loss: 533.426819\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.440201\n",
      "\tTesting Loss: 533.430888\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.429153\n",
      "\tTesting Loss: 533.468292\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.423243\n",
      "\tTesting Loss: 533.425527\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.439723\n",
      "\tTesting Loss: 533.419027\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.430786\n",
      "\tTesting Loss: 533.469543\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.426628\n",
      "\tTesting Loss: 533.419047\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.443019\n",
      "\tTesting Loss: 533.410482\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.431259\n",
      "\tTesting Loss: 533.462179\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.426654\n",
      "\tTesting Loss: 533.418884\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.441294\n",
      "\tTesting Loss: 533.401255\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.432678\n",
      "\tTesting Loss: 533.455994\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.425318\n",
      "\tTesting Loss: 533.418823\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.441488\n",
      "\tTesting Loss: 533.399658\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.434270\n",
      "\tTesting Loss: 533.452637\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.426844\n",
      "\tTesting Loss: 533.418498\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.443601\n",
      "\tTesting Loss: 533.384918\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.436691\n",
      "\tTesting Loss: 533.446899\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.423798\n",
      "\tTesting Loss: 533.420776\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.440750\n",
      "\tTesting Loss: 533.384694\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.439372\n",
      "\tTesting Loss: 533.436117\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.425662\n",
      "\tTesting Loss: 533.425863\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.440465\n",
      "\tTesting Loss: 533.377136\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.439931\n",
      "\tTesting Loss: 533.432353\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.423696\n",
      "\tTesting Loss: 533.429220\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.438858\n",
      "\tTesting Loss: 533.365723\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.443202\n",
      "\tTesting Loss: 533.423859\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.427221\n",
      "\tTesting Loss: 533.436859\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.437881\n",
      "\tTesting Loss: 533.367228\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.441190\n",
      "\tTesting Loss: 533.414408\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.428604\n",
      "\tTesting Loss: 533.435140\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.432452\n",
      "\tTesting Loss: 533.371429\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.444763\n",
      "\tTesting Loss: 533.398153\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.433980\n",
      "\tTesting Loss: 533.444377\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.428747\n",
      "\tTesting Loss: 533.387095\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.444392\n",
      "\tTesting Loss: 533.380290\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.436111\n",
      "\tTesting Loss: 533.441142\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.423533\n",
      "\tTesting Loss: 533.393270\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.443573\n",
      "\tTesting Loss: 533.365143\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.441874\n",
      "\tTesting Loss: 533.433797\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.424553\n",
      "\tTesting Loss: 533.413676\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.443954\n",
      "\tTesting Loss: 533.345398\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.441783\n",
      "\tTesting Loss: 533.421224\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.422999\n",
      "\tTesting Loss: 533.426086\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.441106\n",
      "\tTesting Loss: 533.339681\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.444359\n",
      "\tTesting Loss: 533.411092\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.425257\n",
      "\tTesting Loss: 533.430410\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.436584\n",
      "\tTesting Loss: 533.339457\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.446818\n",
      "\tTesting Loss: 533.392090\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.430298\n",
      "\tTesting Loss: 533.441386\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.433548\n",
      "\tTesting Loss: 533.345215\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.448939\n",
      "\tTesting Loss: 533.370890\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.432070\n",
      "\tTesting Loss: 533.449259\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.427236\n",
      "\tTesting Loss: 533.363831\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.445257\n",
      "\tTesting Loss: 533.367167\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.435364\n",
      "\tTesting Loss: 533.434855\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.423258\n",
      "\tTesting Loss: 533.381551\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.441681\n",
      "\tTesting Loss: 533.349884\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.442556\n",
      "\tTesting Loss: 533.417847\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.424446\n",
      "\tTesting Loss: 533.399801\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.440348\n",
      "\tTesting Loss: 533.334137\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.447103\n",
      "\tTesting Loss: 533.397461\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.425903\n",
      "\tTesting Loss: 533.423014\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.439397\n",
      "\tTesting Loss: 533.323690\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.445704\n",
      "\tTesting Loss: 533.384043\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.426506\n",
      "\tTesting Loss: 533.425222\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.433861\n",
      "\tTesting Loss: 533.323832\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.448346\n",
      "\tTesting Loss: 533.364827\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.430700\n",
      "\tTesting Loss: 533.440572\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.430450\n",
      "\tTesting Loss: 533.333059\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.449493\n",
      "\tTesting Loss: 533.347727\n",
      "\tLearning Rate: 0.000008595\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.433772\n",
      "\tTesting Loss: 533.439412\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.424273\n",
      "\tTesting Loss: 533.349650\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.449226\n",
      "\tTesting Loss: 533.346588\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.434514\n",
      "\tTesting Loss: 533.413859\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.429008\n",
      "\tTesting Loss: 533.347982\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.447873\n",
      "\tTesting Loss: 533.351532\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.432894\n",
      "\tTesting Loss: 533.398926\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.430598\n",
      "\tTesting Loss: 533.340535\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.446096\n",
      "\tTesting Loss: 533.359283\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.430893\n",
      "\tTesting Loss: 533.392049\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.432251\n",
      "\tTesting Loss: 533.340617\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.445134\n",
      "\tTesting Loss: 533.362528\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.431452\n",
      "\tTesting Loss: 533.389994\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.435659\n",
      "\tTesting Loss: 533.334839\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.443001\n",
      "\tTesting Loss: 533.364339\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.430984\n",
      "\tTesting Loss: 533.381205\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.435689\n",
      "\tTesting Loss: 533.339539\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.445353\n",
      "\tTesting Loss: 533.362651\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.435392\n",
      "\tTesting Loss: 533.375509\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.435333\n",
      "\tTesting Loss: 533.338938\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.444499\n",
      "\tTesting Loss: 533.359680\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.433126\n",
      "\tTesting Loss: 533.380147\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.435486\n",
      "\tTesting Loss: 533.336385\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.444992\n",
      "\tTesting Loss: 533.350830\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.437698\n",
      "\tTesting Loss: 533.376587\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.435946\n",
      "\tTesting Loss: 533.335815\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.443019\n",
      "\tTesting Loss: 533.351217\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.438937\n",
      "\tTesting Loss: 533.367086\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.435471\n",
      "\tTesting Loss: 533.341258\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.441620\n",
      "\tTesting Loss: 533.346252\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.439954\n",
      "\tTesting Loss: 533.360413\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.437899\n",
      "\tTesting Loss: 533.341543\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.441895\n",
      "\tTesting Loss: 533.346029\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.439697\n",
      "\tTesting Loss: 533.358419\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.437332\n",
      "\tTesting Loss: 533.344157\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.443644\n",
      "\tTesting Loss: 533.337301\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.443054\n",
      "\tTesting Loss: 533.355520\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.437887\n",
      "\tTesting Loss: 533.347229\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.446470\n",
      "\tTesting Loss: 533.330363\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.442843\n",
      "\tTesting Loss: 533.357524\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.436345\n",
      "\tTesting Loss: 533.341695\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.442734\n",
      "\tTesting Loss: 533.322398\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.446767\n",
      "\tTesting Loss: 533.344055\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.438105\n",
      "\tTesting Loss: 533.342336\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.443125\n",
      "\tTesting Loss: 533.325724\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.448133\n",
      "\tTesting Loss: 533.338450\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.437190\n",
      "\tTesting Loss: 533.347900\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.441020\n",
      "\tTesting Loss: 533.317118\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.449738\n",
      "\tTesting Loss: 533.331289\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.440580\n",
      "\tTesting Loss: 533.354645\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.438690\n",
      "\tTesting Loss: 533.315155\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.451396\n",
      "\tTesting Loss: 533.315969\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.441060\n",
      "\tTesting Loss: 533.363627\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.435969\n",
      "\tTesting Loss: 533.315969\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.452469\n",
      "\tTesting Loss: 533.297913\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.445473\n",
      "\tTesting Loss: 533.365255\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.433189\n",
      "\tTesting Loss: 533.320068\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.450216\n",
      "\tTesting Loss: 533.286397\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.449178\n",
      "\tTesting Loss: 533.357045\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.432119\n",
      "\tTesting Loss: 533.337260\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.451436\n",
      "\tTesting Loss: 533.262899\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.449402\n",
      "\tTesting Loss: 533.350210\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.432838\n",
      "\tTesting Loss: 533.347758\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.449219\n",
      "\tTesting Loss: 533.261658\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.450073\n",
      "\tTesting Loss: 533.334859\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.434896\n",
      "\tTesting Loss: 533.355367\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.441577\n",
      "\tTesting Loss: 533.266307\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.453199\n",
      "\tTesting Loss: 533.315145\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.433792\n",
      "\tTesting Loss: 533.364970\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.439326\n",
      "\tTesting Loss: 533.261434\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.454213\n",
      "\tTesting Loss: 533.298645\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.437124\n",
      "\tTesting Loss: 533.371450\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.436849\n",
      "\tTesting Loss: 533.269460\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.454130\n",
      "\tTesting Loss: 533.286214\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.440882\n",
      "\tTesting Loss: 533.367391\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.433029\n",
      "\tTesting Loss: 533.285380\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.453590\n",
      "\tTesting Loss: 533.265686\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.446548\n",
      "\tTesting Loss: 533.354146\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.429484\n",
      "\tTesting Loss: 533.305857\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.454656\n",
      "\tTesting Loss: 533.246582\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.446813\n",
      "\tTesting Loss: 533.346934\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.428363\n",
      "\tTesting Loss: 533.311951\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.450882\n",
      "\tTesting Loss: 533.246033\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.448344\n",
      "\tTesting Loss: 533.331645\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.434235\n",
      "\tTesting Loss: 533.323018\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.444644\n",
      "\tTesting Loss: 533.247528\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.454338\n",
      "\tTesting Loss: 533.304057\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.434848\n",
      "\tTesting Loss: 533.336812\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.439718\n",
      "\tTesting Loss: 533.250468\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.453496\n",
      "\tTesting Loss: 533.289927\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.438039\n",
      "\tTesting Loss: 533.338826\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.440420\n",
      "\tTesting Loss: 533.253601\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.455327\n",
      "\tTesting Loss: 533.271769\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.439743\n",
      "\tTesting Loss: 533.344950\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.434540\n",
      "\tTesting Loss: 533.261363\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.458049\n",
      "\tTesting Loss: 533.257222\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.442566\n",
      "\tTesting Loss: 533.342173\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.433708\n",
      "\tTesting Loss: 533.262573\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.458298\n",
      "\tTesting Loss: 533.245382\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.442734\n",
      "\tTesting Loss: 533.337402\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.432378\n",
      "\tTesting Loss: 533.266479\n",
      "\tLearning Rate: 0.000007736\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.452301\n",
      "\tTesting Loss: 533.239461\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.446307\n",
      "\tTesting Loss: 533.317118\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.435771\n",
      "\tTesting Loss: 533.266622\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.447055\n",
      "\tTesting Loss: 533.268555\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.443425\n",
      "\tTesting Loss: 533.301402\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.439504\n",
      "\tTesting Loss: 533.269755\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.449699\n",
      "\tTesting Loss: 533.269114\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.441635\n",
      "\tTesting Loss: 533.294352\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.442149\n",
      "\tTesting Loss: 533.269430\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.446508\n",
      "\tTesting Loss: 533.274333\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.441157\n",
      "\tTesting Loss: 533.285990\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.444967\n",
      "\tTesting Loss: 533.262919\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.447014\n",
      "\tTesting Loss: 533.277354\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.443067\n",
      "\tTesting Loss: 533.280253\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.443588\n",
      "\tTesting Loss: 533.270142\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.448151\n",
      "\tTesting Loss: 533.269236\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.445658\n",
      "\tTesting Loss: 533.276326\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.442220\n",
      "\tTesting Loss: 533.272369\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.445368\n",
      "\tTesting Loss: 533.265544\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.448336\n",
      "\tTesting Loss: 533.275798\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.443364\n",
      "\tTesting Loss: 533.273224\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.446767\n",
      "\tTesting Loss: 533.256144\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.447973\n",
      "\tTesting Loss: 533.271515\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.443970\n",
      "\tTesting Loss: 533.267965\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.445686\n",
      "\tTesting Loss: 533.261993\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.446709\n",
      "\tTesting Loss: 533.264252\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.446785\n",
      "\tTesting Loss: 533.265259\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.447787\n",
      "\tTesting Loss: 533.257385\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.447128\n",
      "\tTesting Loss: 533.266683\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.444804\n",
      "\tTesting Loss: 533.262726\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.449160\n",
      "\tTesting Loss: 533.250793\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.446032\n",
      "\tTesting Loss: 533.269531\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.445445\n",
      "\tTesting Loss: 533.254130\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.448201\n",
      "\tTesting Loss: 533.250865\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.446241\n",
      "\tTesting Loss: 533.262512\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.446782\n",
      "\tTesting Loss: 533.251139\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.448479\n",
      "\tTesting Loss: 533.251994\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.449112\n",
      "\tTesting Loss: 533.250977\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.446075\n",
      "\tTesting Loss: 533.254069\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.446447\n",
      "\tTesting Loss: 533.250010\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.451358\n",
      "\tTesting Loss: 533.245728\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.446785\n",
      "\tTesting Loss: 533.256317\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.446823\n",
      "\tTesting Loss: 533.241323\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.451416\n",
      "\tTesting Loss: 533.241252\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.444361\n",
      "\tTesting Loss: 533.257304\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.447049\n",
      "\tTesting Loss: 533.230774\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.448710\n",
      "\tTesting Loss: 533.245748\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.445908\n",
      "\tTesting Loss: 533.248556\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.449600\n",
      "\tTesting Loss: 533.231476\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.449488\n",
      "\tTesting Loss: 533.246541\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.446177\n",
      "\tTesting Loss: 533.241150\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.451411\n",
      "\tTesting Loss: 533.222890\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.448448\n",
      "\tTesting Loss: 533.251272\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.444997\n",
      "\tTesting Loss: 533.238576\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.446513\n",
      "\tTesting Loss: 533.231740\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.448664\n",
      "\tTesting Loss: 533.232422\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.451385\n",
      "\tTesting Loss: 533.230662\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.447225\n",
      "\tTesting Loss: 533.242493\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.446910\n",
      "\tTesting Loss: 533.228485\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.452532\n",
      "\tTesting Loss: 533.220866\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.447667\n",
      "\tTesting Loss: 533.242533\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.446604\n",
      "\tTesting Loss: 533.226807\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.451660\n",
      "\tTesting Loss: 533.218791\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.449188\n",
      "\tTesting Loss: 533.240662\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.448181\n",
      "\tTesting Loss: 533.220235\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.451342\n",
      "\tTesting Loss: 533.220764\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.450785\n",
      "\tTesting Loss: 533.234151\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.447591\n",
      "\tTesting Loss: 533.224497\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.449315\n",
      "\tTesting Loss: 533.218913\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.448736\n",
      "\tTesting Loss: 533.226339\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.450483\n",
      "\tTesting Loss: 533.215200\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.450099\n",
      "\tTesting Loss: 533.225677\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.449412\n",
      "\tTesting Loss: 533.222595\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.453323\n",
      "\tTesting Loss: 533.203959\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.450643\n",
      "\tTesting Loss: 533.223033\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.450144\n",
      "\tTesting Loss: 533.221273\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.449570\n",
      "\tTesting Loss: 533.213155\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.450745\n",
      "\tTesting Loss: 533.214915\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.449549\n",
      "\tTesting Loss: 533.215932\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.450175\n",
      "\tTesting Loss: 533.214254\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.452601\n",
      "\tTesting Loss: 533.208923\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.453562\n",
      "\tTesting Loss: 533.209635\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.452367\n",
      "\tTesting Loss: 533.218089\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.448140\n",
      "\tTesting Loss: 533.219543\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.454173\n",
      "\tTesting Loss: 533.194356\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.451401\n",
      "\tTesting Loss: 533.215251\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.447876\n",
      "\tTesting Loss: 533.209859\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.454173\n",
      "\tTesting Loss: 533.187948\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.452052\n",
      "\tTesting Loss: 533.215535\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.451482\n",
      "\tTesting Loss: 533.207642\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.451487\n",
      "\tTesting Loss: 533.192434\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.452830\n",
      "\tTesting Loss: 533.201965\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.454219\n",
      "\tTesting Loss: 533.201233\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.448573\n",
      "\tTesting Loss: 533.202596\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.451627\n",
      "\tTesting Loss: 533.194865\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.453613\n",
      "\tTesting Loss: 533.199585\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.451681\n",
      "\tTesting Loss: 533.196493\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.456294\n",
      "\tTesting Loss: 533.185771\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.452387\n",
      "\tTesting Loss: 533.211263\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.450007\n",
      "\tTesting Loss: 533.200338\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.455332\n",
      "\tTesting Loss: 533.175171\n",
      "\tLearning Rate: 0.000006962\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.454740\n",
      "\tTesting Loss: 533.195689\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.451886\n",
      "\tTesting Loss: 533.197052\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.450625\n",
      "\tTesting Loss: 533.189901\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.453918\n",
      "\tTesting Loss: 533.187388\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.455114\n",
      "\tTesting Loss: 533.191040\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.453705\n",
      "\tTesting Loss: 533.190369\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.453829\n",
      "\tTesting Loss: 533.188517\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.452911\n",
      "\tTesting Loss: 533.192647\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.455488\n",
      "\tTesting Loss: 533.181040\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.454516\n",
      "\tTesting Loss: 533.184245\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.451594\n",
      "\tTesting Loss: 533.194132\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.455663\n",
      "\tTesting Loss: 533.174764\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.453807\n",
      "\tTesting Loss: 533.188314\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.450661\n",
      "\tTesting Loss: 533.185527\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.456482\n",
      "\tTesting Loss: 533.169281\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.454442\n",
      "\tTesting Loss: 533.189006\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.454585\n",
      "\tTesting Loss: 533.184570\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.454412\n",
      "\tTesting Loss: 533.173848\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.454468\n",
      "\tTesting Loss: 533.177327\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.456309\n",
      "\tTesting Loss: 533.175964\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.453186\n",
      "\tTesting Loss: 533.181132\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.452853\n",
      "\tTesting Loss: 533.171895\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.458295\n",
      "\tTesting Loss: 533.169464\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.451930\n",
      "\tTesting Loss: 533.177226\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.456769\n",
      "\tTesting Loss: 533.159810\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.454239\n",
      "\tTesting Loss: 533.172292\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.454178\n",
      "\tTesting Loss: 533.178894\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.452876\n",
      "\tTesting Loss: 533.163188\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.457136\n",
      "\tTesting Loss: 533.164286\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.455068\n",
      "\tTesting Loss: 533.171316\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.456212\n",
      "\tTesting Loss: 533.169444\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.456762\n",
      "\tTesting Loss: 533.162465\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.457451\n",
      "\tTesting Loss: 533.168579\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.455132\n",
      "\tTesting Loss: 533.168070\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.454361\n",
      "\tTesting Loss: 533.161987\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.457794\n",
      "\tTesting Loss: 533.160095\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.455510\n",
      "\tTesting Loss: 533.167633\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.455152\n",
      "\tTesting Loss: 533.164266\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.456373\n",
      "\tTesting Loss: 533.157288\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.457227\n",
      "\tTesting Loss: 533.158590\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.458267\n",
      "\tTesting Loss: 533.160980\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.456457\n",
      "\tTesting Loss: 533.164307\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.454954\n",
      "\tTesting Loss: 533.160441\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.457052\n",
      "\tTesting Loss: 533.154175\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.458196\n",
      "\tTesting Loss: 533.150838\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.457138\n",
      "\tTesting Loss: 533.154724\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.455447\n",
      "\tTesting Loss: 533.162069\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.454470\n",
      "\tTesting Loss: 533.154419\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.460866\n",
      "\tTesting Loss: 533.140188\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.458654\n",
      "\tTesting Loss: 533.154582\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.457347\n",
      "\tTesting Loss: 533.154500\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.455739\n",
      "\tTesting Loss: 533.153351\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.457184\n",
      "\tTesting Loss: 533.145040\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.460152\n",
      "\tTesting Loss: 533.139313\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.457652\n",
      "\tTesting Loss: 533.148092\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.456172\n",
      "\tTesting Loss: 533.151225\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.455910\n",
      "\tTesting Loss: 533.140849\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.460948\n",
      "\tTesting Loss: 533.132996\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.456724\n",
      "\tTesting Loss: 533.148458\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.456797\n",
      "\tTesting Loss: 533.143290\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.459272\n",
      "\tTesting Loss: 533.136800\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.459737\n",
      "\tTesting Loss: 533.141541\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.458659\n",
      "\tTesting Loss: 533.139465\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.459991\n",
      "\tTesting Loss: 533.138184\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.457952\n",
      "\tTesting Loss: 533.144775\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.458033\n",
      "\tTesting Loss: 533.135824\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.457677\n",
      "\tTesting Loss: 533.130707\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.461820\n",
      "\tTesting Loss: 533.128194\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.456034\n",
      "\tTesting Loss: 533.145925\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.456688\n",
      "\tTesting Loss: 533.130066\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.459956\n",
      "\tTesting Loss: 533.123250\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.460386\n",
      "\tTesting Loss: 533.132060\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.458394\n",
      "\tTesting Loss: 533.133708\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.459473\n",
      "\tTesting Loss: 533.130483\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.459831\n",
      "\tTesting Loss: 533.128062\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.460439\n",
      "\tTesting Loss: 533.125081\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.459127\n",
      "\tTesting Loss: 533.130391\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.459826\n",
      "\tTesting Loss: 533.129079\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.458155\n",
      "\tTesting Loss: 533.131541\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.458242\n",
      "\tTesting Loss: 533.121806\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.462298\n",
      "\tTesting Loss: 533.115682\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.458588\n",
      "\tTesting Loss: 533.132355\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.456909\n",
      "\tTesting Loss: 533.126088\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.461853\n",
      "\tTesting Loss: 533.111735\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.459712\n",
      "\tTesting Loss: 533.125122\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.459356\n",
      "\tTesting Loss: 533.120158\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.462285\n",
      "\tTesting Loss: 533.112020\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.457530\n",
      "\tTesting Loss: 533.122986\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.461400\n",
      "\tTesting Loss: 533.111796\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.456457\n",
      "\tTesting Loss: 533.118123\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.460815\n",
      "\tTesting Loss: 533.109965\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.457308\n",
      "\tTesting Loss: 533.109436\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.460098\n",
      "\tTesting Loss: 533.115072\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.456492\n",
      "\tTesting Loss: 533.117716\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.460307\n",
      "\tTesting Loss: 533.108276\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.458486\n",
      "\tTesting Loss: 533.112793\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.460233\n",
      "\tTesting Loss: 533.111003\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.459325\n",
      "\tTesting Loss: 533.111450\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.460579\n",
      "\tTesting Loss: 533.108480\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.461253\n",
      "\tTesting Loss: 533.107869\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.459417\n",
      "\tTesting Loss: 533.113607\n",
      "\tLearning Rate: 0.000006266\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.456983\n",
      "\tTesting Loss: 533.111949\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.462748\n",
      "\tTesting Loss: 533.096914\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.458789\n",
      "\tTesting Loss: 533.109456\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.459676\n",
      "\tTesting Loss: 533.103861\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.458911\n",
      "\tTesting Loss: 533.107290\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.461014\n",
      "\tTesting Loss: 533.104675\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.459503\n",
      "\tTesting Loss: 533.106313\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.457820\n",
      "\tTesting Loss: 533.104675\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.461487\n",
      "\tTesting Loss: 533.101135\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.458862\n",
      "\tTesting Loss: 533.104757\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.458593\n",
      "\tTesting Loss: 533.102478\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.460032\n",
      "\tTesting Loss: 533.097148\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.458084\n",
      "\tTesting Loss: 533.102570\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.461309\n",
      "\tTesting Loss: 533.097249\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.458583\n",
      "\tTesting Loss: 533.104329\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.460993\n",
      "\tTesting Loss: 533.097107\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.459902\n",
      "\tTesting Loss: 533.095520\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.461957\n",
      "\tTesting Loss: 533.097687\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.459488\n",
      "\tTesting Loss: 533.098877\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.458875\n",
      "\tTesting Loss: 533.097168\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.461672\n",
      "\tTesting Loss: 533.089844\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.459040\n",
      "\tTesting Loss: 533.098287\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.459546\n",
      "\tTesting Loss: 533.095449\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.458888\n",
      "\tTesting Loss: 533.092194\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.460892\n",
      "\tTesting Loss: 533.089620\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.461166\n",
      "\tTesting Loss: 533.090007\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.459808\n",
      "\tTesting Loss: 533.093282\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.457659\n",
      "\tTesting Loss: 533.095256\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.460917\n",
      "\tTesting Loss: 533.083191\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.459707\n",
      "\tTesting Loss: 533.087789\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.459760\n",
      "\tTesting Loss: 533.091461\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.459129\n",
      "\tTesting Loss: 533.086304\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.458967\n",
      "\tTesting Loss: 533.087016\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.461629\n",
      "\tTesting Loss: 533.079081\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.459211\n",
      "\tTesting Loss: 533.088888\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.457143\n",
      "\tTesting Loss: 533.085571\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.461487\n",
      "\tTesting Loss: 533.072133\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.459984\n",
      "\tTesting Loss: 533.086161\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.460785\n",
      "\tTesting Loss: 533.082682\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.461212\n",
      "\tTesting Loss: 533.078379\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.460292\n",
      "\tTesting Loss: 533.085144\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.459389\n",
      "\tTesting Loss: 533.081950\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.458117\n",
      "\tTesting Loss: 533.073934\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.463369\n",
      "\tTesting Loss: 533.071503\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.458333\n",
      "\tTesting Loss: 533.089071\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.458191\n",
      "\tTesting Loss: 533.070343\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.462530\n",
      "\tTesting Loss: 533.068217\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.457288\n",
      "\tTesting Loss: 533.083761\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.460716\n",
      "\tTesting Loss: 533.071320\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.458509\n",
      "\tTesting Loss: 533.070374\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.461197\n",
      "\tTesting Loss: 533.072449\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.456912\n",
      "\tTesting Loss: 533.074198\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.461276\n",
      "\tTesting Loss: 533.067780\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.459007\n",
      "\tTesting Loss: 533.076284\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.461405\n",
      "\tTesting Loss: 533.069865\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.458054\n",
      "\tTesting Loss: 533.074076\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.461095\n",
      "\tTesting Loss: 533.065999\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.461563\n",
      "\tTesting Loss: 533.066915\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.460663\n",
      "\tTesting Loss: 533.072327\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.459930\n",
      "\tTesting Loss: 533.071655\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.459518\n",
      "\tTesting Loss: 533.074880\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.458150\n",
      "\tTesting Loss: 533.071411\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.462570\n",
      "\tTesting Loss: 533.058634\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.460477\n",
      "\tTesting Loss: 533.069061\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.458165\n",
      "\tTesting Loss: 533.069845\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.461309\n",
      "\tTesting Loss: 533.060822\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.458659\n",
      "\tTesting Loss: 533.069600\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.460197\n",
      "\tTesting Loss: 533.062571\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.459501\n",
      "\tTesting Loss: 533.063395\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.459450\n",
      "\tTesting Loss: 533.061503\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.461098\n",
      "\tTesting Loss: 533.060120\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.462646\n",
      "\tTesting Loss: 533.060669\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.459620\n",
      "\tTesting Loss: 533.065440\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.458923\n",
      "\tTesting Loss: 533.062948\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.461769\n",
      "\tTesting Loss: 533.055094\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.460261\n",
      "\tTesting Loss: 533.061391\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.460828\n",
      "\tTesting Loss: 533.060811\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.460225\n",
      "\tTesting Loss: 533.059377\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.459641\n",
      "\tTesting Loss: 533.059306\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.460510\n",
      "\tTesting Loss: 533.060994\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.459345\n",
      "\tTesting Loss: 533.053192\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.462781\n",
      "\tTesting Loss: 533.046743\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.459460\n",
      "\tTesting Loss: 533.062195\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.457352\n",
      "\tTesting Loss: 533.056112\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.461919\n",
      "\tTesting Loss: 533.044423\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.458460\n",
      "\tTesting Loss: 533.058797\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.460276\n",
      "\tTesting Loss: 533.053538\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.457921\n",
      "\tTesting Loss: 533.054647\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.460882\n",
      "\tTesting Loss: 533.045013\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.460108\n",
      "\tTesting Loss: 533.048299\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.459325\n",
      "\tTesting Loss: 533.051524\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.461746\n",
      "\tTesting Loss: 533.053375\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.458076\n",
      "\tTesting Loss: 533.060689\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.459386\n",
      "\tTesting Loss: 533.043772\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.464137\n",
      "\tTesting Loss: 533.038920\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.458817\n",
      "\tTesting Loss: 533.060557\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.456746\n",
      "\tTesting Loss: 533.049489\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.463257\n",
      "\tTesting Loss: 533.030233\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.460653\n",
      "\tTesting Loss: 533.053589\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.458237\n",
      "\tTesting Loss: 533.050059\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.458125\n",
      "\tTesting Loss: 533.042419\n",
      "\tLearning Rate: 0.000005639\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.459707\n",
      "\tTesting Loss: 533.043030\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.461085\n",
      "\tTesting Loss: 533.046977\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.457759\n",
      "\tTesting Loss: 533.045959\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.458196\n",
      "\tTesting Loss: 533.040985\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.460993\n",
      "\tTesting Loss: 533.042196\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.457560\n",
      "\tTesting Loss: 533.043294\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.460622\n",
      "\tTesting Loss: 533.038127\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.457179\n",
      "\tTesting Loss: 533.043457\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.460131\n",
      "\tTesting Loss: 533.037577\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.456991\n",
      "\tTesting Loss: 533.043376\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.460490\n",
      "\tTesting Loss: 533.035014\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.457062\n",
      "\tTesting Loss: 533.043162\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.458888\n",
      "\tTesting Loss: 533.038015\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.456856\n",
      "\tTesting Loss: 533.039734\n",
      "\tLearning Rate: 0.000005075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.459493\n",
      "\tTesting Loss: 533.034098\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.457321\n",
      "\tTesting Loss: 533.034281\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.459005\n",
      "\tTesting Loss: 533.041616\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.456866\n",
      "\tTesting Loss: 533.036662\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.460841\n",
      "\tTesting Loss: 533.030721\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.458076\n",
      "\tTesting Loss: 533.035034\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.459658\n",
      "\tTesting Loss: 533.035950\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.457637\n",
      "\tTesting Loss: 533.028280\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.460182\n",
      "\tTesting Loss: 533.035655\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.457888\n",
      "\tTesting Loss: 533.030701\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.459338\n",
      "\tTesting Loss: 533.034027\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.456457\n",
      "\tTesting Loss: 533.031952\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.458806\n",
      "\tTesting Loss: 533.031270\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.457972\n",
      "\tTesting Loss: 533.024516\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.457993\n",
      "\tTesting Loss: 533.038106\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.457113\n",
      "\tTesting Loss: 533.027476\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.459000\n",
      "\tTesting Loss: 533.033091\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.457603\n",
      "\tTesting Loss: 533.030334\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.458394\n",
      "\tTesting Loss: 533.030253\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.457301\n",
      "\tTesting Loss: 533.030853\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.458199\n",
      "\tTesting Loss: 533.032471\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.457975\n",
      "\tTesting Loss: 533.028442\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.458125\n",
      "\tTesting Loss: 533.032593\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.458196\n",
      "\tTesting Loss: 533.030752\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.458415\n",
      "\tTesting Loss: 533.030233\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.458801\n",
      "\tTesting Loss: 533.027547\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.458244\n",
      "\tTesting Loss: 533.027100\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.456507\n",
      "\tTesting Loss: 533.032674\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.458992\n",
      "\tTesting Loss: 533.023071\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.456838\n",
      "\tTesting Loss: 533.030802\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.459223\n",
      "\tTesting Loss: 533.026306\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.455856\n",
      "\tTesting Loss: 533.028249\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.459941\n",
      "\tTesting Loss: 533.020732\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.456523\n",
      "\tTesting Loss: 533.029704\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.459371\n",
      "\tTesting Loss: 533.024801\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.454702\n",
      "\tTesting Loss: 533.026672\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.459813\n",
      "\tTesting Loss: 533.014923\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.456047\n",
      "\tTesting Loss: 533.030202\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.458110\n",
      "\tTesting Loss: 533.023926\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.454656\n",
      "\tTesting Loss: 533.016347\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.459330\n",
      "\tTesting Loss: 533.019582\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.455811\n",
      "\tTesting Loss: 533.024394\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.456457\n",
      "\tTesting Loss: 533.019938\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.457179\n",
      "\tTesting Loss: 533.012492\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.458641\n",
      "\tTesting Loss: 533.025604\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.457214\n",
      "\tTesting Loss: 533.012105\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.458847\n",
      "\tTesting Loss: 533.020426\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.456685\n",
      "\tTesting Loss: 533.020030\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.458984\n",
      "\tTesting Loss: 533.016093\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.456777\n",
      "\tTesting Loss: 533.016683\n",
      "\tLearning Rate: 0.000005075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.457845\n",
      "\tTesting Loss: 533.016825\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.456863\n",
      "\tTesting Loss: 533.017395\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.458605\n",
      "\tTesting Loss: 533.017273\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.457245\n",
      "\tTesting Loss: 533.012166\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.458377\n",
      "\tTesting Loss: 533.020569\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.455579\n",
      "\tTesting Loss: 533.018860\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.459183\n",
      "\tTesting Loss: 533.010071\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.457453\n",
      "\tTesting Loss: 533.017731\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.457047\n",
      "\tTesting Loss: 533.021271\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.455770\n",
      "\tTesting Loss: 533.017354\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.459925\n",
      "\tTesting Loss: 533.008779\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.457296\n",
      "\tTesting Loss: 533.023153\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.456853\n",
      "\tTesting Loss: 533.014282\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.459351\n",
      "\tTesting Loss: 533.009928\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.456690\n",
      "\tTesting Loss: 533.021851\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.458018\n",
      "\tTesting Loss: 533.009399\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.458806\n",
      "\tTesting Loss: 533.014730\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.457133\n",
      "\tTesting Loss: 533.018616\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.457845\n",
      "\tTesting Loss: 533.010183\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.458954\n",
      "\tTesting Loss: 533.014669\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.456599\n",
      "\tTesting Loss: 533.017995\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.456111\n",
      "\tTesting Loss: 533.009521\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.460464\n",
      "\tTesting Loss: 533.002635\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.456823\n",
      "\tTesting Loss: 533.016734\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.459221\n",
      "\tTesting Loss: 533.005178\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.457133\n",
      "\tTesting Loss: 533.009440\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.459020\n",
      "\tTesting Loss: 533.005890\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.457077\n",
      "\tTesting Loss: 533.012807\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.458738\n",
      "\tTesting Loss: 533.005432\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.457250\n",
      "\tTesting Loss: 533.014119\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.455991\n",
      "\tTesting Loss: 533.009572\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.459803\n",
      "\tTesting Loss: 533.000916\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.456243\n",
      "\tTesting Loss: 533.011515\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.458247\n",
      "\tTesting Loss: 533.001597\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.456584\n",
      "\tTesting Loss: 533.006887\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.458059\n",
      "\tTesting Loss: 533.008158\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.455836\n",
      "\tTesting Loss: 533.005269\n",
      "\tLearning Rate: 0.000005075\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.458562\n",
      "\tTesting Loss: 532.999247\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.457977\n",
      "\tTesting Loss: 533.009298\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.457169\n",
      "\tTesting Loss: 533.005025\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.459056\n",
      "\tTesting Loss: 533.001526\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.457746\n",
      "\tTesting Loss: 533.010162\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.457728\n",
      "\tTesting Loss: 533.006551\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.457204\n",
      "\tTesting Loss: 533.007731\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.456670\n",
      "\tTesting Loss: 533.003794\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.459183\n",
      "\tTesting Loss: 533.001424\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.457255\n",
      "\tTesting Loss: 533.009054\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.458155\n",
      "\tTesting Loss: 533.000407\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.457743\n",
      "\tTesting Loss: 533.007243\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.455945\n",
      "\tTesting Loss: 533.007131\n",
      "\tLearning Rate: 0.000004568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.459300\n",
      "\tTesting Loss: 532.994720\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.457779\n",
      "\tTesting Loss: 533.005086\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.457433\n",
      "\tTesting Loss: 533.005931\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.456848\n",
      "\tTesting Loss: 532.999603\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.458702\n",
      "\tTesting Loss: 533.002441\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.456924\n",
      "\tTesting Loss: 533.005483\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.457342\n",
      "\tTesting Loss: 533.000509\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.458560\n",
      "\tTesting Loss: 533.001790\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.458163\n",
      "\tTesting Loss: 533.000692\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.457403\n",
      "\tTesting Loss: 533.002452\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.457540\n",
      "\tTesting Loss: 532.998332\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.458344\n",
      "\tTesting Loss: 533.001363\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.458038\n",
      "\tTesting Loss: 533.001872\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.458745\n",
      "\tTesting Loss: 532.998413\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.457703\n",
      "\tTesting Loss: 533.003337\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.456609\n",
      "\tTesting Loss: 533.000580\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.460292\n",
      "\tTesting Loss: 532.992157\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.456182\n",
      "\tTesting Loss: 533.000041\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.458481\n",
      "\tTesting Loss: 532.993306\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.456401\n",
      "\tTesting Loss: 532.999268\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.457970\n",
      "\tTesting Loss: 532.996175\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.457342\n",
      "\tTesting Loss: 532.991302\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.458028\n",
      "\tTesting Loss: 532.997172\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.457062\n",
      "\tTesting Loss: 532.990702\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.457504\n",
      "\tTesting Loss: 532.997152\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.457830\n",
      "\tTesting Loss: 532.992574\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.458059\n",
      "\tTesting Loss: 532.996419\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.458328\n",
      "\tTesting Loss: 532.990845\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.457525\n",
      "\tTesting Loss: 532.995321\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.457469\n",
      "\tTesting Loss: 532.996053\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.458435\n",
      "\tTesting Loss: 532.993612\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.458282\n",
      "\tTesting Loss: 532.995321\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.459101\n",
      "\tTesting Loss: 532.993388\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.457535\n",
      "\tTesting Loss: 532.997101\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.459025\n",
      "\tTesting Loss: 532.992126\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.459106\n",
      "\tTesting Loss: 532.995158\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.457062\n",
      "\tTesting Loss: 532.996908\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.458333\n",
      "\tTesting Loss: 532.987386\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.459264\n",
      "\tTesting Loss: 532.992371\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.457616\n",
      "\tTesting Loss: 532.993937\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.457530\n",
      "\tTesting Loss: 532.993042\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.459056\n",
      "\tTesting Loss: 532.989522\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.458997\n",
      "\tTesting Loss: 532.989400\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.457759\n",
      "\tTesting Loss: 532.998281\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.458583\n",
      "\tTesting Loss: 532.989797\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.458384\n",
      "\tTesting Loss: 532.994425\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.457601\n",
      "\tTesting Loss: 532.989156\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.460314\n",
      "\tTesting Loss: 532.982422\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.459417\n",
      "\tTesting Loss: 532.994853\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.456894\n",
      "\tTesting Loss: 532.993978\n",
      "\tLearning Rate: 0.000004568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.459691\n",
      "\tTesting Loss: 532.980896\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.459391\n",
      "\tTesting Loss: 532.993164\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.457583\n",
      "\tTesting Loss: 532.990580\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.459727\n",
      "\tTesting Loss: 532.982574\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.458934\n",
      "\tTesting Loss: 532.991150\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.457575\n",
      "\tTesting Loss: 532.989685\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.461049\n",
      "\tTesting Loss: 532.976522\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.458214\n",
      "\tTesting Loss: 532.989950\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.459356\n",
      "\tTesting Loss: 532.985270\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.459396\n",
      "\tTesting Loss: 532.981354\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.459618\n",
      "\tTesting Loss: 532.988607\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.459305\n",
      "\tTesting Loss: 532.985891\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.458613\n",
      "\tTesting Loss: 532.987895\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.458290\n",
      "\tTesting Loss: 532.982931\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.461286\n",
      "\tTesting Loss: 532.977346\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.458379\n",
      "\tTesting Loss: 532.989878\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.460307\n",
      "\tTesting Loss: 532.978780\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.458204\n",
      "\tTesting Loss: 532.981364\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.461182\n",
      "\tTesting Loss: 532.981059\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.457179\n",
      "\tTesting Loss: 532.980509\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.460765\n",
      "\tTesting Loss: 532.974864\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.457926\n",
      "\tTesting Loss: 532.983195\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.458811\n",
      "\tTesting Loss: 532.977437\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.459457\n",
      "\tTesting Loss: 532.976522\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.458674\n",
      "\tTesting Loss: 532.982625\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.459971\n",
      "\tTesting Loss: 532.972717\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.459277\n",
      "\tTesting Loss: 532.983114\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.459605\n",
      "\tTesting Loss: 532.976359\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.459946\n",
      "\tTesting Loss: 532.980357\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.460462\n",
      "\tTesting Loss: 532.975810\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.460261\n",
      "\tTesting Loss: 532.981313\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.459625\n",
      "\tTesting Loss: 532.982707\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.460772\n",
      "\tTesting Loss: 532.975321\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.461116\n",
      "\tTesting Loss: 532.980611\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.459813\n",
      "\tTesting Loss: 532.980408\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.461146\n",
      "\tTesting Loss: 532.974243\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.461238\n",
      "\tTesting Loss: 532.978343\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.461136\n",
      "\tTesting Loss: 532.977732\n",
      "\tLearning Rate: 0.000004568\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.460688\n",
      "\tTesting Loss: 532.977142\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.460627\n",
      "\tTesting Loss: 532.978099\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.461924\n",
      "\tTesting Loss: 532.973724\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.461929\n",
      "\tTesting Loss: 532.976959\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.461372\n",
      "\tTesting Loss: 532.978587\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.461695\n",
      "\tTesting Loss: 532.977437\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.461377\n",
      "\tTesting Loss: 532.974182\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.462606\n",
      "\tTesting Loss: 532.973531\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.461383\n",
      "\tTesting Loss: 532.978170\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.460648\n",
      "\tTesting Loss: 532.976278\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.464111\n",
      "\tTesting Loss: 532.970500\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.460406\n",
      "\tTesting Loss: 532.974508\n",
      "\tLearning Rate: 0.000004111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.463521\n",
      "\tTesting Loss: 532.970276\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.459595\n",
      "\tTesting Loss: 532.971832\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.462682\n",
      "\tTesting Loss: 532.967356\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.462140\n",
      "\tTesting Loss: 532.972514\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.461149\n",
      "\tTesting Loss: 532.971191\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.462873\n",
      "\tTesting Loss: 532.963562\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.461589\n",
      "\tTesting Loss: 532.974080\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.462596\n",
      "\tTesting Loss: 532.967204\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.462064\n",
      "\tTesting Loss: 532.971029\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.463109\n",
      "\tTesting Loss: 532.966410\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.462825\n",
      "\tTesting Loss: 532.972493\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.461296\n",
      "\tTesting Loss: 532.970886\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.464264\n",
      "\tTesting Loss: 532.968811\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.462196\n",
      "\tTesting Loss: 532.972900\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.463964\n",
      "\tTesting Loss: 532.966024\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.463053\n",
      "\tTesting Loss: 532.971578\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.463786\n",
      "\tTesting Loss: 532.968587\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.462852\n",
      "\tTesting Loss: 532.968709\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.463455\n",
      "\tTesting Loss: 532.968058\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.464310\n",
      "\tTesting Loss: 532.966756\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.462855\n",
      "\tTesting Loss: 532.966593\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.464116\n",
      "\tTesting Loss: 532.968282\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.462199\n",
      "\tTesting Loss: 532.969645\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.465630\n",
      "\tTesting Loss: 532.963114\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.462433\n",
      "\tTesting Loss: 532.968892\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.464968\n",
      "\tTesting Loss: 532.965047\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.463615\n",
      "\tTesting Loss: 532.970856\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.465307\n",
      "\tTesting Loss: 532.961446\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.463819\n",
      "\tTesting Loss: 532.971232\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.463399\n",
      "\tTesting Loss: 532.965607\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.465332\n",
      "\tTesting Loss: 532.962545\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.461721\n",
      "\tTesting Loss: 532.966960\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.464854\n",
      "\tTesting Loss: 532.957987\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.463155\n",
      "\tTesting Loss: 532.967326\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.462886\n",
      "\tTesting Loss: 532.961222\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.463735\n",
      "\tTesting Loss: 532.958811\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.463455\n",
      "\tTesting Loss: 532.966471\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.463857\n",
      "\tTesting Loss: 532.957865\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.463796\n",
      "\tTesting Loss: 532.963704\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.464587\n",
      "\tTesting Loss: 532.963013\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.464066\n",
      "\tTesting Loss: 532.963959\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.463326\n",
      "\tTesting Loss: 532.966705\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.464406\n",
      "\tTesting Loss: 532.962301\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.465027\n",
      "\tTesting Loss: 532.963521\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.464915\n",
      "\tTesting Loss: 532.965474\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.464658\n",
      "\tTesting Loss: 532.963908\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.464045\n",
      "\tTesting Loss: 532.963867\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.463959\n",
      "\tTesting Loss: 532.963643\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.465693\n",
      "\tTesting Loss: 532.960266\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.464144\n",
      "\tTesting Loss: 532.966400\n",
      "\tLearning Rate: 0.000004111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.464340\n",
      "\tTesting Loss: 532.963542\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.465329\n",
      "\tTesting Loss: 532.963094\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.464462\n",
      "\tTesting Loss: 532.962423\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.463720\n",
      "\tTesting Loss: 532.961202\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.465347\n",
      "\tTesting Loss: 532.956319\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.463425\n",
      "\tTesting Loss: 532.962840\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.465897\n",
      "\tTesting Loss: 532.955444\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.462708\n",
      "\tTesting Loss: 532.960378\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.464864\n",
      "\tTesting Loss: 532.954183\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.463033\n",
      "\tTesting Loss: 532.959635\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.464045\n",
      "\tTesting Loss: 532.958506\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.464940\n",
      "\tTesting Loss: 532.954529\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.463249\n",
      "\tTesting Loss: 532.964071\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.465111\n",
      "\tTesting Loss: 532.952891\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.464045\n",
      "\tTesting Loss: 532.964498\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.465134\n",
      "\tTesting Loss: 532.955404\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.465111\n",
      "\tTesting Loss: 532.960571\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.463654\n",
      "\tTesting Loss: 532.965515\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.465322\n",
      "\tTesting Loss: 532.951172\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.465169\n",
      "\tTesting Loss: 532.958974\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.462896\n",
      "\tTesting Loss: 532.960510\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.465012\n",
      "\tTesting Loss: 532.954834\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.464350\n",
      "\tTesting Loss: 532.960358\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.465207\n",
      "\tTesting Loss: 532.958577\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.464218\n",
      "\tTesting Loss: 532.957550\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.466128\n",
      "\tTesting Loss: 532.956401\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.463460\n",
      "\tTesting Loss: 532.961100\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.465627\n",
      "\tTesting Loss: 532.956024\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.463552\n",
      "\tTesting Loss: 532.961924\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.465586\n",
      "\tTesting Loss: 532.954356\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.464783\n",
      "\tTesting Loss: 532.959005\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.464643\n",
      "\tTesting Loss: 532.958781\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.463593\n",
      "\tTesting Loss: 532.960490\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.465640\n",
      "\tTesting Loss: 532.956360\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.465368\n",
      "\tTesting Loss: 532.956055\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.465393\n",
      "\tTesting Loss: 532.958639\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.464254\n",
      "\tTesting Loss: 532.960551\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.463989\n",
      "\tTesting Loss: 532.956930\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.465749\n",
      "\tTesting Loss: 532.954193\n",
      "\tLearning Rate: 0.000004111\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.463593\n",
      "\tTesting Loss: 532.958089\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.465571\n",
      "\tTesting Loss: 532.955943\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.463760\n",
      "\tTesting Loss: 532.954254\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.465431\n",
      "\tTesting Loss: 532.957784\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.463842\n",
      "\tTesting Loss: 532.954386\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.465736\n",
      "\tTesting Loss: 532.958435\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.464022\n",
      "\tTesting Loss: 532.952454\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.464116\n",
      "\tTesting Loss: 532.956685\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.464900\n",
      "\tTesting Loss: 532.955017\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.464910\n",
      "\tTesting Loss: 532.956909\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.465754\n",
      "\tTesting Loss: 532.953837\n",
      "\tLearning Rate: 0.000003700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.464269\n",
      "\tTesting Loss: 532.960744\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.465220\n",
      "\tTesting Loss: 532.953898\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.466034\n",
      "\tTesting Loss: 532.960347\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.464638\n",
      "\tTesting Loss: 532.956848\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.465897\n",
      "\tTesting Loss: 532.957011\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.465243\n",
      "\tTesting Loss: 532.960022\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.465683\n",
      "\tTesting Loss: 532.956156\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.466827\n",
      "\tTesting Loss: 532.958191\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.464396\n",
      "\tTesting Loss: 532.961812\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.465617\n",
      "\tTesting Loss: 532.955160\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.466703\n",
      "\tTesting Loss: 532.958069\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.464539\n",
      "\tTesting Loss: 532.959198\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.467369\n",
      "\tTesting Loss: 532.952820\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.464650\n",
      "\tTesting Loss: 532.962779\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.465057\n",
      "\tTesting Loss: 532.949453\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.466487\n",
      "\tTesting Loss: 532.956838\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.463760\n",
      "\tTesting Loss: 532.957967\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.466558\n",
      "\tTesting Loss: 532.951925\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.465876\n",
      "\tTesting Loss: 532.958679\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.465724\n",
      "\tTesting Loss: 532.954407\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.465998\n",
      "\tTesting Loss: 532.959819\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.465897\n",
      "\tTesting Loss: 532.956075\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.465810\n",
      "\tTesting Loss: 532.955709\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.465149\n",
      "\tTesting Loss: 532.957865\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.466141\n",
      "\tTesting Loss: 532.954234\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.466385\n",
      "\tTesting Loss: 532.955200\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.465408\n",
      "\tTesting Loss: 532.960693\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.465073\n",
      "\tTesting Loss: 532.957489\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.465281\n",
      "\tTesting Loss: 532.948985\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.467697\n",
      "\tTesting Loss: 532.957662\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.465347\n",
      "\tTesting Loss: 532.956543\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.464574\n",
      "\tTesting Loss: 532.957377\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.465660\n",
      "\tTesting Loss: 532.951681\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.467555\n",
      "\tTesting Loss: 532.955505\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.463234\n",
      "\tTesting Loss: 532.956065\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.467443\n",
      "\tTesting Loss: 532.947144\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.466278\n",
      "\tTesting Loss: 532.958455\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.463806\n",
      "\tTesting Loss: 532.954956\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.467209\n",
      "\tTesting Loss: 532.952026\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.465876\n",
      "\tTesting Loss: 532.957570\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.466273\n",
      "\tTesting Loss: 532.951396\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.466380\n",
      "\tTesting Loss: 532.958049\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.466441\n",
      "\tTesting Loss: 532.953308\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.465652\n",
      "\tTesting Loss: 532.955811\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.465719\n",
      "\tTesting Loss: 532.954610\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.468374\n",
      "\tTesting Loss: 532.948771\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.464040\n",
      "\tTesting Loss: 532.955526\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.467967\n",
      "\tTesting Loss: 532.949117\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.465805\n",
      "\tTesting Loss: 532.955902\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.467252\n",
      "\tTesting Loss: 532.950378\n",
      "\tLearning Rate: 0.000003700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.466293\n",
      "\tTesting Loss: 532.956828\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.467026\n",
      "\tTesting Loss: 532.952576\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.466660\n",
      "\tTesting Loss: 532.954712\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.467298\n",
      "\tTesting Loss: 532.954936\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.467636\n",
      "\tTesting Loss: 532.949788\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.465487\n",
      "\tTesting Loss: 532.955566\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.466522\n",
      "\tTesting Loss: 532.951569\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.468086\n",
      "\tTesting Loss: 532.949422\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.464706\n",
      "\tTesting Loss: 532.957987\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.466965\n",
      "\tTesting Loss: 532.945190\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.468572\n",
      "\tTesting Loss: 532.950195\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.464953\n",
      "\tTesting Loss: 532.953552\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.467478\n",
      "\tTesting Loss: 532.946431\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.467122\n",
      "\tTesting Loss: 532.953542\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.467209\n",
      "\tTesting Loss: 532.948761\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.465276\n",
      "\tTesting Loss: 532.953044\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.468335\n",
      "\tTesting Loss: 532.944845\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.467834\n",
      "\tTesting Loss: 532.952372\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.464945\n",
      "\tTesting Loss: 532.950277\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.468501\n",
      "\tTesting Loss: 532.946472\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.467697\n",
      "\tTesting Loss: 532.951131\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.465548\n",
      "\tTesting Loss: 532.950002\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.468038\n",
      "\tTesting Loss: 532.945923\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.468191\n",
      "\tTesting Loss: 532.952576\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.465083\n",
      "\tTesting Loss: 532.949605\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.470271\n",
      "\tTesting Loss: 532.943380\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.466665\n",
      "\tTesting Loss: 532.952810\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.469055\n",
      "\tTesting Loss: 532.943553\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.467044\n",
      "\tTesting Loss: 532.953898\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.468806\n",
      "\tTesting Loss: 532.947673\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.468722\n",
      "\tTesting Loss: 532.949890\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.468498\n",
      "\tTesting Loss: 532.949626\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.469872\n",
      "\tTesting Loss: 532.946808\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.466291\n",
      "\tTesting Loss: 532.952911\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.469172\n",
      "\tTesting Loss: 532.942159\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.469983\n",
      "\tTesting Loss: 532.950470\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.468836\n",
      "\tTesting Loss: 532.946208\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.467677\n",
      "\tTesting Loss: 532.945882\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.469874\n",
      "\tTesting Loss: 532.944845\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.467484\n",
      "\tTesting Loss: 532.947998\n",
      "\tLearning Rate: 0.000003700\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.469599\n",
      "\tTesting Loss: 532.941528\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.470240\n",
      "\tTesting Loss: 532.946818\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.469152\n",
      "\tTesting Loss: 532.945475\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.467880\n",
      "\tTesting Loss: 532.944824\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.469938\n",
      "\tTesting Loss: 532.944041\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.470047\n",
      "\tTesting Loss: 532.945638\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.468135\n",
      "\tTesting Loss: 532.943949\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.470393\n",
      "\tTesting Loss: 532.945791\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.469068\n",
      "\tTesting Loss: 532.946655\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.469950\n",
      "\tTesting Loss: 532.944967\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.469701\n",
      "\tTesting Loss: 532.945699\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.468480\n",
      "\tTesting Loss: 532.946014\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.470197\n",
      "\tTesting Loss: 532.941956\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.470403\n",
      "\tTesting Loss: 532.944865\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.468486\n",
      "\tTesting Loss: 532.945404\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.470629\n",
      "\tTesting Loss: 532.942607\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.470240\n",
      "\tTesting Loss: 532.947693\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.469762\n",
      "\tTesting Loss: 532.942200\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.469935\n",
      "\tTesting Loss: 532.944316\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.469607\n",
      "\tTesting Loss: 532.945363\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.469793\n",
      "\tTesting Loss: 532.943461\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.470528\n",
      "\tTesting Loss: 532.945709\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.470078\n",
      "\tTesting Loss: 532.945801\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.471792\n",
      "\tTesting Loss: 532.943268\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.468508\n",
      "\tTesting Loss: 532.945496\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.471034\n",
      "\tTesting Loss: 532.940084\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.470891\n",
      "\tTesting Loss: 532.943665\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.469050\n",
      "\tTesting Loss: 532.942586\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.471583\n",
      "\tTesting Loss: 532.940653\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.470286\n",
      "\tTesting Loss: 532.943797\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.469826\n",
      "\tTesting Loss: 532.940643\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.470571\n",
      "\tTesting Loss: 532.942627\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.470245\n",
      "\tTesting Loss: 532.942362\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.471082\n",
      "\tTesting Loss: 532.940755\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.471176\n",
      "\tTesting Loss: 532.939830\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.469696\n",
      "\tTesting Loss: 532.942139\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.470520\n",
      "\tTesting Loss: 532.938700\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.471786\n",
      "\tTesting Loss: 532.938639\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.468999\n",
      "\tTesting Loss: 532.941366\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.471624\n",
      "\tTesting Loss: 532.937195\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.470942\n",
      "\tTesting Loss: 532.940175\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.470896\n",
      "\tTesting Loss: 532.939067\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.469365\n",
      "\tTesting Loss: 532.940613\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.471809\n",
      "\tTesting Loss: 532.937236\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.471207\n",
      "\tTesting Loss: 532.940877\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.470690\n",
      "\tTesting Loss: 532.937500\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.470352\n",
      "\tTesting Loss: 532.940969\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.471720\n",
      "\tTesting Loss: 532.937317\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.471161\n",
      "\tTesting Loss: 532.940033\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.469620\n",
      "\tTesting Loss: 532.937398\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.471901\n",
      "\tTesting Loss: 532.935954\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.470891\n",
      "\tTesting Loss: 532.938588\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.469559\n",
      "\tTesting Loss: 532.936646\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.471659\n",
      "\tTesting Loss: 532.936127\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.471453\n",
      "\tTesting Loss: 532.938792\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.469604\n",
      "\tTesting Loss: 532.934285\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.471476\n",
      "\tTesting Loss: 532.936951\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.471222\n",
      "\tTesting Loss: 532.937541\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.471021\n",
      "\tTesting Loss: 532.936401\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.469406\n",
      "\tTesting Loss: 532.935608\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.471476\n",
      "\tTesting Loss: 532.933634\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.471369\n",
      "\tTesting Loss: 532.937398\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.469950\n",
      "\tTesting Loss: 532.934428\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.471949\n",
      "\tTesting Loss: 532.935659\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.471336\n",
      "\tTesting Loss: 532.937988\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.470668\n",
      "\tTesting Loss: 532.933777\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.471868\n",
      "\tTesting Loss: 532.936381\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.470454\n",
      "\tTesting Loss: 532.936829\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.472036\n",
      "\tTesting Loss: 532.934469\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.471670\n",
      "\tTesting Loss: 532.936056\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.469777\n",
      "\tTesting Loss: 532.933879\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.471415\n",
      "\tTesting Loss: 532.932617\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.472788\n",
      "\tTesting Loss: 532.931437\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.469076\n",
      "\tTesting Loss: 532.932027\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.473038\n",
      "\tTesting Loss: 532.930054\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.469716\n",
      "\tTesting Loss: 532.934611\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.471858\n",
      "\tTesting Loss: 532.929036\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.471914\n",
      "\tTesting Loss: 532.935221\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.469844\n",
      "\tTesting Loss: 532.933004\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.472321\n",
      "\tTesting Loss: 532.929352\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.471575\n",
      "\tTesting Loss: 532.933472\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.471507\n",
      "\tTesting Loss: 532.930461\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.470431\n",
      "\tTesting Loss: 532.933716\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.471909\n",
      "\tTesting Loss: 532.929708\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.472102\n",
      "\tTesting Loss: 532.929199\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.469653\n",
      "\tTesting Loss: 532.931671\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.472903\n",
      "\tTesting Loss: 532.927327\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.472107\n",
      "\tTesting Loss: 532.931488\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.471171\n",
      "\tTesting Loss: 532.928518\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.471036\n",
      "\tTesting Loss: 532.930959\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.473297\n",
      "\tTesting Loss: 532.927897\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.471802\n",
      "\tTesting Loss: 532.932129\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.470388\n",
      "\tTesting Loss: 532.928121\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.472722\n",
      "\tTesting Loss: 532.927836\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.472982\n",
      "\tTesting Loss: 532.929260\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.470815\n",
      "\tTesting Loss: 532.929047\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.471715\n",
      "\tTesting Loss: 532.927928\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.471812\n",
      "\tTesting Loss: 532.929871\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.473053\n",
      "\tTesting Loss: 532.925975\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.471535\n",
      "\tTesting Loss: 532.930837\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.471324\n",
      "\tTesting Loss: 532.927104\n",
      "\tLearning Rate: 0.000003330\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.472575\n",
      "\tTesting Loss: 532.925293\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.471464\n",
      "\tTesting Loss: 532.928019\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.473653\n",
      "\tTesting Loss: 532.924825\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.470612\n",
      "\tTesting Loss: 532.929667\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.473958\n",
      "\tTesting Loss: 532.922221\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.470484\n",
      "\tTesting Loss: 532.930023\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.473068\n",
      "\tTesting Loss: 532.920797\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.472130\n",
      "\tTesting Loss: 532.930562\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.473328\n",
      "\tTesting Loss: 532.922892\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.470891\n",
      "\tTesting Loss: 532.929047\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.473933\n",
      "\tTesting Loss: 532.920898\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.470871\n",
      "\tTesting Loss: 532.929403\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.472649\n",
      "\tTesting Loss: 532.920451\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.472580\n",
      "\tTesting Loss: 532.927734\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.472819\n",
      "\tTesting Loss: 532.922526\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.471153\n",
      "\tTesting Loss: 532.926290\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.473648\n",
      "\tTesting Loss: 532.921794\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.471400\n",
      "\tTesting Loss: 532.925893\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.472880\n",
      "\tTesting Loss: 532.919902\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.473389\n",
      "\tTesting Loss: 532.925049\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.471227\n",
      "\tTesting Loss: 532.920695\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.473445\n",
      "\tTesting Loss: 532.920013\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.473170\n",
      "\tTesting Loss: 532.923564\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.471626\n",
      "\tTesting Loss: 532.920919\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.473455\n",
      "\tTesting Loss: 532.919291\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.471563\n",
      "\tTesting Loss: 532.921712\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.473409\n",
      "\tTesting Loss: 532.918752\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.473170\n",
      "\tTesting Loss: 532.921224\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.471405\n",
      "\tTesting Loss: 532.920227\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.473190\n",
      "\tTesting Loss: 532.918742\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.473689\n",
      "\tTesting Loss: 532.919922\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.469701\n",
      "\tTesting Loss: 532.917145\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.474813\n",
      "\tTesting Loss: 532.916921\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.471868\n",
      "\tTesting Loss: 532.922445\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.471868\n",
      "\tTesting Loss: 532.913259\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.473160\n",
      "\tTesting Loss: 532.920736\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.473635\n",
      "\tTesting Loss: 532.917043\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.471705\n",
      "\tTesting Loss: 532.918386\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.472743\n",
      "\tTesting Loss: 532.916239\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.474040\n",
      "\tTesting Loss: 532.916524\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.470118\n",
      "\tTesting Loss: 532.917796\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.474955\n",
      "\tTesting Loss: 532.913798\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.471934\n",
      "\tTesting Loss: 532.921977\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.472504\n",
      "\tTesting Loss: 532.911357\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.472870\n",
      "\tTesting Loss: 532.919108\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.473022\n",
      "\tTesting Loss: 532.913605\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.472509\n",
      "\tTesting Loss: 532.917419\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.472140\n",
      "\tTesting Loss: 532.914429\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.473162\n",
      "\tTesting Loss: 532.915243\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.473531\n",
      "\tTesting Loss: 532.914530\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.469401\n",
      "\tTesting Loss: 532.912740\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.474915\n",
      "\tTesting Loss: 532.914246\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.471387\n",
      "\tTesting Loss: 532.918538\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.472346\n",
      "\tTesting Loss: 532.910319\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.472631\n",
      "\tTesting Loss: 532.916117\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.473671\n",
      "\tTesting Loss: 532.911886\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.469869\n",
      "\tTesting Loss: 532.914622\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.474996\n",
      "\tTesting Loss: 532.910645\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.472003\n",
      "\tTesting Loss: 532.917308\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.472127\n",
      "\tTesting Loss: 532.908478\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.473155\n",
      "\tTesting Loss: 532.917287\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.473648\n",
      "\tTesting Loss: 532.909709\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.470647\n",
      "\tTesting Loss: 532.912832\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.474620\n",
      "\tTesting Loss: 532.912781\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.472593\n",
      "\tTesting Loss: 532.912730\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.470314\n",
      "\tTesting Loss: 532.908712\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.474915\n",
      "\tTesting Loss: 532.912496\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.471705\n",
      "\tTesting Loss: 532.913066\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.473287\n",
      "\tTesting Loss: 532.907440\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.472438\n",
      "\tTesting Loss: 532.914307\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.473399\n",
      "\tTesting Loss: 532.907756\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.473584\n",
      "\tTesting Loss: 532.911092\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.470149\n",
      "\tTesting Loss: 532.909332\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.475057\n",
      "\tTesting Loss: 532.910085\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.471776\n",
      "\tTesting Loss: 532.913940\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.472763\n",
      "\tTesting Loss: 532.905884\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.472420\n",
      "\tTesting Loss: 532.912628\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.473262\n",
      "\tTesting Loss: 532.907145\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.472692\n",
      "\tTesting Loss: 532.912211\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.472941\n",
      "\tTesting Loss: 532.909047\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.473689\n",
      "\tTesting Loss: 532.907288\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.469833\n",
      "\tTesting Loss: 532.909749\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.475693\n",
      "\tTesting Loss: 532.907166\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.471629\n",
      "\tTesting Loss: 532.912170\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.473063\n",
      "\tTesting Loss: 532.902649\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.472565\n",
      "\tTesting Loss: 532.912455\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.475032\n",
      "\tTesting Loss: 532.903849\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.469556\n",
      "\tTesting Loss: 532.910685\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.476440\n",
      "\tTesting Loss: 532.901835\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.468547\n",
      "\tTesting Loss: 532.910665\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.476964\n",
      "\tTesting Loss: 532.897675\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.468486\n",
      "\tTesting Loss: 532.913096\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.476278\n",
      "\tTesting Loss: 532.894745\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.468638\n",
      "\tTesting Loss: 532.912557\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.475006\n",
      "\tTesting Loss: 532.892558\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.469121\n",
      "\tTesting Loss: 532.914876\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.475306\n",
      "\tTesting Loss: 532.894938\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.468928\n",
      "\tTesting Loss: 532.916829\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.475461\n",
      "\tTesting Loss: 532.894796\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.469238\n",
      "\tTesting Loss: 532.914469\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.475311\n",
      "\tTesting Loss: 532.896118\n",
      "\tLearning Rate: 0.000002997\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.469269\n",
      "\tTesting Loss: 532.915507\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.475357\n",
      "\tTesting Loss: 532.895894\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.470215\n",
      "\tTesting Loss: 532.910461\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.475711\n",
      "\tTesting Loss: 532.897329\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.468717\n",
      "\tTesting Loss: 532.914307\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.477002\n",
      "\tTesting Loss: 532.896362\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.469650\n",
      "\tTesting Loss: 532.914062\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.476852\n",
      "\tTesting Loss: 532.893087\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.466761\n",
      "\tTesting Loss: 532.913066\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.478325\n",
      "\tTesting Loss: 532.890584\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.464630\n",
      "\tTesting Loss: 532.914347\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.479952\n",
      "\tTesting Loss: 532.887370\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.461759\n",
      "\tTesting Loss: 532.911458\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.481705\n",
      "\tTesting Loss: 532.887085\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.458399\n",
      "\tTesting Loss: 532.903483\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.482910\n",
      "\tTesting Loss: 532.894307\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.455994\n",
      "\tTesting Loss: 532.888011\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.483907\n",
      "\tTesting Loss: 532.906708\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.453883\n",
      "\tTesting Loss: 532.863424\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.485415\n",
      "\tTesting Loss: 532.920593\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.448349\n",
      "\tTesting Loss: 532.825989\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.484436\n",
      "\tTesting Loss: 532.934631\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.448448\n",
      "\tTesting Loss: 532.810669\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.482033\n",
      "\tTesting Loss: 532.933990\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.450348\n",
      "\tTesting Loss: 532.810791\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.478958\n",
      "\tTesting Loss: 532.924967\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.456088\n",
      "\tTesting Loss: 532.826192\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.475927\n",
      "\tTesting Loss: 532.911316\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.460520\n",
      "\tTesting Loss: 532.841431\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.473429\n",
      "\tTesting Loss: 532.902140\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.462865\n",
      "\tTesting Loss: 532.853607\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.472000\n",
      "\tTesting Loss: 532.895976\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.464701\n",
      "\tTesting Loss: 532.863403\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.470993\n",
      "\tTesting Loss: 532.893992\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.466456\n",
      "\tTesting Loss: 532.870565\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.470805\n",
      "\tTesting Loss: 532.890706\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.466909\n",
      "\tTesting Loss: 532.874674\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.471395\n",
      "\tTesting Loss: 532.890625\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.466192\n",
      "\tTesting Loss: 532.874736\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.471761\n",
      "\tTesting Loss: 532.892273\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.466520\n",
      "\tTesting Loss: 532.875427\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.470907\n",
      "\tTesting Loss: 532.891561\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.468170\n",
      "\tTesting Loss: 532.880096\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.471029\n",
      "\tTesting Loss: 532.890717\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.468201\n",
      "\tTesting Loss: 532.879659\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.470591\n",
      "\tTesting Loss: 532.888774\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.468447\n",
      "\tTesting Loss: 532.882263\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.470734\n",
      "\tTesting Loss: 532.888082\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.468725\n",
      "\tTesting Loss: 532.881917\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.469444\n",
      "\tTesting Loss: 532.888265\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.469493\n",
      "\tTesting Loss: 532.884135\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.470589\n",
      "\tTesting Loss: 532.886637\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.469660\n",
      "\tTesting Loss: 532.884705\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.468831\n",
      "\tTesting Loss: 532.886037\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.470230\n",
      "\tTesting Loss: 532.885406\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.468653\n",
      "\tTesting Loss: 532.884562\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.470785\n",
      "\tTesting Loss: 532.886007\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.468104\n",
      "\tTesting Loss: 532.881226\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.470940\n",
      "\tTesting Loss: 532.886617\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.467682\n",
      "\tTesting Loss: 532.880697\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.471011\n",
      "\tTesting Loss: 532.886739\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.467946\n",
      "\tTesting Loss: 532.881755\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.470846\n",
      "\tTesting Loss: 532.886780\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.469205\n",
      "\tTesting Loss: 532.881673\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.471138\n",
      "\tTesting Loss: 532.886658\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.468788\n",
      "\tTesting Loss: 532.882690\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.471405\n",
      "\tTesting Loss: 532.886617\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.468608\n",
      "\tTesting Loss: 532.881653\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.471822\n",
      "\tTesting Loss: 532.886078\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.466710\n",
      "\tTesting Loss: 532.879852\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.472880\n",
      "\tTesting Loss: 532.889160\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.466240\n",
      "\tTesting Loss: 532.876811\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.472661\n",
      "\tTesting Loss: 532.889242\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.466553\n",
      "\tTesting Loss: 532.876383\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.472799\n",
      "\tTesting Loss: 532.888814\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.465983\n",
      "\tTesting Loss: 532.875488\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.473251\n",
      "\tTesting Loss: 532.888814\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.465736\n",
      "\tTesting Loss: 532.874125\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.473694\n",
      "\tTesting Loss: 532.889384\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.465205\n",
      "\tTesting Loss: 532.872559\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.473083\n",
      "\tTesting Loss: 532.890778\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.466380\n",
      "\tTesting Loss: 532.873311\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.472870\n",
      "\tTesting Loss: 532.889709\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.466858\n",
      "\tTesting Loss: 532.874268\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.474238\n",
      "\tTesting Loss: 532.887207\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.466314\n",
      "\tTesting Loss: 532.873250\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.474658\n",
      "\tTesting Loss: 532.887512\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.464122\n",
      "\tTesting Loss: 532.870219\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.475647\n",
      "\tTesting Loss: 532.890004\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.462924\n",
      "\tTesting Loss: 532.865865\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.475723\n",
      "\tTesting Loss: 532.892161\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.462616\n",
      "\tTesting Loss: 532.864115\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.474879\n",
      "\tTesting Loss: 532.892792\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.465434\n",
      "\tTesting Loss: 532.866048\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.475245\n",
      "\tTesting Loss: 532.887126\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.463120\n",
      "\tTesting Loss: 532.862590\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.476410\n",
      "\tTesting Loss: 532.890645\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.463003\n",
      "\tTesting Loss: 532.860453\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.476285\n",
      "\tTesting Loss: 532.889954\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.462168\n",
      "\tTesting Loss: 532.858521\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.476888\n",
      "\tTesting Loss: 532.890696\n",
      "\tLearning Rate: 0.000002697\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.461413\n",
      "\tTesting Loss: 532.855275\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.475782\n",
      "\tTesting Loss: 532.894836\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.462013\n",
      "\tTesting Loss: 532.847361\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.474523\n",
      "\tTesting Loss: 532.894653\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.465922\n",
      "\tTesting Loss: 532.855408\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.473648\n",
      "\tTesting Loss: 532.888184\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.466057\n",
      "\tTesting Loss: 532.857808\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.473221\n",
      "\tTesting Loss: 532.887014\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.466787\n",
      "\tTesting Loss: 532.860555\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.472509\n",
      "\tTesting Loss: 532.883392\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.467789\n",
      "\tTesting Loss: 532.864115\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.472582\n",
      "\tTesting Loss: 532.881836\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.468025\n",
      "\tTesting Loss: 532.865601\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.472692\n",
      "\tTesting Loss: 532.879496\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.467306\n",
      "\tTesting Loss: 532.866150\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.472595\n",
      "\tTesting Loss: 532.879639\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.467672\n",
      "\tTesting Loss: 532.866536\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.472443\n",
      "\tTesting Loss: 532.878754\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.467860\n",
      "\tTesting Loss: 532.866791\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.471944\n",
      "\tTesting Loss: 532.877157\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.469401\n",
      "\tTesting Loss: 532.867940\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.471662\n",
      "\tTesting Loss: 532.876444\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.469045\n",
      "\tTesting Loss: 532.867940\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.471881\n",
      "\tTesting Loss: 532.874532\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.467387\n",
      "\tTesting Loss: 532.867147\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.472198\n",
      "\tTesting Loss: 532.875824\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.467738\n",
      "\tTesting Loss: 532.865529\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.472412\n",
      "\tTesting Loss: 532.876048\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.467422\n",
      "\tTesting Loss: 532.864848\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.471812\n",
      "\tTesting Loss: 532.876099\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.467921\n",
      "\tTesting Loss: 532.864766\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.472201\n",
      "\tTesting Loss: 532.875346\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.468689\n",
      "\tTesting Loss: 532.865997\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.472092\n",
      "\tTesting Loss: 532.874776\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.469355\n",
      "\tTesting Loss: 532.865733\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.471980\n",
      "\tTesting Loss: 532.874003\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.469096\n",
      "\tTesting Loss: 532.865397\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.472305\n",
      "\tTesting Loss: 532.872996\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.467677\n",
      "\tTesting Loss: 532.865824\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.473068\n",
      "\tTesting Loss: 532.873250\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.467204\n",
      "\tTesting Loss: 532.863007\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.473160\n",
      "\tTesting Loss: 532.875041\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.467077\n",
      "\tTesting Loss: 532.861074\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.473165\n",
      "\tTesting Loss: 532.873393\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.466771\n",
      "\tTesting Loss: 532.858805\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.472298\n",
      "\tTesting Loss: 532.874980\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.469442\n",
      "\tTesting Loss: 532.863230\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.472290\n",
      "\tTesting Loss: 532.872396\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.468849\n",
      "\tTesting Loss: 532.862528\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.472850\n",
      "\tTesting Loss: 532.873454\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.468613\n",
      "\tTesting Loss: 532.862528\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.472722\n",
      "\tTesting Loss: 532.871063\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.467061\n",
      "\tTesting Loss: 532.862508\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.473790\n",
      "\tTesting Loss: 532.872772\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.466049\n",
      "\tTesting Loss: 532.857503\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.473727\n",
      "\tTesting Loss: 532.873474\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.466057\n",
      "\tTesting Loss: 532.856903\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.473999\n",
      "\tTesting Loss: 532.873413\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.467621\n",
      "\tTesting Loss: 532.858348\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.473745\n",
      "\tTesting Loss: 532.872487\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.467911\n",
      "\tTesting Loss: 532.857300\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.473696\n",
      "\tTesting Loss: 532.871490\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.467402\n",
      "\tTesting Loss: 532.857798\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.473923\n",
      "\tTesting Loss: 532.871023\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.466540\n",
      "\tTesting Loss: 532.856669\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.474391\n",
      "\tTesting Loss: 532.871368\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.464986\n",
      "\tTesting Loss: 532.853546\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.474533\n",
      "\tTesting Loss: 532.871663\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.464973\n",
      "\tTesting Loss: 532.851980\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.475065\n",
      "\tTesting Loss: 532.871348\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.464411\n",
      "\tTesting Loss: 532.849284\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.474152\n",
      "\tTesting Loss: 532.873596\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.467270\n",
      "\tTesting Loss: 532.852356\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.474185\n",
      "\tTesting Loss: 532.869324\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.466298\n",
      "\tTesting Loss: 532.851949\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.474574\n",
      "\tTesting Loss: 532.870382\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.465317\n",
      "\tTesting Loss: 532.850749\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.474864\n",
      "\tTesting Loss: 532.870809\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.464167\n",
      "\tTesting Loss: 532.848857\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.475484\n",
      "\tTesting Loss: 532.870992\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.463511\n",
      "\tTesting Loss: 532.844604\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.474513\n",
      "\tTesting Loss: 532.872762\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.465561\n",
      "\tTesting Loss: 532.846476\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.473928\n",
      "\tTesting Loss: 532.868327\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.465579\n",
      "\tTesting Loss: 532.847331\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.474286\n",
      "\tTesting Loss: 532.868245\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.465294\n",
      "\tTesting Loss: 532.848165\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.474920\n",
      "\tTesting Loss: 532.868022\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.462926\n",
      "\tTesting Loss: 532.843831\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.475220\n",
      "\tTesting Loss: 532.869253\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.463440\n",
      "\tTesting Loss: 532.841756\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.474747\n",
      "\tTesting Loss: 532.868571\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.463160\n",
      "\tTesting Loss: 532.840535\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.474068\n",
      "\tTesting Loss: 532.870097\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.465093\n",
      "\tTesting Loss: 532.842834\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.473666\n",
      "\tTesting Loss: 532.867065\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.465500\n",
      "\tTesting Loss: 532.843302\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.473846\n",
      "\tTesting Loss: 532.866781\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.465355\n",
      "\tTesting Loss: 532.844238\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.474136\n",
      "\tTesting Loss: 532.864258\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.464666\n",
      "\tTesting Loss: 532.843597\n",
      "\tLearning Rate: 0.000002427\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.474279\n",
      "\tTesting Loss: 532.865011\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.464493\n",
      "\tTesting Loss: 532.839518\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.473607\n",
      "\tTesting Loss: 532.868988\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.465332\n",
      "\tTesting Loss: 532.840332\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.473033\n",
      "\tTesting Loss: 532.866201\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.466003\n",
      "\tTesting Loss: 532.842855\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.471975\n",
      "\tTesting Loss: 532.863597\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.466446\n",
      "\tTesting Loss: 532.843750\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.470904\n",
      "\tTesting Loss: 532.861532\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.468760\n",
      "\tTesting Loss: 532.848501\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.470533\n",
      "\tTesting Loss: 532.857839\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.469426\n",
      "\tTesting Loss: 532.851267\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.470383\n",
      "\tTesting Loss: 532.856242\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.469111\n",
      "\tTesting Loss: 532.852081\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.470919\n",
      "\tTesting Loss: 532.856445\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.469421\n",
      "\tTesting Loss: 532.851237\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.469414\n",
      "\tTesting Loss: 532.855693\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.470413\n",
      "\tTesting Loss: 532.852661\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.468681\n",
      "\tTesting Loss: 532.852529\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.470612\n",
      "\tTesting Loss: 532.853648\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.469777\n",
      "\tTesting Loss: 532.853465\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.470083\n",
      "\tTesting Loss: 532.852010\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.470093\n",
      "\tTesting Loss: 532.852966\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.469523\n",
      "\tTesting Loss: 532.852559\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.470434\n",
      "\tTesting Loss: 532.853170\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.470154\n",
      "\tTesting Loss: 532.851115\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.468857\n",
      "\tTesting Loss: 532.852946\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.471085\n",
      "\tTesting Loss: 532.852661\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.469536\n",
      "\tTesting Loss: 532.852336\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.470673\n",
      "\tTesting Loss: 532.851420\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.470022\n",
      "\tTesting Loss: 532.852000\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.470408\n",
      "\tTesting Loss: 532.852132\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.470029\n",
      "\tTesting Loss: 532.851084\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.468892\n",
      "\tTesting Loss: 532.851318\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.471247\n",
      "\tTesting Loss: 532.852559\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.469950\n",
      "\tTesting Loss: 532.849162\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.469014\n",
      "\tTesting Loss: 532.850545\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.470601\n",
      "\tTesting Loss: 532.851054\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.470205\n",
      "\tTesting Loss: 532.851206\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.470731\n",
      "\tTesting Loss: 532.850952\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.471011\n",
      "\tTesting Loss: 532.849589\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.468679\n",
      "\tTesting Loss: 532.849447\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.472036\n",
      "\tTesting Loss: 532.850932\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.467801\n",
      "\tTesting Loss: 532.846598\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.472463\n",
      "\tTesting Loss: 532.852153\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.467311\n",
      "\tTesting Loss: 532.844767\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.473297\n",
      "\tTesting Loss: 532.853353\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.467257\n",
      "\tTesting Loss: 532.842489\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.472341\n",
      "\tTesting Loss: 532.852641\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.467651\n",
      "\tTesting Loss: 532.842570\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.472514\n",
      "\tTesting Loss: 532.851807\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.467534\n",
      "\tTesting Loss: 532.842896\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.472829\n",
      "\tTesting Loss: 532.853251\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.467453\n",
      "\tTesting Loss: 532.840739\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.472448\n",
      "\tTesting Loss: 532.853292\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.467224\n",
      "\tTesting Loss: 532.841451\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.473185\n",
      "\tTesting Loss: 532.852681\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.467529\n",
      "\tTesting Loss: 532.841003\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.473015\n",
      "\tTesting Loss: 532.852885\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.467585\n",
      "\tTesting Loss: 532.839528\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.472453\n",
      "\tTesting Loss: 532.853678\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.468781\n",
      "\tTesting Loss: 532.841451\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.472097\n",
      "\tTesting Loss: 532.850250\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.469574\n",
      "\tTesting Loss: 532.843547\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.472768\n",
      "\tTesting Loss: 532.850393\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.468569\n",
      "\tTesting Loss: 532.842448\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.473445\n",
      "\tTesting Loss: 532.850138\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.466832\n",
      "\tTesting Loss: 532.839864\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.474014\n",
      "\tTesting Loss: 532.851969\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.467028\n",
      "\tTesting Loss: 532.837392\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.474093\n",
      "\tTesting Loss: 532.852102\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.467265\n",
      "\tTesting Loss: 532.836060\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.473317\n",
      "\tTesting Loss: 532.850393\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.467585\n",
      "\tTesting Loss: 532.837351\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.473602\n",
      "\tTesting Loss: 532.850749\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.468498\n",
      "\tTesting Loss: 532.837545\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.473460\n",
      "\tTesting Loss: 532.849670\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.468880\n",
      "\tTesting Loss: 532.839742\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.473447\n",
      "\tTesting Loss: 532.847432\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.468597\n",
      "\tTesting Loss: 532.839803\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.473897\n",
      "\tTesting Loss: 532.848867\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.468282\n",
      "\tTesting Loss: 532.839620\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.474559\n",
      "\tTesting Loss: 532.848185\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.466466\n",
      "\tTesting Loss: 532.836060\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.476166\n",
      "\tTesting Loss: 532.850454\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.466034\n",
      "\tTesting Loss: 532.832581\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.476156\n",
      "\tTesting Loss: 532.850911\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.465836\n",
      "\tTesting Loss: 532.827413\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.474388\n",
      "\tTesting Loss: 532.848816\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.466476\n",
      "\tTesting Loss: 532.829773\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.474798\n",
      "\tTesting Loss: 532.847758\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.466082\n",
      "\tTesting Loss: 532.828634\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.474564\n",
      "\tTesting Loss: 532.849162\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.466298\n",
      "\tTesting Loss: 532.829020\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.474274\n",
      "\tTesting Loss: 532.848877\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.467824\n",
      "\tTesting Loss: 532.830261\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.473994\n",
      "\tTesting Loss: 532.846334\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.468119\n",
      "\tTesting Loss: 532.832906\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.474223\n",
      "\tTesting Loss: 532.846130\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.467700\n",
      "\tTesting Loss: 532.831197\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.474167\n",
      "\tTesting Loss: 532.845398\n",
      "\tLearning Rate: 0.000002185\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.466461\n",
      "\tTesting Loss: 532.831136\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.474462\n",
      "\tTesting Loss: 532.847829\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.466848\n",
      "\tTesting Loss: 532.825734\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.474197\n",
      "\tTesting Loss: 532.848185\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.467771\n",
      "\tTesting Loss: 532.827535\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.473033\n",
      "\tTesting Loss: 532.845378\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.468801\n",
      "\tTesting Loss: 532.829305\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.472504\n",
      "\tTesting Loss: 532.843038\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.470179\n",
      "\tTesting Loss: 532.833303\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.472051\n",
      "\tTesting Loss: 532.840474\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.470215\n",
      "\tTesting Loss: 532.834208\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.472229\n",
      "\tTesting Loss: 532.840179\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.470322\n",
      "\tTesting Loss: 532.835429\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.471914\n",
      "\tTesting Loss: 532.839518\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.470434\n",
      "\tTesting Loss: 532.835561\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.470955\n",
      "\tTesting Loss: 532.838908\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.470950\n",
      "\tTesting Loss: 532.835714\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.472178\n",
      "\tTesting Loss: 532.837972\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.468959\n",
      "\tTesting Loss: 532.833944\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.473170\n",
      "\tTesting Loss: 532.839518\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.468056\n",
      "\tTesting Loss: 532.830861\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.473404\n",
      "\tTesting Loss: 532.840454\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.468562\n",
      "\tTesting Loss: 532.829620\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.472946\n",
      "\tTesting Loss: 532.840149\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.468877\n",
      "\tTesting Loss: 532.828979\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.472310\n",
      "\tTesting Loss: 532.839233\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.469228\n",
      "\tTesting Loss: 532.831075\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.472839\n",
      "\tTesting Loss: 532.838298\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.469279\n",
      "\tTesting Loss: 532.830709\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.472196\n",
      "\tTesting Loss: 532.837555\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.470540\n",
      "\tTesting Loss: 532.832886\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.472163\n",
      "\tTesting Loss: 532.836121\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.470449\n",
      "\tTesting Loss: 532.833435\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.472112\n",
      "\tTesting Loss: 532.835958\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.470105\n",
      "\tTesting Loss: 532.831523\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.472275\n",
      "\tTesting Loss: 532.835958\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.469976\n",
      "\tTesting Loss: 532.831787\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.472402\n",
      "\tTesting Loss: 532.835744\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.470454\n",
      "\tTesting Loss: 532.831604\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.472478\n",
      "\tTesting Loss: 532.834585\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.469945\n",
      "\tTesting Loss: 532.832031\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.472753\n",
      "\tTesting Loss: 532.834371\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.468531\n",
      "\tTesting Loss: 532.827840\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.473918\n",
      "\tTesting Loss: 532.835765\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.468150\n",
      "\tTesting Loss: 532.826324\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.474208\n",
      "\tTesting Loss: 532.836182\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.468000\n",
      "\tTesting Loss: 532.823802\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 547.473592\n",
      "\tTesting Loss: 532.835978\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 547.468468\n",
      "\tTesting Loss: 532.823110\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 547.473607\n",
      "\tTesting Loss: 532.834513\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 547.468206\n",
      "\tTesting Loss: 532.821960\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 547.473109\n",
      "\tTesting Loss: 532.834585\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 547.468867\n",
      "\tTesting Loss: 532.822489\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 547.472796\n",
      "\tTesting Loss: 532.833598\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 547.469131\n",
      "\tTesting Loss: 532.824870\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [1/50]\n",
      "\tTraining Loss: 547.473134\n",
      "\tTesting Loss: 532.833171\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 547.469767\n",
      "\tTesting Loss: 532.825022\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 547.472855\n",
      "\tTesting Loss: 532.831746\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 547.468882\n",
      "\tTesting Loss: 532.824554\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 547.473317\n",
      "\tTesting Loss: 532.833160\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 547.469279\n",
      "\tTesting Loss: 532.823018\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 547.472829\n",
      "\tTesting Loss: 532.830729\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 547.468455\n",
      "\tTesting Loss: 532.823140\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 547.473653\n",
      "\tTesting Loss: 532.830241\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 547.468592\n",
      "\tTesting Loss: 532.821523\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 547.473200\n",
      "\tTesting Loss: 532.831238\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 547.469721\n",
      "\tTesting Loss: 532.823161\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 547.472595\n",
      "\tTesting Loss: 532.829091\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 547.470164\n",
      "\tTesting Loss: 532.823944\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 547.473025\n",
      "\tTesting Loss: 532.828756\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 547.469752\n",
      "\tTesting Loss: 532.823680\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 547.473022\n",
      "\tTesting Loss: 532.828735\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 547.469421\n",
      "\tTesting Loss: 532.823222\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 547.471883\n",
      "\tTesting Loss: 532.828288\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 547.470230\n",
      "\tTesting Loss: 532.823313\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 547.472956\n",
      "\tTesting Loss: 532.827647\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 547.469625\n",
      "\tTesting Loss: 532.822734\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 547.473089\n",
      "\tTesting Loss: 532.826508\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 547.467730\n",
      "\tTesting Loss: 532.819468\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 547.474495\n",
      "\tTesting Loss: 532.830139\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 547.467458\n",
      "\tTesting Loss: 532.816823\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 547.474416\n",
      "\tTesting Loss: 532.830098\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 547.467580\n",
      "\tTesting Loss: 532.813416\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 547.473841\n",
      "\tTesting Loss: 532.827474\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 547.468040\n",
      "\tTesting Loss: 532.813477\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 547.472847\n",
      "\tTesting Loss: 532.825928\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 547.468221\n",
      "\tTesting Loss: 532.814657\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 547.473674\n",
      "\tTesting Loss: 532.825033\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 547.467906\n",
      "\tTesting Loss: 532.814748\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 547.473826\n",
      "\tTesting Loss: 532.826447\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 547.467616\n",
      "\tTesting Loss: 532.811605\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 547.472946\n",
      "\tTesting Loss: 532.826599\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 547.468445\n",
      "\tTesting Loss: 532.813171\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 547.472692\n",
      "\tTesting Loss: 532.823832\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 547.468290\n",
      "\tTesting Loss: 532.813741\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 547.472623\n",
      "\tTesting Loss: 532.824219\n",
      "\tLearning Rate: 0.000001966\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 547.468267\n",
      "\tTesting Loss: 532.814697\n",
      "\tLearning Rate: 0.000001966\n"
     ]
    }
   ],
   "source": [
    "# ---- train and eval ---- \n",
    "if overwrite:\n",
    "    # remove model_path, loss_path, eval_metrics_path\n",
    "    if os.path.exists(model_path):\n",
    "        os.remove(model_path)\n",
    "    if os.path.exists(eval_path):\n",
    "        os.remove(eval_path)\n",
    "    overwrite = False # reset overwrite to False\n",
    "if overwrite or not os.path.exists(model_path):\n",
    "    for i in range(num_saves): \n",
    "        train_losses_tmp, test_losses_tmp = train(model, \n",
    "                                                train_dataloader,\n",
    "                                                test_dataloader, \n",
    "                                                optimizer, \n",
    "                                                scheduler,\n",
    "                                                num_epochs, \n",
    "                                                device,\n",
    "                                                loss_type)\n",
    "        train_losses = train_losses + train_losses_tmp\n",
    "        test_losses = test_losses + test_losses_tmp\n",
    "        # every num_epochs, evaluate the model\n",
    "        train_eval_metrics = eval_model(model, y_true_train, ts_df_train, txt_ls, ts_encoder_name, text_encoder_name)\n",
    "        test_eval_metrics = eval_model(model, y_true_test, ts_df_test, txt_ls, ts_encoder_name, text_encoder_name)\n",
    "        train_eval_metrics_list.append(train_eval_metrics)\n",
    "        test_eval_metrics_list.append(test_eval_metrics)\n",
    "        # save model and losses\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        torch.save({\n",
    "            'train_losses': train_losses,\n",
    "            'test_losses': test_losses,\n",
    "            'train_evals': train_eval_metrics_list,\n",
    "            'test_evals': test_eval_metrics_list }, eval_path)\n",
    "        if i % 10 == 0: # every 10 saves, evaluate the model\n",
    "            eval_dict = torch.load(eval_path)\n",
    "            eval_dict_eng = eng_eval_metrics(eval_dict, binary=True, plot=True)\n",
    "    \n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    eval_dict = torch.load(eval_path)\n",
    "    eval_dict_eng = eng_eval_metrics(eval_dict, binary=True, plot_confusion_matrices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
