{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "from encoder import *\n",
    "from data import *\n",
    "from clip import *\n",
    "from eval import *\n",
    "from augmentor import *\n",
    "from describer import *\n",
    "print(\"using device: \", device)\n",
    "random_state = 333\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import pkg_resources\n",
    "# print(pkg_resources.get_distribution('python-calamine').version)\n",
    "\n",
    "# for debugging\n",
    "# import importlib\n",
    "# import models\n",
    "# importlib.reload(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'device': 'cpu',\n",
       " 'batch_size': 128,\n",
       " 'text_encoder_name': 'sentence-transformers/all-mpnet-base-v2',\n",
       " 'ts_encoder_name': 'hr_vae_linear_medium',\n",
       " 'ts_aug': False,\n",
       " 'ts_normalize': False,\n",
       " 'ts_encode': False,\n",
       " 'block_target': False,\n",
       " 'balance': False,\n",
       " 'model_name': 'clip_hr_death_raw___die7d_ga_bwt_sumb_simple_succ_inc_histogram',\n",
       " 'embedded_dim': 128,\n",
       " 'init_lr': 0.0001,\n",
       " 'patience': 20,\n",
       " 'num_saves': 20,\n",
       " 'num_epochs': 100,\n",
       " 'loss_type': 'block_diagonal',\n",
       " 'txt_ls': ['will die', 'will survive'],\n",
       " 'text_config': {'cl': {'die7d': True, 'fio2': False},\n",
       "  'demo': {'ga_bwt': True, 'gre': False, 'apgar_mage': False},\n",
       "  'ts': {'sumb': True,\n",
       "   'sumd': False,\n",
       "   'simple': True,\n",
       "   'full': False,\n",
       "   'event1': False,\n",
       "   'succ_inc': True,\n",
       "   'histogram': True}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- update configs ----\n",
    "overwrite = False\n",
    "model_name = 'clip_hr_death_raw'\n",
    "text_config['cl']['die7d'] = True # udpate text_config here if needed\n",
    "model_name = model_name + \"___\" + \"_\".join(get_true_components(text_config))\n",
    "txt_ls = ['will die', 'will survive']\n",
    "\n",
    "update_config(\n",
    "    model_name = model_name,\n",
    "    ts_aug = False,\n",
    "    ts_normalize = False,\n",
    "    ts_encode = False,\n",
    "    balance = False,\n",
    "    block_target = False,\n",
    "    txt_ls = txt_ls,\n",
    "    patience = 20,\n",
    "    num_saves = 20,\n",
    "    num_epochs = 100,\n",
    "    text_config = text_config\n",
    ")\n",
    "config_dict = get_config_dict()\n",
    "config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of patients with positive labels:\n",
      "VitalID\n",
      "1018    8\n",
      "5170    8\n",
      "1464    8\n",
      "2361    8\n",
      "2791    8\n",
      "dtype: int64\n",
      "This infant will survive.  This infant has gestational age 24 weeks. Birth weight is 360 grams.  No Bradycardia events.   Very low amount of consecutive increases. \n",
      "\n",
      "Sample of patients with positive labels:\n",
      "TestID\n",
      "817     8\n",
      "1903    8\n",
      "801     8\n",
      "508     8\n",
      "2518    8\n",
      "dtype: int64\n",
      "This infant will survive.  This infant has gestational age 33 weeks. Birth weight is 2630 grams.  No Bradycardia events.  It shows high variability.  Low amount of consecutive increases. \n"
     ]
    }
   ],
   "source": [
    "# -----------------Train Data-----------------\n",
    "df = pd.read_excel('../../data/PAS Challenge HR Data.xlsx', engine=\"calamine\")\n",
    "df.columns = df.columns.astype(str)\n",
    "df_y = pd.read_excel('../../data/PAS Challenge Outcome Data.xlsx', engine=\"calamine\")[['VitalID', 'DiedNICU', 'DeathAge']]\n",
    "df_demo = pd.read_excel('../../data/PAS Challenge Demographic Data.xlsx', engine=\"calamine\")\n",
    "df_x = pd.read_excel('../../data/PAS Challenge Model Data.xlsx', engine=\"calamine\")\n",
    "df = df.merge(df_x[['VitalID', 'VitalTime', 'Age']], on=['VitalID', 'VitalTime'], how='left')\n",
    "df = label_death7d(df, df_y, id_col='VitalID')\n",
    "df = df.merge(df_demo, on='VitalID', how='left')\n",
    "df_desc = generate_descriptions(ts_df = df.loc[:, '1':'300'], id_df = df.loc[:, ['VitalID', 'VitalTime']])\n",
    "df = df.merge(df_desc, on=['VitalID', 'VitalTime'], how='left')\n",
    "df = gen_text_input_column(df, config_dict['text_config'])\n",
    "df['rowid'] = df.index.to_series() \n",
    "df_train = df\n",
    "\n",
    "# -----------------Test Data-----------------\n",
    "df_y_test = pd.read_excel('../../data/Test Data/Test Demographic Key.xlsx', sheet_name=0, engine=\"calamine\")\n",
    "df_test = pd.read_excel('../../data/Test Data/Test HR Data.xlsx', sheet_name=0, engine=\"calamine\") # test hr with description\n",
    "df_test.columns = df_test.columns.astype(str)\n",
    "df_test = label_death7d(df_test, df_y_test, id_col='TestID')\n",
    "df_demo_test = pd.read_excel('../../data/Test Data/Test Demographic Data.xlsx', sheet_name=0, engine=\"calamine\")\n",
    "df_test = df_test.merge(df_demo_test, on='TestID', how='left')\n",
    "df_test['rowid'] = df_test.index.to_series()\n",
    "df_test['VitalTime'] = df_test['Age']*24*60*60 # convert to second since birth\n",
    "df_test['VitalTime'] = df_test['VitalTime'].astype(int)\n",
    "rename_dict = {'TestID': 'VitalID'}\n",
    "df_test = df_test.rename(columns=rename_dict)\n",
    "\n",
    "df_desc_test = generate_descriptions(ts_df = df_test.loc[:, '1':'300'], id_df = df_test.loc[:, ['VitalID', 'VitalTime']])\n",
    "df_test = df_test.merge(df_desc_test, on=['VitalID', 'VitalTime'], how='left')\n",
    "df_test = gen_text_input_column(df_test, config_dict['text_config'])\n",
    "df_test_org = df_test[df.columns]\n",
    "df_test, df_leftout = train_test_split(df_test_org, test_size=0.5, stratify=df_test_org['cl_event'], random_state=random_state) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884, 300)\n",
      "(741, 300)\n"
     ]
    }
   ],
   "source": [
    "txt_ls_org = ['This infant will die in 7 days. ', 'This infant will survive. ']\n",
    "y_col = 'cl_event'\n",
    "# ---- downsample negative class(es) ----\n",
    "# sample 5000 rows from df_test[df_test[y_col]==txt_ls_org[1]], without replacement\n",
    "neg_sample_size = 5000\n",
    "df_test_downsampled = df_test[df_test[y_col]==txt_ls_org[1]].sample(n=neg_sample_size, replace=False)\n",
    "df_test = pd.concat([df_test[df_test[y_col]==txt_ls_org[0]], df_test_downsampled])\n",
    "df_train_downsampled = df_train[df_train[y_col]==txt_ls_org[1]].sample(n=neg_sample_size, replace=False)\n",
    "df_train = pd.concat([df_train[df_train[y_col]==txt_ls_org[0]], df_train_downsampled])\n",
    "\n",
    "\n",
    "# ---- augment + balance train data----\n",
    "target_event_rate = len(df_test[df_test[y_col]==txt_ls_org[0]])/len(df_test)\n",
    "max_size = int(target_event_rate*len(df_train))\n",
    "if config_dict['ts_aug']:\n",
    "    df_train = augment_balance_data(df_train, \n",
    "                                    txt_ls_org, \n",
    "                                    y_col, \n",
    "                                    config_dict, \n",
    "                                    pretrained_model_path='./pretrained/hr_vae_linear_medium.pth', \n",
    "                                    K=10,\n",
    "                                    max_size=max_size)\n",
    "\n",
    "\n",
    "# ---- block or not ----\n",
    "# important for generating labels for block target\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "if not config_dict['block_target']:\n",
    "    df_train['label'] = df_train.index.to_series()\n",
    "    df_test['label'] = df_test.index.to_series()\n",
    "else:\n",
    "    df_train['label'] = df_train['rowid'].astype(int)\n",
    "    df_test['label'] = df_test['rowid'].astype(int)\n",
    "\n",
    "def get_y_true_and_ts_df(df_new, txt_ls_org):\n",
    "    df_new_y = pd.get_dummies(df_new['cl_event'])\n",
    "    df_new_y = df_new_y[txt_ls_org]\n",
    "    y_true = torch.tensor(df_new_y.values)\n",
    "    ts_df = df_new.loc[:,'1':'300']\n",
    "    return y_true, ts_df\n",
    "\n",
    "y_true_train, ts_df_train = get_y_true_and_ts_df(df_train, txt_ls_org)     # create y_true_train and ts_df_train\n",
    "y_true_test, ts_df_test = get_y_true_and_ts_df(df_test, txt_ls_org)     # create y_true_test and ts_df_test\n",
    "print(ts_df_train.shape)\n",
    "print(ts_df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All required objects are properly defined and non-empty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyqiu/Documents/Documents JoyQiu Work/Research/LLMTimeSeries/llm_nicu_vitalsigns/clip_env/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# ---- ready encoded dataloaders ---- \n",
    "def check_data_ready():\n",
    "    required_objects = {\n",
    "        'y_true_train': (pd.DataFrame, np.ndarray, torch.Tensor),\n",
    "        'ts_df_train': pd.DataFrame,\n",
    "        'y_true_test': (pd.DataFrame, np.ndarray, torch.Tensor),\n",
    "        'ts_df_test': pd.DataFrame,\n",
    "        'txt_ls': list\n",
    "    }\n",
    "    \n",
    "    for obj_name, expected_type in required_objects.items():\n",
    "        # Check if object exists in globals\n",
    "        if obj_name not in globals():\n",
    "            raise ValueError(f\"Missing required object: {obj_name}\")\n",
    "        \n",
    "        obj = globals()[obj_name]\n",
    "        # Check if object is None\n",
    "        if obj is None:\n",
    "            raise ValueError(f\"Object {obj_name} is None\")\n",
    "            \n",
    "        # Check type\n",
    "        if not isinstance(obj, expected_type):\n",
    "            raise TypeError(f\"{obj_name} should be of type {expected_type}, but got {type(obj)}\")\n",
    "        \n",
    "        # Check if empty\n",
    "        if hasattr(obj, '__len__') and len(obj) == 0:\n",
    "            raise ValueError(f\"{obj_name} is empty\")\n",
    "\n",
    "    print(\"âœ“ All required objects are properly defined and non-empty\")\n",
    "    return True\n",
    "try:\n",
    "    check_data_ready()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please define all required objects before proceeding\")\n",
    "\n",
    "\n",
    "ts_f_train, tx_f_train, labels_train = get_features(df_train,config_dict['ts_encoder_name'], \n",
    "                                                    config_dict['text_encoder_name'], \n",
    "                                                    config_dict['ts_normalize'],\n",
    "                                                    config_dict['ts_encode'])\n",
    "train_dataloader = CLIPDataset(ts_f_train, tx_f_train, labels_train).dataloader(batch_size=config_dict['batch_size'])\n",
    "ts_f_test, tx_f_test, labels_test = get_features(df_test, config_dict['ts_encoder_name'], \n",
    "                                                 config_dict['text_encoder_name'], \n",
    "                                                 config_dict['ts_normalize'],\n",
    "                                                 config_dict['ts_encode'])\n",
    "test_dataloader = CLIPDataset(ts_f_test, tx_f_test, labels_test).dataloader(batch_size=config_dict['batch_size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n",
      "Layer (type:depth-idx)                                       Param #\n",
      "=====================================================================================\n",
      "CLIPModel                                                    1\n",
      "â”œâ”€Sequential: 1-1                                            --\n",
      "â”‚    â””â”€Linear: 2-1                                           38,528\n",
      "â”‚    â””â”€BatchNorm1d: 2-2                                      256\n",
      "â”‚    â””â”€LeakyReLU: 2-3                                        --\n",
      "â”‚    â””â”€Dropout: 2-4                                          --\n",
      "â”‚    â””â”€ResidualBlock: 2-5                                    --\n",
      "â”‚    â”‚    â””â”€Sequential: 3-1                                  66,688\n",
      "â”‚    â”‚    â””â”€LeakyReLU: 3-2                                   --\n",
      "â”‚    â””â”€ResidualBlock: 2-6                                    --\n",
      "â”‚    â”‚    â””â”€Sequential: 3-3                                  66,688\n",
      "â”‚    â”‚    â””â”€LeakyReLU: 3-4                                   --\n",
      "â”‚    â””â”€ResidualBlock: 2-7                                    --\n",
      "â”‚    â”‚    â””â”€Sequential: 3-5                                  132,992\n",
      "â”‚    â”‚    â””â”€LeakyReLU: 3-6                                   --\n",
      "â”‚    â””â”€ResidualBlock: 2-8                                    --\n",
      "â”‚    â”‚    â””â”€Sequential: 3-7                                  132,992\n",
      "â”‚    â”‚    â””â”€LeakyReLU: 3-8                                   --\n",
      "â”‚    â””â”€ResidualBlock: 2-9                                    --\n",
      "â”‚    â”‚    â””â”€Sequential: 3-9                                  265,600\n",
      "â”‚    â”‚    â””â”€LeakyReLU: 3-10                                  --\n",
      "â”‚    â””â”€ResidualBlock: 2-10                                   --\n",
      "â”‚    â”‚    â””â”€Sequential: 3-11                                 265,600\n",
      "â”‚    â”‚    â””â”€LeakyReLU: 3-12                                  --\n",
      "â”‚    â””â”€ResidualBlock: 2-11                                   --\n",
      "â”‚    â”‚    â””â”€Sequential: 3-13                                 530,816\n",
      "â”‚    â”‚    â””â”€LeakyReLU: 3-14                                  --\n",
      "â”‚    â””â”€ResidualBlock: 2-12                                   --\n",
      "â”‚    â”‚    â””â”€Sequential: 3-15                                 530,816\n",
      "â”‚    â”‚    â””â”€LeakyReLU: 3-16                                  --\n",
      "â”‚    â””â”€Linear: 2-13                                          16,512\n",
      "â”œâ”€Sequential: 1-2                                            --\n",
      "â”‚    â””â”€Linear: 2-14                                          393,728\n",
      "â”‚    â””â”€LayerNorm: 2-15                                       1,024\n",
      "â”‚    â””â”€GELU: 2-16                                            --\n",
      "â”‚    â””â”€Dropout: 2-17                                         --\n",
      "â”‚    â””â”€TransformerBlock: 2-18                                --\n",
      "â”‚    â”‚    â””â”€LayerNorm: 3-17                                  1,024\n",
      "â”‚    â”‚    â””â”€MultiheadAttention: 3-18                         1,050,624\n",
      "â”‚    â”‚    â””â”€LayerNorm: 3-19                                  1,024\n",
      "â”‚    â”‚    â””â”€Sequential: 3-20                                 2,099,712\n",
      "â”‚    â””â”€TransformerBlock: 2-19                                --\n",
      "â”‚    â”‚    â””â”€LayerNorm: 3-21                                  1,024\n",
      "â”‚    â”‚    â””â”€MultiheadAttention: 3-22                         1,050,624\n",
      "â”‚    â”‚    â””â”€LayerNorm: 3-23                                  1,024\n",
      "â”‚    â”‚    â””â”€Sequential: 3-24                                 2,099,712\n",
      "â”‚    â””â”€TransformerBlock: 2-20                                --\n",
      "â”‚    â”‚    â””â”€LayerNorm: 3-25                                  1,024\n",
      "â”‚    â”‚    â””â”€MultiheadAttention: 3-26                         1,050,624\n",
      "â”‚    â”‚    â””â”€LayerNorm: 3-27                                  1,024\n",
      "â”‚    â”‚    â””â”€Sequential: 3-28                                 2,099,712\n",
      "â”‚    â””â”€Linear: 2-21                                          65,664\n",
      "=====================================================================================\n",
      "Total params: 11,965,057\n",
      "Trainable params: 11,965,057\n",
      "Non-trainable params: 0\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ---- ready data storage ---- \n",
    "import shutil\n",
    "if overwrite:\n",
    "    # remove the subfolder './results/'+model_name\n",
    "    if os.path.exists('./results/'+model_name):\n",
    "        shutil.rmtree('./results/'+model_name)\n",
    "    overwrite = False # reset overwrite to False\n",
    "\n",
    "if not os.path.exists('./results/'+model_name):\n",
    "    os.makedirs('./results/'+model_name)\n",
    "model_path = './results/'+model_name+'/model.pth' \n",
    "eval_path = './results/'+model_name+'/evals.pth'\n",
    "config_path = './results/'+model_name+'/config.pth'\n",
    "if not os.path.exists(config_path):\n",
    "    torch.save(config_dict, config_path)\n",
    "\n",
    "# ---- ready model ----\n",
    "ts_dim = TSFeature(df_train.loc[:1,'1':'300'], config_dict['ts_encoder_name'], \n",
    "                                                 config_dict['ts_normalize'],\n",
    "                                                 config_dict['ts_encode']).features.shape[1]\n",
    "tx_dim= TXTFeature(['test'],config_dict['text_encoder_name']).features.shape[1]\n",
    "model = CLIPModel(\n",
    "        ts_dim=ts_dim,\n",
    "        text_dim=tx_dim,\n",
    "        output_dim=config_dict['embedded_dim']\n",
    "    )\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config_dict['init_lr'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',           # Reduce LR when metric stops decreasing\n",
    "    factor=0.9,          # Multiply LR by this factor\n",
    "    patience=config_dict['patience'],          # Number of epochs to wait before reducing LR\n",
    "    min_lr=1e-20         # Don't reduce LR below this value\n",
    ")\n",
    "train_eval_metrics_list = []\n",
    "test_eval_metrics_list = []\n",
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]\n",
      "\tTraining Loss: 3.827505\n",
      "\tTesting Loss: 5.396325\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [2/100]\n",
      "\tTraining Loss: 3.846236\n",
      "\tTesting Loss: 5.209673\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [3/100]\n",
      "\tTraining Loss: 3.662640\n",
      "\tTesting Loss: 5.688996\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [4/100]\n",
      "\tTraining Loss: 3.505734\n",
      "\tTesting Loss: 6.131682\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [5/100]\n",
      "\tTraining Loss: 3.468042\n",
      "\tTesting Loss: 5.980513\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [6/100]\n",
      "\tTraining Loss: 3.455246\n",
      "\tTesting Loss: 6.039387\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [7/100]\n",
      "\tTraining Loss: 3.395776\n",
      "\tTesting Loss: 5.979971\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [8/100]\n",
      "\tTraining Loss: 3.209582\n",
      "\tTesting Loss: 6.833103\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [9/100]\n",
      "\tTraining Loss: 3.218774\n",
      "\tTesting Loss: 6.370430\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [10/100]\n",
      "\tTraining Loss: 3.065310\n",
      "\tTesting Loss: 6.462130\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [11/100]\n",
      "\tTraining Loss: 3.091133\n",
      "\tTesting Loss: 7.028344\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [12/100]\n",
      "\tTraining Loss: 3.080417\n",
      "\tTesting Loss: 6.173850\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [13/100]\n",
      "\tTraining Loss: 3.106021\n",
      "\tTesting Loss: 6.676570\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [14/100]\n",
      "\tTraining Loss: 2.984902\n",
      "\tTesting Loss: 7.233903\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [15/100]\n",
      "\tTraining Loss: 2.910335\n",
      "\tTesting Loss: 7.138088\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [16/100]\n",
      "\tTraining Loss: 2.814644\n",
      "\tTesting Loss: 7.081638\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [17/100]\n",
      "\tTraining Loss: 2.975642\n",
      "\tTesting Loss: 6.678174\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [18/100]\n",
      "\tTraining Loss: 2.847622\n",
      "\tTesting Loss: 7.316087\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [19/100]\n",
      "\tTraining Loss: 2.803694\n",
      "\tTesting Loss: 6.845910\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [20/100]\n",
      "\tTraining Loss: 2.763042\n",
      "\tTesting Loss: 7.441422\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [21/100]\n",
      "\tTraining Loss: 2.741855\n",
      "\tTesting Loss: 7.538310\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [22/100]\n",
      "\tTraining Loss: 2.664951\n",
      "\tTesting Loss: 7.213001\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [23/100]\n",
      "\tTraining Loss: 2.740750\n",
      "\tTesting Loss: 7.012583\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [24/100]\n",
      "\tTraining Loss: 2.705479\n",
      "\tTesting Loss: 7.015962\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [25/100]\n",
      "\tTraining Loss: 2.656383\n",
      "\tTesting Loss: 7.034514\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [26/100]\n",
      "\tTraining Loss: 2.595411\n",
      "\tTesting Loss: 8.935522\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [27/100]\n",
      "\tTraining Loss: 2.492689\n",
      "\tTesting Loss: 7.293181\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [28/100]\n",
      "\tTraining Loss: 2.502571\n",
      "\tTesting Loss: 7.587973\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [29/100]\n",
      "\tTraining Loss: 2.533188\n",
      "\tTesting Loss: 7.004519\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [30/100]\n",
      "\tTraining Loss: 2.377209\n",
      "\tTesting Loss: 7.164699\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [31/100]\n",
      "\tTraining Loss: 2.504830\n",
      "\tTesting Loss: 7.596882\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [32/100]\n",
      "\tTraining Loss: 2.490092\n",
      "\tTesting Loss: 8.052664\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [33/100]\n",
      "\tTraining Loss: 2.420358\n",
      "\tTesting Loss: 7.849253\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [34/100]\n",
      "\tTraining Loss: 2.443341\n",
      "\tTesting Loss: 7.978422\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [35/100]\n",
      "\tTraining Loss: 2.402762\n",
      "\tTesting Loss: 7.901091\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [36/100]\n",
      "\tTraining Loss: 2.370709\n",
      "\tTesting Loss: 8.371196\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [37/100]\n",
      "\tTraining Loss: 2.403588\n",
      "\tTesting Loss: 7.870939\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [38/100]\n",
      "\tTraining Loss: 2.437875\n",
      "\tTesting Loss: 8.243085\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [39/100]\n",
      "\tTraining Loss: 2.427940\n",
      "\tTesting Loss: 8.284822\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [40/100]\n",
      "\tTraining Loss: 2.419992\n",
      "\tTesting Loss: 7.828719\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [41/100]\n",
      "\tTraining Loss: 2.361741\n",
      "\tTesting Loss: 8.441148\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [42/100]\n",
      "\tTraining Loss: 2.290058\n",
      "\tTesting Loss: 8.064114\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [43/100]\n",
      "\tTraining Loss: 2.299612\n",
      "\tTesting Loss: 7.760921\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [44/100]\n",
      "\tTraining Loss: 2.169796\n",
      "\tTesting Loss: 8.667420\n",
      "\tLearning Rate: 0.000081000\n",
      "Epoch [45/100]\n",
      "\tTraining Loss: 2.171207\n",
      "\tTesting Loss: 7.673144\n",
      "\tLearning Rate: 0.000081000\n",
      "Epoch [46/100]\n",
      "\tTraining Loss: 2.176332\n",
      "\tTesting Loss: 8.757311\n",
      "\tLearning Rate: 0.000081000\n",
      "Epoch [47/100]\n",
      "\tTraining Loss: 2.151201\n",
      "\tTesting Loss: 8.426949\n",
      "\tLearning Rate: 0.000081000\n",
      "Epoch [48/100]\n",
      "\tTraining Loss: 2.208958\n",
      "\tTesting Loss: 8.403007\n",
      "\tLearning Rate: 0.000081000\n",
      "Epoch [49/100]\n",
      "\tTraining Loss: 2.180835\n",
      "\tTesting Loss: 8.189464\n",
      "\tLearning Rate: 0.000081000\n",
      "Epoch [50/100]\n",
      "\tTraining Loss: 2.085225\n",
      "\tTesting Loss: 8.299444\n",
      "\tLearning Rate: 0.000081000\n",
      "Epoch [51/100]\n",
      "\tTraining Loss: 2.061159\n",
      "\tTesting Loss: 8.488881\n",
      "\tLearning Rate: 0.000081000\n",
      "Epoch [52/100]\n",
      "\tTraining Loss: 2.051059\n",
      "\tTesting Loss: 8.034946\n",
      "\tLearning Rate: 0.000081000\n",
      "Epoch [53/100]\n",
      "\tTraining Loss: 2.047776\n",
      "\tTesting Loss: 8.909962\n",
      "\tLearning Rate: 0.000081000\n",
      "Epoch [54/100]\n",
      "\tTraining Loss: 1.982330\n",
      "\tTesting Loss: 8.422718\n",
      "\tLearning Rate: 0.000081000\n",
      "Epoch [55/100]\n",
      "\tTraining Loss: 1.945636\n",
      "\tTesting Loss: 8.869187\n",
      "\tLearning Rate: 0.000081000\n",
      "Epoch [56/100]\n",
      "\tTraining Loss: 1.964085\n",
      "\tTesting Loss: 8.304687\n",
      "\tLearning Rate: 0.000081000\n",
      "Epoch [57/100]\n",
      "\tTraining Loss: 1.971809\n",
      "\tTesting Loss: 8.950658\n",
      "\tLearning Rate: 0.000081000\n",
      "Epoch [58/100]\n",
      "\tTraining Loss: 1.966586\n",
      "\tTesting Loss: 7.910306\n",
      "\tLearning Rate: 0.000081000\n",
      "Epoch [59/100]\n",
      "\tTraining Loss: 2.038942\n",
      "\tTesting Loss: 9.296269\n",
      "\tLearning Rate: 0.000081000\n",
      "Epoch [60/100]\n",
      "\tTraining Loss: 1.900043\n",
      "\tTesting Loss: 7.869479\n",
      "\tLearning Rate: 0.000081000\n",
      "Epoch [61/100]\n",
      "\tTraining Loss: 2.028583\n",
      "\tTesting Loss: 9.326317\n",
      "\tLearning Rate: 0.000081000\n",
      "Epoch [62/100]\n",
      "\tTraining Loss: 1.950797\n",
      "\tTesting Loss: 7.888951\n",
      "\tLearning Rate: 0.000081000\n",
      "Epoch [63/100]\n",
      "\tTraining Loss: 1.979689\n",
      "\tTesting Loss: 8.629422\n",
      "\tLearning Rate: 0.000081000\n",
      "Epoch [64/100]\n",
      "\tTraining Loss: 1.859779\n",
      "\tTesting Loss: 9.222760\n",
      "\tLearning Rate: 0.000081000\n",
      "Epoch [65/100]\n",
      "\tTraining Loss: 1.820129\n",
      "\tTesting Loss: 9.162486\n",
      "\tLearning Rate: 0.000072900\n",
      "Epoch [66/100]\n",
      "\tTraining Loss: 1.877195\n",
      "\tTesting Loss: 8.528626\n",
      "\tLearning Rate: 0.000072900\n",
      "Epoch [67/100]\n",
      "\tTraining Loss: 1.884878\n",
      "\tTesting Loss: 9.132614\n",
      "\tLearning Rate: 0.000072900\n",
      "Epoch [68/100]\n",
      "\tTraining Loss: 1.948215\n",
      "\tTesting Loss: 8.869543\n",
      "\tLearning Rate: 0.000072900\n",
      "Epoch [69/100]\n",
      "\tTraining Loss: 1.856922\n",
      "\tTesting Loss: 8.949226\n",
      "\tLearning Rate: 0.000072900\n",
      "Epoch [70/100]\n",
      "\tTraining Loss: 1.821050\n",
      "\tTesting Loss: 8.877251\n",
      "\tLearning Rate: 0.000072900\n",
      "Epoch [71/100]\n",
      "\tTraining Loss: 1.849961\n",
      "\tTesting Loss: 8.615345\n",
      "\tLearning Rate: 0.000072900\n",
      "Epoch [72/100]\n",
      "\tTraining Loss: 1.717924\n",
      "\tTesting Loss: 8.858334\n",
      "\tLearning Rate: 0.000072900\n",
      "Epoch [73/100]\n",
      "\tTraining Loss: 1.734022\n",
      "\tTesting Loss: 8.961972\n",
      "\tLearning Rate: 0.000072900\n",
      "Epoch [74/100]\n",
      "\tTraining Loss: 1.805732\n",
      "\tTesting Loss: 8.199868\n",
      "\tLearning Rate: 0.000072900\n",
      "Epoch [75/100]\n",
      "\tTraining Loss: 1.786815\n",
      "\tTesting Loss: 8.803382\n",
      "\tLearning Rate: 0.000072900\n",
      "Epoch [76/100]\n",
      "\tTraining Loss: 1.750017\n",
      "\tTesting Loss: 8.413063\n",
      "\tLearning Rate: 0.000072900\n",
      "Epoch [77/100]\n",
      "\tTraining Loss: 1.713845\n",
      "\tTesting Loss: 8.840247\n",
      "\tLearning Rate: 0.000072900\n",
      "Epoch [78/100]\n",
      "\tTraining Loss: 1.712087\n",
      "\tTesting Loss: 8.825568\n",
      "\tLearning Rate: 0.000072900\n",
      "Epoch [79/100]\n",
      "\tTraining Loss: 1.629316\n",
      "\tTesting Loss: 8.974211\n",
      "\tLearning Rate: 0.000072900\n",
      "Epoch [80/100]\n",
      "\tTraining Loss: 1.609288\n",
      "\tTesting Loss: 8.961068\n",
      "\tLearning Rate: 0.000072900\n",
      "Epoch [81/100]\n",
      "\tTraining Loss: 1.583531\n",
      "\tTesting Loss: 9.713060\n",
      "\tLearning Rate: 0.000072900\n",
      "Epoch [82/100]\n",
      "\tTraining Loss: 1.664162\n",
      "\tTesting Loss: 8.757136\n",
      "\tLearning Rate: 0.000072900\n",
      "Epoch [83/100]\n",
      "\tTraining Loss: 1.665795\n",
      "\tTesting Loss: 8.968095\n",
      "\tLearning Rate: 0.000072900\n",
      "Epoch [84/100]\n",
      "\tTraining Loss: 1.595860\n",
      "\tTesting Loss: 9.329195\n",
      "\tLearning Rate: 0.000072900\n",
      "Epoch [85/100]\n",
      "\tTraining Loss: 1.631494\n",
      "\tTesting Loss: 9.005831\n",
      "\tLearning Rate: 0.000072900\n",
      "Epoch [86/100]\n",
      "\tTraining Loss: 1.607057\n",
      "\tTesting Loss: 9.053238\n",
      "\tLearning Rate: 0.000065610\n",
      "Epoch [87/100]\n",
      "\tTraining Loss: 1.591469\n",
      "\tTesting Loss: 9.764250\n",
      "\tLearning Rate: 0.000065610\n",
      "Epoch [88/100]\n",
      "\tTraining Loss: 1.525112\n",
      "\tTesting Loss: 8.690629\n",
      "\tLearning Rate: 0.000065610\n",
      "Epoch [89/100]\n",
      "\tTraining Loss: 1.575621\n",
      "\tTesting Loss: 9.348887\n",
      "\tLearning Rate: 0.000065610\n",
      "Epoch [90/100]\n",
      "\tTraining Loss: 1.498033\n",
      "\tTesting Loss: 9.445587\n",
      "\tLearning Rate: 0.000065610\n",
      "Epoch [91/100]\n",
      "\tTraining Loss: 1.500379\n",
      "\tTesting Loss: 9.212198\n",
      "\tLearning Rate: 0.000065610\n",
      "Epoch [92/100]\n",
      "\tTraining Loss: 1.507367\n",
      "\tTesting Loss: 9.989393\n",
      "\tLearning Rate: 0.000065610\n",
      "Epoch [93/100]\n",
      "\tTraining Loss: 1.493330\n",
      "\tTesting Loss: 9.403367\n",
      "\tLearning Rate: 0.000065610\n",
      "Epoch [94/100]\n",
      "\tTraining Loss: 1.501685\n",
      "\tTesting Loss: 9.594506\n",
      "\tLearning Rate: 0.000065610\n",
      "Epoch [95/100]\n",
      "\tTraining Loss: 1.486250\n",
      "\tTesting Loss: 9.205672\n",
      "\tLearning Rate: 0.000065610\n",
      "Epoch [96/100]\n",
      "\tTraining Loss: 1.480008\n",
      "\tTesting Loss: 9.681090\n",
      "\tLearning Rate: 0.000065610\n",
      "Epoch [97/100]\n",
      "\tTraining Loss: 1.452205\n",
      "\tTesting Loss: 9.398537\n",
      "\tLearning Rate: 0.000065610\n",
      "Epoch [98/100]\n",
      "\tTraining Loss: 1.434946\n",
      "\tTesting Loss: 9.514969\n",
      "\tLearning Rate: 0.000065610\n",
      "Epoch [99/100]\n",
      "\tTraining Loss: 1.393386\n",
      "\tTesting Loss: 9.364412\n",
      "\tLearning Rate: 0.000065610\n",
      "Epoch [100/100]\n",
      "\tTraining Loss: 1.452767\n",
      "\tTesting Loss: 9.427721\n",
      "\tLearning Rate: 0.000065610\n",
      "----------------------------------------------------------------------\n",
      "Metric     |  Training  |  Testing\n",
      "----------------------------------------------------------------------\n",
      "F1        |   0.139   |   0.127\n",
      "Precision |   0.197   |   0.154\n",
      "Recall    |   0.107   |   0.108\n",
      "AUROC     |   0.301   |   0.349\n",
      "AUPRC     |   0.322   |   0.252\n",
      "----------------------------------------------------------------------\n",
      "Epoch [1/100]\n",
      "\tTraining Loss: 1.434292\n",
      "\tTesting Loss: 9.379814\n",
      "\tLearning Rate: 0.000065610\n",
      "Epoch [2/100]\n",
      "\tTraining Loss: 1.430434\n",
      "\tTesting Loss: 9.985066\n",
      "\tLearning Rate: 0.000065610\n",
      "Epoch [3/100]\n",
      "\tTraining Loss: 1.454251\n",
      "\tTesting Loss: 9.100808\n",
      "\tLearning Rate: 0.000065610\n",
      "Epoch [4/100]\n",
      "\tTraining Loss: 1.411181\n",
      "\tTesting Loss: 9.764014\n",
      "\tLearning Rate: 0.000065610\n",
      "Epoch [5/100]\n",
      "\tTraining Loss: 1.361520\n",
      "\tTesting Loss: 9.033571\n",
      "\tLearning Rate: 0.000065610\n",
      "Epoch [6/100]\n",
      "\tTraining Loss: 1.462073\n",
      "\tTesting Loss: 9.712053\n",
      "\tLearning Rate: 0.000065610\n",
      "Epoch [7/100]\n",
      "\tTraining Loss: 1.388208\n",
      "\tTesting Loss: 10.607869\n",
      "\tLearning Rate: 0.000059049\n",
      "Epoch [8/100]\n",
      "\tTraining Loss: 1.344864\n",
      "\tTesting Loss: 9.754517\n",
      "\tLearning Rate: 0.000059049\n",
      "Epoch [9/100]\n",
      "\tTraining Loss: 1.307466\n",
      "\tTesting Loss: 9.711851\n",
      "\tLearning Rate: 0.000059049\n",
      "Epoch [10/100]\n",
      "\tTraining Loss: 1.325691\n",
      "\tTesting Loss: 10.836261\n",
      "\tLearning Rate: 0.000059049\n",
      "Epoch [11/100]\n",
      "\tTraining Loss: 1.344406\n",
      "\tTesting Loss: 9.464751\n",
      "\tLearning Rate: 0.000059049\n",
      "Epoch [12/100]\n",
      "\tTraining Loss: 1.340567\n",
      "\tTesting Loss: 10.315694\n",
      "\tLearning Rate: 0.000059049\n",
      "Epoch [13/100]\n",
      "\tTraining Loss: 1.312741\n",
      "\tTesting Loss: 9.269658\n",
      "\tLearning Rate: 0.000059049\n",
      "Epoch [14/100]\n",
      "\tTraining Loss: 1.336976\n",
      "\tTesting Loss: 9.914862\n",
      "\tLearning Rate: 0.000059049\n",
      "Epoch [15/100]\n",
      "\tTraining Loss: 1.307187\n",
      "\tTesting Loss: 10.824551\n",
      "\tLearning Rate: 0.000059049\n",
      "Epoch [16/100]\n",
      "\tTraining Loss: 1.263788\n",
      "\tTesting Loss: 9.830077\n",
      "\tLearning Rate: 0.000059049\n",
      "Epoch [17/100]\n",
      "\tTraining Loss: 1.240317\n",
      "\tTesting Loss: 9.675945\n",
      "\tLearning Rate: 0.000059049\n",
      "Epoch [18/100]\n",
      "\tTraining Loss: 1.315529\n",
      "\tTesting Loss: 9.813883\n",
      "\tLearning Rate: 0.000059049\n",
      "\n",
      "Training interrupted by user. Saving current progress...\n",
      "----------------------------------------------------------------------\n",
      "Metric     |  Training  |  Testing\n",
      "----------------------------------------------------------------------\n",
      "F1        |   0.101   |   0.074\n",
      "Precision |   0.202   |   0.119\n",
      "Recall    |   0.068   |   0.054\n",
      "AUROC     |   0.379   |   0.432\n",
      "AUPRC     |   0.350   |   0.274\n",
      "----------------------------------------------------------------------\n",
      "Epoch [1/100]\n",
      "\tTraining Loss: 1.326143\n",
      "\tTesting Loss: 9.488615\n",
      "\tLearning Rate: 0.000059049\n",
      "Epoch [2/100]\n",
      "\tTraining Loss: 1.212969\n",
      "\tTesting Loss: 10.235800\n",
      "\tLearning Rate: 0.000059049\n",
      "Epoch [3/100]\n",
      "\tTraining Loss: 1.243627\n",
      "\tTesting Loss: 10.070794\n",
      "\tLearning Rate: 0.000059049\n",
      "Epoch [4/100]\n",
      "\tTraining Loss: 1.272419\n",
      "\tTesting Loss: 9.524966\n",
      "\tLearning Rate: 0.000059049\n",
      "Epoch [5/100]\n",
      "\tTraining Loss: 1.253944\n",
      "\tTesting Loss: 10.272397\n",
      "\tLearning Rate: 0.000059049\n",
      "Epoch [6/100]\n",
      "\tTraining Loss: 1.223665\n",
      "\tTesting Loss: 9.082382\n",
      "\tLearning Rate: 0.000059049\n",
      "Epoch [7/100]\n",
      "\tTraining Loss: 1.218768\n",
      "\tTesting Loss: 10.885924\n",
      "\tLearning Rate: 0.000059049\n",
      "Epoch [8/100]\n",
      "\tTraining Loss: 1.200619\n",
      "\tTesting Loss: 9.541077\n",
      "\tLearning Rate: 0.000059049\n",
      "Epoch [9/100]\n",
      "\tTraining Loss: 1.172502\n",
      "\tTesting Loss: 10.064028\n",
      "\tLearning Rate: 0.000059049\n",
      "Epoch [10/100]\n",
      "\tTraining Loss: 1.129686\n",
      "\tTesting Loss: 10.350180\n",
      "\tLearning Rate: 0.000053144\n"
     ]
    }
   ],
   "source": [
    "if overwrite or not os.path.exists(model_path):\n",
    "    \n",
    "    for i in range(config_dict['num_saves']): \n",
    "        train_losses_tmp, test_losses_tmp = train(model, \n",
    "                                                train_dataloader,\n",
    "                                                test_dataloader, \n",
    "                                                optimizer, \n",
    "                                                scheduler,\n",
    "                                                config_dict['num_epochs'], \n",
    "                                                device,\n",
    "                                                config_dict['loss_type'])\n",
    "        train_losses = train_losses + train_losses_tmp\n",
    "        test_losses = test_losses + test_losses_tmp\n",
    "        # every num_epochs, evaluate the model\n",
    "        train_eval_metrics = eval_model(model, y_true_train, ts_df_train, config_dict['txt_ls'], \n",
    "                                        config_dict['ts_encoder_name'], config_dict['text_encoder_name'],\n",
    "                                        config_dict['ts_normalize'], config_dict['ts_encode'])\n",
    "        test_eval_metrics = eval_model(model, y_true_test, ts_df_test, config_dict['txt_ls'], \n",
    "                                       config_dict['ts_encoder_name'], config_dict['text_encoder_name'],\n",
    "                                       config_dict['ts_normalize'], config_dict['ts_encode'])\n",
    "        train_eval_metrics_list.append(train_eval_metrics)\n",
    "        test_eval_metrics_list.append(test_eval_metrics)\n",
    "        # save model and losses\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        torch.save({\n",
    "            'train_losses': train_losses,\n",
    "            'test_losses': test_losses,\n",
    "            'train_evals': train_eval_metrics_list,\n",
    "            'test_evals': test_eval_metrics_list }, eval_path)\n",
    "        # if i % 10 == 0: # every 10 saves, evaluate the model\n",
    "        eval_dict = torch.load(eval_path)\n",
    "        eval_dict_eng = eng_eval_metrics(eval_dict, binary=True)\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"Metric     |  Training  |  Testing\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"F1        |   {eval_dict_eng['train_f1'][-1]:.3f}   |   {eval_dict_eng['test_f1'][-1]:.3f}\")\n",
    "        print(f\"Precision |   {eval_dict_eng['train_precision'][-1]:.3f}   |   {eval_dict_eng['test_precision'][-1]:.3f}\")\n",
    "        print(f\"Recall    |   {eval_dict_eng['train_recall'][-1]:.3f}   |   {eval_dict_eng['test_recall'][-1]:.3f}\")\n",
    "        print(f\"AUROC     |   {eval_dict_eng['train_auroc'][-1]:.3f}   |   {eval_dict_eng['test_auroc'][-1]:.3f}\")\n",
    "        print(f\"AUPRC     |   {eval_dict_eng['train_auprc'][-1]:.3f}   |   {eval_dict_eng['test_auprc'][-1]:.3f}\")\n",
    "        print(\"-\" * 70)\n",
    "    \n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    eval_dict = torch.load(eval_path)\n",
    "    train_losses = eval_dict['train_losses']\n",
    "    test_losses = eval_dict['test_losses']\n",
    "    train_eval_metrics_list = eval_dict['train_evals']\n",
    "    test_eval_metrics_list = eval_dict['test_evals']\n",
    "    eval_dict_eng = eng_eval_metrics(eval_dict, binary=True, plot_confusion_matrices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
