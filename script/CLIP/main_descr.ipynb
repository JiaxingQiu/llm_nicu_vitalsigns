{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "from encoders import *\n",
    "from data import *\n",
    "from models import *\n",
    "from evals import *\n",
    "print(\"using device: \", device)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for debugging\n",
    "# import importlib\n",
    "# import models\n",
    "# importlib.reload(models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ugly Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ugly engineering here\n",
    "df = pd.read_csv('../../data/HR_events.csv')\n",
    "df_y = pd.read_excel('../../data/PAS Challenge Outcome Data.xlsx', engine=\"calamine\")\n",
    "df_y = df_y[['VitalID', 'Died']]\n",
    "df = df.merge(df_y, on='VitalID', how='left')\n",
    "df['label'] = df.index.to_series()\n",
    "df['text'] = df['Died'].apply(lambda x: 'This infant will die in 7 days. ' if x == 1 else 'This infant will survive. ')\n",
    "# df['text'] = df['text'] +' '+ df['event_description'].astype(str)\n",
    "\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, stratify=df['text'])    \n",
    "df_train['text'] = df_train['text'] +' '+ df_train['event_description'].astype(str)\n",
    "df_test['text'] = df_test['text'] +' '+ df_test['event_description'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "df_new = df_train\n",
    "df_new['outcome'] = df_new['Died'].apply(lambda x: 'class1' if x == 1 else 'class2')\n",
    "df_new_y = pd.get_dummies(df_new['outcome'])\n",
    "y_true_train = torch.tensor(df_new_y.values)\n",
    "ts_df_train = df_new.loc[:,'1':'300']\n",
    "\n",
    "\n",
    "df_new = df_test\n",
    "df_new['outcome'] = df_new['Died'].apply(lambda x: 'class1' if x == 1 else 'class2')\n",
    "df_new_y = pd.get_dummies(df_new['outcome'])\n",
    "y_true_test = torch.tensor(df_new_y.values)\n",
    "ts_df_test = df_new.loc[:,'1':'300']\n",
    "\n",
    "txt_ls = ['die', 'survive']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All required objects are properly defined and non-empty\n"
     ]
    }
   ],
   "source": [
    "# ---- config that need to be changed ----\n",
    "overwrite = True\n",
    "model_name = 'clip_hr_death'\n",
    "\n",
    "\n",
    "def check_data_ready():\n",
    "    required_objects = {\n",
    "        'y_true_train': (pd.DataFrame, np.ndarray, torch.Tensor),\n",
    "        'ts_df_train': pd.DataFrame,\n",
    "        'y_true_test': (pd.DataFrame, np.ndarray, torch.Tensor),\n",
    "        'ts_df_test': pd.DataFrame,\n",
    "        'txt_ls': list\n",
    "    }\n",
    "    \n",
    "    for obj_name, expected_type in required_objects.items():\n",
    "        # Check if object exists in globals\n",
    "        if obj_name not in globals():\n",
    "            raise ValueError(f\"Missing required object: {obj_name}\")\n",
    "        \n",
    "        obj = globals()[obj_name]\n",
    "        # Check if object is None\n",
    "        if obj is None:\n",
    "            raise ValueError(f\"Object {obj_name} is None\")\n",
    "            \n",
    "        # Check type\n",
    "        if not isinstance(obj, expected_type):\n",
    "            raise TypeError(f\"{obj_name} should be of type {expected_type}, but got {type(obj)}\")\n",
    "        \n",
    "        # Check if empty\n",
    "        if hasattr(obj, '__len__') and len(obj) == 0:\n",
    "            raise ValueError(f\"{obj_name} is empty\")\n",
    "\n",
    "    print(\"✓ All required objects are properly defined and non-empty\")\n",
    "    return True\n",
    "try:\n",
    "    check_data_ready()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please define all required objects before proceeding\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "CLIPModel                                1\n",
      "├─Sequential: 1-1                        --\n",
      "│    └─Linear: 2-1                       4,224\n",
      "│    └─LeakyReLU: 2-2                    --\n",
      "│    └─Linear: 2-3                       33,024\n",
      "│    └─LeakyReLU: 2-4                    --\n",
      "│    └─Linear: 2-5                       32,896\n",
      "│    └─LeakyReLU: 2-6                    --\n",
      "│    └─Linear: 2-7                       16,512\n",
      "├─Sequential: 1-2                        --\n",
      "│    └─Linear: 2-8                       98,432\n",
      "│    └─LeakyReLU: 2-9                    --\n",
      "│    └─Linear: 2-10                      33,024\n",
      "│    └─LeakyReLU: 2-11                   --\n",
      "│    └─Linear: 2-12                      65,792\n",
      "│    └─LeakyReLU: 2-13                   --\n",
      "│    └─Linear: 2-14                      32,896\n",
      "│    └─LeakyReLU: 2-15                   --\n",
      "│    └─Linear: 2-16                      16,512\n",
      "=================================================================\n",
      "Total params: 333,313\n",
      "Trainable params: 333,313\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- ready dataloader ---- \n",
    "ts_f_train, tx_f_train, labels_train = get_features(df_train,ts_encoder_name,text_encoder_name)\n",
    "train_dataloader = CLIPDataset(ts_f_train, tx_f_train, labels_train).dataloader(batch_size=batch_size)\n",
    "ts_f_test, tx_f_test, labels_test = get_features(df_test, ts_encoder_name,text_encoder_name)\n",
    "test_dataloader = CLIPDataset(ts_f_test, tx_f_test, labels_test).dataloader(batch_size=batch_size)\n",
    "\n",
    "# ---- ready model ---- \n",
    "model_path = './results/'+model_name+'_'+loss_type+'.pth' \n",
    "eval_path = './results/'+model_name+'_'+loss_type+'_evals.pth'\n",
    "# Initialize model\n",
    "model = CLIPModel(\n",
    "        ts_dim=ts_f_train.shape[1],    # 32\n",
    "        text_dim=tx_f_train.shape[1],  # 768\n",
    "        projection_dim=embedded_dim\n",
    "    )\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=init_lr)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',           # Reduce LR when metric stops decreasing\n",
    "    factor=0.9,          # Multiply LR by this factor\n",
    "    patience=patience,          # Number of epochs to wait before reducing LR\n",
    "    min_lr=1e-20         # Don't reduce LR below this value\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "\tTraining Loss: 4.803387\n",
      "\tTesting Loss: 4.813509\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 4.802964\n",
      "\tTesting Loss: 4.813450\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 4.802935\n",
      "\tTesting Loss: 4.813432\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 4.802928\n",
      "\tTesting Loss: 4.813428\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 4.802924\n",
      "\tTesting Loss: 4.813427\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 4.802922\n",
      "\tTesting Loss: 4.813425\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 4.802919\n",
      "\tTesting Loss: 4.813423\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 4.802917\n",
      "\tTesting Loss: 4.813422\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 4.802915\n",
      "\tTesting Loss: 4.813420\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 4.802912\n",
      "\tTesting Loss: 4.813419\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 4.802911\n",
      "\tTesting Loss: 4.813418\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 4.802908\n",
      "\tTesting Loss: 4.813417\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 4.802905\n",
      "\tTesting Loss: 4.813417\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 4.802902\n",
      "\tTesting Loss: 4.813416\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 4.802899\n",
      "\tTesting Loss: 4.813414\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 4.802895\n",
      "\tTesting Loss: 4.813413\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 4.802890\n",
      "\tTesting Loss: 4.813411\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 4.802883\n",
      "\tTesting Loss: 4.813408\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 4.802875\n",
      "\tTesting Loss: 4.813406\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 4.802864\n",
      "\tTesting Loss: 4.813402\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 4.802849\n",
      "\tTesting Loss: 4.813398\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 4.802826\n",
      "\tTesting Loss: 4.813390\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [23/50]\n",
      "\tTraining Loss: 4.802789\n",
      "\tTesting Loss: 4.813390\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [24/50]\n",
      "\tTraining Loss: 4.802743\n",
      "\tTesting Loss: 4.813380\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [25/50]\n",
      "\tTraining Loss: 4.802697\n",
      "\tTesting Loss: 4.813421\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [26/50]\n",
      "\tTraining Loss: 4.802605\n",
      "\tTesting Loss: 4.813560\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [27/50]\n",
      "\tTraining Loss: 4.802665\n",
      "\tTesting Loss: 4.813449\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [28/50]\n",
      "\tTraining Loss: 4.802512\n",
      "\tTesting Loss: 4.813347\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [29/50]\n",
      "\tTraining Loss: 4.802233\n",
      "\tTesting Loss: 4.813634\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [30/50]\n",
      "\tTraining Loss: 4.802928\n",
      "\tTesting Loss: 4.813409\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [31/50]\n",
      "\tTraining Loss: 4.802029\n",
      "\tTesting Loss: 4.813354\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [32/50]\n",
      "\tTraining Loss: 4.801663\n",
      "\tTesting Loss: 4.814371\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [33/50]\n",
      "\tTraining Loss: 4.801594\n",
      "\tTesting Loss: 4.813488\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [34/50]\n",
      "\tTraining Loss: 4.801077\n",
      "\tTesting Loss: 4.813439\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [35/50]\n",
      "\tTraining Loss: 4.800615\n",
      "\tTesting Loss: 4.816671\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [36/50]\n",
      "\tTraining Loss: 4.800516\n",
      "\tTesting Loss: 4.814872\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [37/50]\n",
      "\tTraining Loss: 4.799356\n",
      "\tTesting Loss: 4.813966\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [38/50]\n",
      "\tTraining Loss: 4.803889\n",
      "\tTesting Loss: 4.818537\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [39/50]\n",
      "\tTraining Loss: 4.800780\n",
      "\tTesting Loss: 4.813853\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [40/50]\n",
      "\tTraining Loss: 4.799835\n",
      "\tTesting Loss: 4.813780\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [41/50]\n",
      "\tTraining Loss: 4.798516\n",
      "\tTesting Loss: 4.814050\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [42/50]\n",
      "\tTraining Loss: 4.796552\n",
      "\tTesting Loss: 4.815081\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [43/50]\n",
      "\tTraining Loss: 4.803496\n",
      "\tTesting Loss: 4.815322\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [44/50]\n",
      "\tTraining Loss: 4.798963\n",
      "\tTesting Loss: 4.814107\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [45/50]\n",
      "\tTraining Loss: 4.797770\n",
      "\tTesting Loss: 4.814437\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [46/50]\n",
      "\tTraining Loss: 4.796246\n",
      "\tTesting Loss: 4.815213\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [47/50]\n",
      "\tTraining Loss: 4.794621\n",
      "\tTesting Loss: 4.818659\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [48/50]\n",
      "\tTraining Loss: 4.797090\n",
      "\tTesting Loss: 4.818990\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [49/50]\n",
      "\tTraining Loss: 4.794356\n",
      "\tTesting Loss: 4.817019\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [50/50]\n",
      "\tTraining Loss: 4.792822\n",
      "\tTesting Loss: 4.818064\n",
      "\tLearning Rate: 0.000100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "\tTraining Loss: 4.805924\n",
      "\tTesting Loss: 4.828467\n",
      "\tLearning Rate: 0.000100000\n",
      "Epoch [2/50]\n",
      "\tTraining Loss: 4.798311\n",
      "\tTesting Loss: 4.816089\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [3/50]\n",
      "\tTraining Loss: 4.795211\n",
      "\tTesting Loss: 4.815814\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [4/50]\n",
      "\tTraining Loss: 4.793763\n",
      "\tTesting Loss: 4.815940\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [5/50]\n",
      "\tTraining Loss: 4.792291\n",
      "\tTesting Loss: 4.816978\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [6/50]\n",
      "\tTraining Loss: 4.790884\n",
      "\tTesting Loss: 4.818792\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [7/50]\n",
      "\tTraining Loss: 4.789606\n",
      "\tTesting Loss: 4.819715\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [8/50]\n",
      "\tTraining Loss: 4.788645\n",
      "\tTesting Loss: 4.821910\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [9/50]\n",
      "\tTraining Loss: 4.801335\n",
      "\tTesting Loss: 4.827511\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [10/50]\n",
      "\tTraining Loss: 4.793592\n",
      "\tTesting Loss: 4.817952\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [11/50]\n",
      "\tTraining Loss: 4.790599\n",
      "\tTesting Loss: 4.818440\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [12/50]\n",
      "\tTraining Loss: 4.789269\n",
      "\tTesting Loss: 4.820134\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [13/50]\n",
      "\tTraining Loss: 4.788314\n",
      "\tTesting Loss: 4.821882\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [14/50]\n",
      "\tTraining Loss: 4.787726\n",
      "\tTesting Loss: 4.823392\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [15/50]\n",
      "\tTraining Loss: 4.788011\n",
      "\tTesting Loss: 4.828178\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [16/50]\n",
      "\tTraining Loss: 4.788208\n",
      "\tTesting Loss: 4.824902\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [17/50]\n",
      "\tTraining Loss: 4.788476\n",
      "\tTesting Loss: 4.825901\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [18/50]\n",
      "\tTraining Loss: 4.787259\n",
      "\tTesting Loss: 4.827395\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [19/50]\n",
      "\tTraining Loss: 4.786541\n",
      "\tTesting Loss: 4.826794\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [20/50]\n",
      "\tTraining Loss: 4.785633\n",
      "\tTesting Loss: 4.827679\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [21/50]\n",
      "\tTraining Loss: 4.786285\n",
      "\tTesting Loss: 4.826411\n",
      "\tLearning Rate: 0.000090000\n",
      "Epoch [22/50]\n",
      "\tTraining Loss: 4.786121\n",
      "\tTesting Loss: 4.830805\n",
      "\tLearning Rate: 0.000090000\n"
     ]
    }
   ],
   "source": [
    "# ---- train and eval ---- \n",
    "if overwrite:\n",
    "    # remove model_path, loss_path, eval_metrics_path\n",
    "    if os.path.exists(model_path):\n",
    "        os.remove(model_path)\n",
    "    if os.path.exists(eval_path):\n",
    "        os.remove(eval_path)\n",
    "    overwrite = False # reset overwrite to False\n",
    "if overwrite or not os.path.exists(model_path):\n",
    "    for i in range(num_saves): \n",
    "        train_losses_tmp, test_losses_tmp = train(model, \n",
    "                                                train_dataloader,\n",
    "                                                test_dataloader, \n",
    "                                                optimizer, \n",
    "                                                scheduler,\n",
    "                                                num_epochs, \n",
    "                                                device,\n",
    "                                                loss_type)\n",
    "        train_losses = train_losses + train_losses_tmp\n",
    "        test_losses = test_losses + test_losses_tmp\n",
    "        # every num_epochs, evaluate the model\n",
    "        train_eval_metrics = eval_model(model, y_true_train, ts_df_train, txt_ls, ts_encoder_name, text_encoder_name)\n",
    "        test_eval_metrics = eval_model(model, y_true_test, ts_df_test, txt_ls, ts_encoder_name, text_encoder_name)\n",
    "        train_eval_metrics_list.append(train_eval_metrics)\n",
    "        test_eval_metrics_list.append(test_eval_metrics)\n",
    "        # save model and losses\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        torch.save({\n",
    "            'train_losses': train_losses,\n",
    "            'test_losses': test_losses,\n",
    "            'train_evals': train_eval_metrics_list,\n",
    "            'test_evals': test_eval_metrics_list }, eval_path)\n",
    "        if i % 10 == 0: # every 10 saves, evaluate the model\n",
    "            eval_dict = torch.load(eval_path)\n",
    "            eval_dict_eng = eng_eval_metrics(eval_dict, binary=True, plot=True)\n",
    "    \n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    eval_dict = torch.load(eval_path)\n",
    "    eval_dict_eng = eng_eval_metrics(eval_dict, binary=True, plot_confusion_matrices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
