{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f95983df-c3ee-4d76-8289-930a0aa224ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sfs/gpfs/tardis/home/jq2uw/llm_nicu_vitalsigns/script'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "os.chdir(\"../../\") # set to llm_nicu_vitalsigns/script/\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd62a0d-b4a8-476c-acf9-037c33fed94d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_median_bounds(cell):\n",
    "    match = re.match(r\"([-\\d.]+)\\s*\\[\\s*([-\\d.]+),\\s*([-\\d.]+)\\s*\\]\", str(cell))\n",
    "    if match:\n",
    "        median = float(match.group(1))\n",
    "        lower = float(match.group(2))\n",
    "        upper = float(match.group(3))\n",
    "        return median, lower, upper\n",
    "    return None, None, None\n",
    "\n",
    "\n",
    "def get_w_evals(w: float, attr_suffix: str) -> pd.DataFrame:\n",
    "    dataset_names = {\n",
    "        'syn_gt': 'Synthetic w/ ground truth',\n",
    "        'syn'   : 'Synthetic',\n",
    "        'air'   : 'Air Quality',\n",
    "        'nicu' : 'NICU Heart Rate'\n",
    "    }\n",
    "\n",
    "    dfs = []\n",
    "    for k, nice_name in dataset_names.items():\n",
    "        try:\n",
    "            # --- load 4 result files -------------------------------------------------\n",
    "            base   = f\"{k}{attr_suffix}\"\n",
    "            it     = pd.read_csv(f\"./VITAL/results/{base}_self/res_df_iqr{w}.csv\")\n",
    "            it_open= pd.read_csv(f\"./VITAL/results/{base}_open/res_df_iqr{w}.csv\")\n",
    "\n",
    "            te_dir = \"tedit_lite\" if attr_suffix == '_at' else \"tedit_lite_tx\"\n",
    "            te     = pd.read_csv(f\"./{te_dir}/tedit_save/te/{k}/res_df_iqr.csv\")\n",
    "            tw     = pd.read_csv(f\"./{te_dir}/tedit_save/tw/{k}/res_df_iqr.csv\")\n",
    "\n",
    "            # --- add model column ----------------------------------------------------\n",
    "            it['Model']      = 'InstructTime'\n",
    "            it_open['Model'] = 'InstructTime (open-vocab)'\n",
    "            te['Model']      = 'TEdit'\n",
    "            tw['Model']      = 'Time Weaver'\n",
    "            if attr_suffix == \"_at\":\n",
    "                res = pd.concat([it, te, tw], ignore_index=True)\n",
    "            else:\n",
    "                res = pd.concat([it, it_open, te, tw], ignore_index=True)\n",
    "\n",
    "            # --- pull out median / lower / upper as separate rows --------------------\n",
    "            q25, q50, q75 = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "            metric_cols = [c for c in res.columns if c != 'Model']\n",
    "\n",
    "            for col in metric_cols:\n",
    "                med, lo, up = [], [], []\n",
    "                for cell in res[col]:\n",
    "                    m, l, u = parse_median_bounds(cell)\n",
    "                    med.append(m); lo.append(l); up.append(u)\n",
    "                q50[col], q25[col], q75[col] = med, lo, up\n",
    "\n",
    "            for df, q in zip([q25, q50, q75], [25, 50, 75]):\n",
    "                df['Model']     = res['Model']\n",
    "                df['quantile']  = q\n",
    "\n",
    "            out = pd.concat([q25, q50, q75], ignore_index=True)\n",
    "            out['dataset'] = nice_name          # human-readable!\n",
    "            dfs.append(out)\n",
    "\n",
    "        except FileNotFoundError as err:          # capture the exception ⇢ err\n",
    "            print(f\"skip {k}: {err}\")            # full message, e.g. “[Errno 2] …/file.csv: No such file or directory”\n",
    "            continue\n",
    "\n",
    "    big = pd.concat(dfs, ignore_index=True)\n",
    "    big['w'] = w\n",
    "    return big\n",
    "\n",
    "\n",
    "def plot_metric(df: pd.DataFrame,\n",
    "                metric: str,\n",
    "                dot_models = ('InstructTime', 'InstructTime (open-vocab)'),\n",
    "                line_models= ('TEdit', 'Time Weaver'),\n",
    "                jitter     = 0.01,\n",
    "                yscale     = 'linear',\n",
    "                figsize    = (5,3)):\n",
    "    \"\"\"\n",
    "    Draw one row of subplots (one per dataset) for the given metric.\n",
    "    \"\"\"\n",
    "    datasets = df['dataset'].unique()\n",
    "    n = len(datasets)\n",
    "    fig, axs = plt.subplots(1, n, figsize=(figsize[0]*n, figsize[1]), sharey=False)\n",
    "    axs = axs if n > 1 else [axs]\n",
    "\n",
    "    # jitter only for dot models\n",
    "    jit = {m: jitter*(i-0.5) if m in dot_models else 0.0\n",
    "           for i,m in enumerate(dot_models+line_models)}\n",
    "\n",
    "    for i, dname in enumerate(datasets):\n",
    "        ax   = axs[i]\n",
    "        sub  = df[df['dataset']==dname]\n",
    "\n",
    "        # pivot -> columns 25,50,75\n",
    "        piv = (sub.pivot_table(index=['w','Model'], columns='quantile', values=metric)\n",
    "                  .reset_index()\n",
    "                  .rename(columns={25:'q25', 50:'q50', 75:'q75'})\n",
    "                  .dropna(subset=['q50']))\n",
    "\n",
    "        for model in piv['Model'].unique():\n",
    "            mdat = piv[piv['Model']==model].sort_values('w')\n",
    "            x    = mdat['w'] + jit.get(model,0)\n",
    "\n",
    "            if model in line_models:\n",
    "                ax.plot(mdat['w'], mdat['q50'], label=model, lw=1.6)\n",
    "            else:\n",
    "                ax.plot(mdat['w'], mdat['q50'], label=model, lw=1.2,\n",
    "                        marker='o', ms=4, linestyle='-')   # dots + connecting line\n",
    "\n",
    "        ax.set_xlabel('w')\n",
    "        if i==0: ax.set_ylabel(metric)\n",
    "        ax.set_title(dname)\n",
    "        ax.set_xticks(sorted(sub['w'].unique()))      # ticks only at actual w’s\n",
    "        ax.set_yscale(yscale)\n",
    "        ax.grid(True, ls='--', lw=0.5)\n",
    "\n",
    "    # legend outside on right\n",
    "    handles, labels = axs[-1].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='center left',\n",
    "               bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "    plt.tight_layout(rect=[0,0,0.92,1])   # leave space for legend\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e3d5cf-c06e-4881-9068-dd0f62fb8096",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip nicu: [Errno 2] No such file or directory: './tedit_lite/tedit_save/te/nicu/res_df_iqr.csv'\n",
      "skip nicu: [Errno 2] No such file or directory: './tedit_lite/tedit_save/te/nicu/res_df_iqr.csv'\n",
      "skip nicu: [Errno 2] No such file or directory: './tedit_lite/tedit_save/te/nicu/res_df_iqr.csv'\n",
      "skip nicu: [Errno 2] No such file or directory: './tedit_lite/tedit_save/te/nicu/res_df_iqr.csv'\n",
      "skip nicu: [Errno 2] No such file or directory: './tedit_lite/tedit_save/te/nicu/res_df_iqr.csv'\n",
      "skip nicu: [Errno 2] No such file or directory: './tedit_lite_tx/tedit_save/te/nicu/res_df_iqr.csv'\n",
      "skip nicu: [Errno 2] No such file or directory: './tedit_lite_tx/tedit_save/te/nicu/res_df_iqr.csv'\n",
      "skip nicu: [Errno 2] No such file or directory: './tedit_lite_tx/tedit_save/te/nicu/res_df_iqr.csv'\n",
      "skip nicu: [Errno 2] No such file or directory: './tedit_lite_tx/tedit_save/te/nicu/res_df_iqr.csv'\n",
      "skip nicu: [Errno 2] No such file or directory: './tedit_lite_tx/tedit_save/te/nicu/res_df_iqr.csv'\n"
     ]
    }
   ],
   "source": [
    "w_dfs = []\n",
    "for w in [0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    w_dfs.append(get_w_evals(w, '_at'))\n",
    "w_df1 = pd.concat(w_dfs, axis=0, ignore_index=True)\n",
    "w_df1['setting'] = 'Attribute-based'\n",
    "w_dfs = []\n",
    "for w in [0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    w_dfs.append(get_w_evals(w, ''))\n",
    "w_df2 = pd.concat(w_dfs, axis=0, ignore_index=True)\n",
    "w_df2['setting'] = 'Text-based'\n",
    "\n",
    "w_df = pd.concat([w_df1, w_df2], axis=0, ignore_index=True)\n",
    "w_df.to_csv(\"./VITAL/results/paper/w_df.csv\", index=False)\n",
    "# plot_metric(w_df, metric='RaTS ↑', yscale='linear')\n",
    "# plot_metric(w_df, metric='DTW distance decrease ↓', jitter=0.015)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6aaa43-7a4b-4ba3-accf-81f91c126b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLIP",
   "language": "python",
   "name": "clip_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
