{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Set R environment variables using the conda environment path\n",
    "# r_home = '/sfs/gpfs/tardis/home/jq2uw/llm_nicu_vitalsigns/clip_env/lib/R'\n",
    "# os.environ['R_HOME'] = r_home\n",
    "# os.environ['R_LIBS'] = f\"{r_home}/library\"\n",
    "# os.environ['R_LIBS_USER'] = os.path.expanduser('~/R/goolf/4.3')\n",
    "# os.environ['LD_LIBRARY_PATH'] = f\"{r_home}/lib:\" + os.environ.get('LD_LIBRARY_PATH', '')\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 333\n",
      "using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "from encoder import *\n",
    "from decoder import *\n",
    "from data import *\n",
    "from vital import *\n",
    "from train import *\n",
    "from eval import *\n",
    "from augmentor import *\n",
    "from describer import *\n",
    "from masker import *\n",
    "print(\"using device: \", device)\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import pkg_resources\n",
    "# print(pkg_resources.get_distribution('python-calamine').version)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (customize) configs\n",
    "overwrite = True\n",
    "model_name = 'testtest'\n",
    "text_config['cl']['die7d'] = True # udpate text_config here if needed\n",
    "text_config['split'] = True\n",
    "text_config['demo']['gre'] = True\n",
    "text_config['demo']['apgar_mage'] = True\n",
    "model_name = model_name + \"___\" + \"_\".join(get_true_components(text_config))\n",
    "\n",
    "update_config(\n",
    "    model_name = model_name,\n",
    "    ts_aug = False, # Data settings\n",
    "    ts_subseq = False,\n",
    "    ts_augsub = False,\n",
    "    downsample_size = 10,\n",
    "    balance = False,\n",
    "    block_target = False,\n",
    "    embedded_dim = 32,\n",
    "    batch_size = 2048, # Data loader settings\n",
    "    ts_global_normalize = True,\n",
    "    ts_local_normalize = False,# True,\n",
    "    patience = 100, # Training settings\n",
    "    num_saves = 20,\n",
    "    num_epochs = 10000,\n",
    "    init_lr = 1e-4,\n",
    "    text_config = text_config,\n",
    "    text_col_ls = ['cl_event', 'ts_description', 'demo_ga', 'demo_weight', 'demo_apgar', 'demo_mother']\n",
    ")\n",
    "config_dict = get_config_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of patients with positive labels:\n",
      "VitalID\n",
      "1018    8\n",
      "5170    8\n",
      "1464    8\n",
      "2361    8\n",
      "2791    8\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Using backend LokyBackend with 36 concurrent workers.\n",
      "[Parallel(n_jobs=36)]: Done 142 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=36)]: Done 728 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=36)]: Done 3323 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=36)]: Done 6832 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=36)]: Done 11232 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=36)]: Done 16432 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=36)]: Done 22432 tasks      | elapsed:   25.7s\n",
      "[Parallel(n_jobs=36)]: Done 29232 tasks      | elapsed:   30.1s\n",
      "[Parallel(n_jobs=36)]: Done 36832 tasks      | elapsed:   35.3s\n",
      "[Parallel(n_jobs=36)]: Done 45232 tasks      | elapsed:   40.8s\n",
      "[Parallel(n_jobs=36)]: Done 54432 tasks      | elapsed:   46.9s\n",
      "[Parallel(n_jobs=36)]: Done 64432 tasks      | elapsed:   54.1s\n",
      "[Parallel(n_jobs=36)]: Done 65353 out of 65353 | elapsed:   54.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This infant will survive.  This infant has gestational age 24 weeks. Birth weight is 360 grams. This infant is Female Black non-Hispanic. The Apgar5 scores 6. Mother is 21 years old.    Moderate variability.  Very low amount of consecutive increases. \n",
      "\n",
      "Available text columns:\n",
      "['cl_event', 'ts_description', 'demo_ga', 'demo_weight', 'demo_gender', 'demo_race', 'demo_ethnicity', 'demo_apgar', 'demo_mother', 'cl_die7d']\n",
      "\n",
      "Sample of patients with positive labels:\n",
      "TestID\n",
      "817     8\n",
      "1903    8\n",
      "801     8\n",
      "508     8\n",
      "2518    8\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Using backend LokyBackend with 36 concurrent workers.\n",
      "[Parallel(n_jobs=36)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=36)]: Done 1800 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=36)]: Done 4600 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=36)]: Done 8200 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=36)]: Done 12600 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=36)]: Done 17800 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=36)]: Done 23800 tasks      | elapsed:   15.2s\n",
      "[Parallel(n_jobs=36)]: Done 30600 tasks      | elapsed:   19.5s\n",
      "[Parallel(n_jobs=36)]: Done 38200 tasks      | elapsed:   24.7s\n",
      "[Parallel(n_jobs=36)]: Done 46600 tasks      | elapsed:   30.2s\n",
      "[Parallel(n_jobs=36)]: Done 55800 tasks      | elapsed:   35.8s\n",
      "[Parallel(n_jobs=36)]: Done 61570 out of 61570 | elapsed:   39.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This infant will survive.  This infant has gestational age 33 weeks. Birth weight is 2630 grams. This infant is Male non-Black non-Hispanic. The Apgar5 scores 9. Mother is 26 years old.    High variability.  Low amount of consecutive increases. \n",
      "\n",
      "Available text columns:\n",
      "['cl_event', 'ts_description', 'demo_ga', 'demo_weight', 'demo_gender', 'demo_race', 'demo_ethnicity', 'demo_apgar', 'demo_mother', 'cl_die7d']\n",
      "After downsampling:\n",
      "cl_event\n",
      "This infant will die in 7 days.     384\n",
      "This infant will survive.            10\n",
      "Name: count, dtype: int64\n",
      "After downsampling:\n",
      "cl_event\n",
      "This infant will die in 7 days.     241\n",
      "This infant will survive.            10\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jq2uw/.local/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cl_event\n",
      "This infant will die in 7 days.     384\n",
      "This infant will survive.            10\n",
      "Name: count, dtype: int64\n",
      "cl_event\n",
      "This infant will die in 7 days.     241\n",
      "This infant will survive.            10\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# run preprocess.py to ready the data\n",
    "with open('preprocess.py', 'r') as file:\n",
    "    exec(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize VITAL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport vital\n",
    "%autoreload 1\n",
    "%aimport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                                                 Param #\n",
      "===============================================================================================\n",
      "VITAL3D                                                                1\n",
      "├─TSVAEEncoder: 1-1                                                    --\n",
      "│    └─Sequential: 2-1                                                 --\n",
      "│    │    └─Linear: 3-1                                                77,056\n",
      "│    │    └─LeakyReLU: 3-2                                             --\n",
      "│    │    └─Linear: 3-3                                                32,896\n",
      "│    │    └─LeakyReLU: 3-4                                             --\n",
      "│    └─Linear: 2-2                                                     4,128\n",
      "│    └─Linear: 2-3                                                     4,128\n",
      "├─TextEncoderWithAttention: 1-2                                        32\n",
      "│    └─ModuleList: 2-4                                                 --\n",
      "│    │    └─Sequential: 3-5                                            1,595,552\n",
      "│    │    └─Sequential: 3-6                                            1,595,552\n",
      "│    │    └─Sequential: 3-7                                            1,595,552\n",
      "│    │    └─Sequential: 3-8                                            1,595,552\n",
      "│    │    └─Sequential: 3-9                                            1,595,552\n",
      "│    │    └─Sequential: 3-10                                           1,595,552\n",
      "│    └─MultiheadAttention: 2-5                                         3,168\n",
      "│    │    └─NonDynamicallyQuantizableLinear: 3-11                      1,056\n",
      "├─TSVAEDecoder: 1-3                                                    --\n",
      "│    └─Sequential: 2-6                                                 --\n",
      "│    │    └─Linear: 3-12                                               4,224\n",
      "│    │    └─LeakyReLU: 3-13                                            --\n",
      "│    │    └─Linear: 3-14                                               38,700\n",
      "===============================================================================================\n",
      "Total params: 9,738,701\n",
      "Trainable params: 9,738,701\n",
      "Non-trainable params: 0\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "# customize model\n",
    "if overwrite:    \n",
    "    # check if ts_f_dim is already in the memory\n",
    "    if 'ts_f_dim' not in locals():\n",
    "        # get the dimension out\n",
    "        if config_dict['3d']:\n",
    "            ts_f_dim, tx_f_dim_ls, labels_dim = get_features3d(df_train.iloc[:1,:], \n",
    "                                                                config_dict['text_encoder_name'], \n",
    "                                                                config_dict['ts_normalize_mean'],\n",
    "                                                                config_dict['ts_normalize_std'],\n",
    "                                                                text_col_ls = config_dict['text_col_ls'])\n",
    "        else:\n",
    "            ts_f_dim, tx_f_dim, labels_dim = get_features(df_train.iloc[:1,:], \n",
    "                                                            config_dict['text_encoder_name'], \n",
    "                                                            config_dict['ts_normalize_mean'],\n",
    "                                                            config_dict['ts_normalize_std'])\n",
    "    \n",
    "    ts_encoder = None\n",
    "    ts_decoder = None\n",
    "    # #--- custom ts encoder in encoder.py ---\n",
    "    # e = MLPEncoder(\n",
    "    #     ts_dim=ts_f_dim.shape[1], \n",
    "    #     output_dim=config_dict['embedded_dim']\n",
    "    # )\n",
    "    # ts_encoder = TSVAEEncoderWrapper(e)\n",
    "    # # --- custom ts decoder in decoder.py ---\n",
    "    # d = MLPDecoder(\n",
    "    #     ts_dim=ts_f_dim.shape[1], \n",
    "    #     output_dim=config_dict['embedded_dim']\n",
    "    # )\n",
    "    # ts_decoder = TSVAEDecoderWrapper(d)\n",
    "\n",
    "    if config_dict['3d']:\n",
    "        model = VITAL3D(\n",
    "                    ts_dim=ts_f_dim.shape[1],\n",
    "                    text_dim=tx_f_dim_ls[0].shape[1],\n",
    "                    n_text=len(tx_f_dim_ls),\n",
    "                    output_dim=config_dict['embedded_dim'],\n",
    "                    ts_encoder=ts_encoder,\n",
    "                    ts_decoder=ts_decoder\n",
    "                )\n",
    "    else:\n",
    "        model = VITAL(\n",
    "                    ts_dim=ts_f_dim.shape[1],\n",
    "                    text_dim=tx_f_dim.shape[1],\n",
    "                    output_dim=config_dict['embedded_dim'],\n",
    "                    ts_encoder=ts_encoder,\n",
    "                    ts_decoder=ts_decoder\n",
    "                )\n",
    "    update_config(model_init = model)\n",
    "    config_dict = get_config_dict()\n",
    "    \n",
    "    # ------------------------- ready training -------------------------\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config_dict['init_lr'],\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min',\n",
    "        factor=0.9,         \n",
    "        patience=config_dict['patience'],       \n",
    "        verbose=True,\n",
    "        min_lr=1e-10,        \n",
    "        threshold=1e-4,      \n",
    "        cooldown=20          \n",
    "    )\n",
    "\n",
    "    kl_annealer = KLAnnealer(start=1.0, \n",
    "                             end=1.0, \n",
    "                             epochs=10000) # for the first 1000 epochs, favor reconstruction more\n",
    "\n",
    "\n",
    "    train_eval_metrics_list = []\n",
    "    test_eval_metrics_list = []\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    # ------------------------- ready output directory -------------------------\n",
    "    import shutil\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir)\n",
    "    # torch.save(config_dict, config_path)\n",
    "    # overwrite = False # reset overwrite to False\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 1.0\n",
      "Epoch [1/1000]\n",
      "\tTraining Loss: 27.625423\n",
      "\tTesting Loss: 22.341518\n",
      "\tLearning Rate: 0.000100000\n",
      "beta: 1.0\n",
      "Epoch [2/1000]\n",
      "\tTraining Loss: 23.361652\n",
      "\tTesting Loss: 21.267193\n",
      "\tLearning Rate: 0.000100000\n",
      "beta: 1.0\n"
     ]
    }
   ],
   "source": [
    "train_losses_tmp, test_losses_tmp = train_vital(model, \n",
    "                                                train_dataloader,\n",
    "                                                test_dataloader, \n",
    "                                                optimizer, \n",
    "                                                scheduler,\n",
    "                                                kl_annealer,\n",
    "                                                num_epochs=1000, \n",
    "                                                train_type='clip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
