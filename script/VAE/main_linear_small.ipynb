{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyqiu/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cpu\n",
      "using device:  cpu\n",
      "/Users/joyqiu/Documents/Documents JoyQiu Work/Research/LLMTimeSeries/llm_nicu_vitalsigns\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from models import *\n",
    "\n",
    "\n",
    "import os\n",
    "os.chdir('../..')\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = './data/PAS Challenge HR Data.xlsx'  \n",
    "batch_size = 128\n",
    "train_dataloader, test_dataloader, df_scaled, df = prepare_data(data_path, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Kernel Shape\n",
       "============================================================================================================================================\n",
       "VAE_Linear_Small                         [128, 300]                [128, 300]                --                        --\n",
       "Sequential: 1-1                        [128, 300]                [128, 50]                 --                        --\n",
       "    Linear: 2-1                       [128, 300]                [128, 150]                45,150                    --\n",
       "    LeakyReLU: 2-2                    [128, 150]                [128, 150]                --                        --\n",
       "    Linear: 2-3                       [128, 150]                [128, 150]                22,650                    --\n",
       "    LeakyReLU: 2-4                    [128, 150]                [128, 150]                --                        --\n",
       "    Linear: 2-5                       [128, 150]                [128, 50]                 7,550                     --\n",
       "    LeakyReLU: 2-6                    [128, 50]                 [128, 50]                 --                        --\n",
       "Linear: 1-2                            [128, 50]                 [128, 1]                  51                        --\n",
       "Linear: 1-3                            [128, 50]                 [128, 1]                  51                        --\n",
       "Sequential: 1-4                        [128, 1]                  [128, 300]                --                        --\n",
       "    Linear: 2-7                       [128, 1]                  [128, 50]                 100                       --\n",
       "    LeakyReLU: 2-8                    [128, 50]                 [128, 50]                 --                        --\n",
       "    Linear: 2-9                       [128, 50]                 [128, 150]                7,650                     --\n",
       "    LeakyReLU: 2-10                   [128, 150]                [128, 150]                --                        --\n",
       "    Linear: 2-11                      [128, 150]                [128, 150]                22,650                    --\n",
       "    LeakyReLU: 2-12                   [128, 150]                [128, 150]                --                        --\n",
       "    Linear: 2-13                      [128, 150]                [128, 300]                45,300                    --\n",
       "============================================================================================================================================\n",
       "Total params: 151,152\n",
       "Trainable params: 151,152\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 19.35\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.15\n",
       "Forward/backward pass size (MB): 1.03\n",
       "Params size (MB): 0.60\n",
       "Estimated Total Size (MB): 1.78\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overwrite = True\n",
    "model_path = './script/VAE/results/hr_vae_linear_small.pth' \n",
    "loss_path = './script/VAE/results/hr_train_test_losses_linear_small.pth'\n",
    "# Initialize model\n",
    "model = VAE_Linear_Small().to(device)\n",
    "nn_summary(model, \n",
    "        input_size=(batch_size, 300),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\"],\n",
    "        depth=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000000000000]\n",
      "\tTraining Loss: 1.012663\n",
      "\tTesting Loss: 1.001191\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [2/10000000000000]\n",
      "\tTraining Loss: 1.000452\n",
      "\tTesting Loss: 1.000443\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [3/10000000000000]\n",
      "\tTraining Loss: 0.999974\n",
      "\tTesting Loss: 1.000022\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [4/10000000000000]\n",
      "\tTraining Loss: 0.999766\n",
      "\tTesting Loss: 0.999935\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [5/10000000000000]\n",
      "\tTraining Loss: 0.999672\n",
      "\tTesting Loss: 0.999752\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [6/10000000000000]\n",
      "\tTraining Loss: 0.999648\n",
      "\tTesting Loss: 0.999991\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [7/10000000000000]\n",
      "\tTraining Loss: 0.999616\n",
      "\tTesting Loss: 0.999779\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [8/10000000000000]\n",
      "\tTraining Loss: 0.999629\n",
      "\tTesting Loss: 0.999737\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [9/10000000000000]\n",
      "\tTraining Loss: 0.999627\n",
      "\tTesting Loss: 0.999800\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [10/10000000000000]\n",
      "\tTraining Loss: 0.999606\n",
      "\tTesting Loss: 0.999738\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [11/10000000000000]\n",
      "\tTraining Loss: 0.999608\n",
      "\tTesting Loss: 0.999726\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [12/10000000000000]\n",
      "\tTraining Loss: 0.999589\n",
      "\tTesting Loss: 0.999792\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [13/10000000000000]\n",
      "\tTraining Loss: 0.999596\n",
      "\tTesting Loss: 0.999726\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [14/10000000000000]\n",
      "\tTraining Loss: 0.999613\n",
      "\tTesting Loss: 0.999750\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [15/10000000000000]\n",
      "\tTraining Loss: 0.999597\n",
      "\tTesting Loss: 0.999700\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [16/10000000000000]\n",
      "\tTraining Loss: 0.999600\n",
      "\tTesting Loss: 0.999741\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [17/10000000000000]\n",
      "\tTraining Loss: 0.999586\n",
      "\tTesting Loss: 0.999729\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [18/10000000000000]\n",
      "\tTraining Loss: 0.999589\n",
      "\tTesting Loss: 0.999779\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [19/10000000000000]\n",
      "\tTraining Loss: 0.999582\n",
      "\tTesting Loss: 0.999702\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [20/10000000000000]\n",
      "\tTraining Loss: 0.999599\n",
      "\tTesting Loss: 0.999728\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [21/10000000000000]\n",
      "\tTraining Loss: 0.999588\n",
      "\tTesting Loss: 0.999728\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [22/10000000000000]\n",
      "\tTraining Loss: 0.999588\n",
      "\tTesting Loss: 0.999674\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [23/10000000000000]\n",
      "\tTraining Loss: 0.999583\n",
      "\tTesting Loss: 0.999681\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [24/10000000000000]\n",
      "\tTraining Loss: 0.999584\n",
      "\tTesting Loss: 0.999738\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [25/10000000000000]\n",
      "\tTraining Loss: 0.999579\n",
      "\tTesting Loss: 0.999693\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [26/10000000000000]\n",
      "\tTraining Loss: 0.999592\n",
      "\tTesting Loss: 0.999068\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [27/10000000000000]\n",
      "\tTraining Loss: 0.999592\n",
      "\tTesting Loss: 0.999857\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [28/10000000000000]\n",
      "\tTraining Loss: 0.999580\n",
      "\tTesting Loss: 0.999715\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [29/10000000000000]\n",
      "\tTraining Loss: 0.999598\n",
      "\tTesting Loss: 0.999831\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [30/10000000000000]\n",
      "\tTraining Loss: 0.999569\n",
      "\tTesting Loss: 0.999745\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [31/10000000000000]\n",
      "\tTraining Loss: 0.999573\n",
      "\tTesting Loss: 0.999772\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [32/10000000000000]\n",
      "\tTraining Loss: 1.001617\n",
      "\tTesting Loss: 0.999667\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [33/10000000000000]\n",
      "\tTraining Loss: 0.999586\n",
      "\tTesting Loss: 0.999719\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [34/10000000000000]\n",
      "\tTraining Loss: 0.999582\n",
      "\tTesting Loss: 0.999789\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [35/10000000000000]\n",
      "\tTraining Loss: 0.999582\n",
      "\tTesting Loss: 0.999697\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [36/10000000000000]\n",
      "\tTraining Loss: 0.999584\n",
      "\tTesting Loss: 0.999790\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [37/10000000000000]\n",
      "\tTraining Loss: 0.999580\n",
      "\tTesting Loss: 0.999853\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [38/10000000000000]\n",
      "\tTraining Loss: 0.999598\n",
      "\tTesting Loss: 0.999725\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [39/10000000000000]\n",
      "\tTraining Loss: 0.999585\n",
      "\tTesting Loss: 0.999666\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [40/10000000000000]\n",
      "\tTraining Loss: 0.999590\n",
      "\tTesting Loss: 0.999763\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [41/10000000000000]\n",
      "\tTraining Loss: 0.999582\n",
      "\tTesting Loss: 0.999734\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [42/10000000000000]\n",
      "\tTraining Loss: 0.895611\n",
      "\tTesting Loss: 0.863770\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [43/10000000000000]\n",
      "\tTraining Loss: 0.858864\n",
      "\tTesting Loss: 0.859796\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [44/10000000000000]\n",
      "\tTraining Loss: 0.853661\n",
      "\tTesting Loss: 0.852074\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [45/10000000000000]\n",
      "\tTraining Loss: 0.852034\n",
      "\tTesting Loss: 0.852348\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [46/10000000000000]\n",
      "\tTraining Loss: 0.847600\n",
      "\tTesting Loss: 0.843936\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [47/10000000000000]\n",
      "\tTraining Loss: 0.845884\n",
      "\tTesting Loss: 0.847496\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [48/10000000000000]\n",
      "\tTraining Loss: 0.844111\n",
      "\tTesting Loss: 0.842725\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [49/10000000000000]\n",
      "\tTraining Loss: 0.841119\n",
      "\tTesting Loss: 0.836721\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [50/10000000000000]\n",
      "\tTraining Loss: 0.841990\n",
      "\tTesting Loss: 0.843221\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [51/10000000000000]\n",
      "\tTraining Loss: 0.839082\n",
      "\tTesting Loss: 0.839640\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [52/10000000000000]\n",
      "\tTraining Loss: 0.839504\n",
      "\tTesting Loss: 0.837366\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [53/10000000000000]\n",
      "\tTraining Loss: 0.836464\n",
      "\tTesting Loss: 0.836965\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [54/10000000000000]\n",
      "\tTraining Loss: 0.834966\n",
      "\tTesting Loss: 0.836945\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [55/10000000000000]\n",
      "\tTraining Loss: 0.835621\n",
      "\tTesting Loss: 0.833992\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [56/10000000000000]\n",
      "\tTraining Loss: 0.833037\n",
      "\tTesting Loss: 0.829927\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [57/10000000000000]\n",
      "\tTraining Loss: 0.832937\n",
      "\tTesting Loss: 0.831698\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [58/10000000000000]\n",
      "\tTraining Loss: 0.831174\n",
      "\tTesting Loss: 0.834625\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [59/10000000000000]\n",
      "\tTraining Loss: 0.828978\n",
      "\tTesting Loss: 0.832613\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [60/10000000000000]\n",
      "\tTraining Loss: 0.828877\n",
      "\tTesting Loss: 0.825519\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [61/10000000000000]\n",
      "\tTraining Loss: 0.828955\n",
      "\tTesting Loss: 0.828601\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [62/10000000000000]\n",
      "\tTraining Loss: 0.828140\n",
      "\tTesting Loss: 0.826432\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [63/10000000000000]\n",
      "\tTraining Loss: 0.834148\n",
      "\tTesting Loss: 0.828015\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [64/10000000000000]\n",
      "\tTraining Loss: 0.827692\n",
      "\tTesting Loss: 0.836984\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [65/10000000000000]\n",
      "\tTraining Loss: 0.828886\n",
      "\tTesting Loss: 0.828110\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [66/10000000000000]\n",
      "\tTraining Loss: 0.827510\n",
      "\tTesting Loss: 0.827973\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [67/10000000000000]\n",
      "\tTraining Loss: 0.826568\n",
      "\tTesting Loss: 0.830850\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [68/10000000000000]\n",
      "\tTraining Loss: 0.827009\n",
      "\tTesting Loss: 0.833934\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [69/10000000000000]\n",
      "\tTraining Loss: 0.824523\n",
      "\tTesting Loss: 0.826470\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [70/10000000000000]\n",
      "\tTraining Loss: 0.824865\n",
      "\tTesting Loss: 0.822408\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [71/10000000000000]\n",
      "\tTraining Loss: 0.823346\n",
      "\tTesting Loss: 0.822712\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [72/10000000000000]\n",
      "\tTraining Loss: 0.824549\n",
      "\tTesting Loss: 0.829486\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [73/10000000000000]\n",
      "\tTraining Loss: 0.821999\n",
      "\tTesting Loss: 0.818609\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [74/10000000000000]\n",
      "\tTraining Loss: 0.825468\n",
      "\tTesting Loss: 0.823668\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [75/10000000000000]\n",
      "\tTraining Loss: 0.823564\n",
      "\tTesting Loss: 0.826742\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [76/10000000000000]\n",
      "\tTraining Loss: 0.822541\n",
      "\tTesting Loss: 0.823165\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [77/10000000000000]\n",
      "\tTraining Loss: 0.822083\n",
      "\tTesting Loss: 0.822709\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [78/10000000000000]\n",
      "\tTraining Loss: 0.822771\n",
      "\tTesting Loss: 0.827550\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [79/10000000000000]\n",
      "\tTraining Loss: 0.823149\n",
      "\tTesting Loss: 0.823312\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [80/10000000000000]\n",
      "\tTraining Loss: 0.821840\n",
      "\tTesting Loss: 0.825749\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [81/10000000000000]\n",
      "\tTraining Loss: 0.820378\n",
      "\tTesting Loss: 0.821542\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [82/10000000000000]\n",
      "\tTraining Loss: 0.819688\n",
      "\tTesting Loss: 0.822719\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [83/10000000000000]\n",
      "\tTraining Loss: 0.821575\n",
      "\tTesting Loss: 0.826461\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [84/10000000000000]\n",
      "\tTraining Loss: 0.823400\n",
      "\tTesting Loss: 0.821609\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [85/10000000000000]\n",
      "\tTraining Loss: 0.822537\n",
      "\tTesting Loss: 0.821382\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [86/10000000000000]\n",
      "\tTraining Loss: 0.822185\n",
      "\tTesting Loss: 0.820310\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [87/10000000000000]\n",
      "\tTraining Loss: 0.822371\n",
      "\tTesting Loss: 0.822420\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [88/10000000000000]\n",
      "\tTraining Loss: 0.822093\n",
      "\tTesting Loss: 0.819099\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [89/10000000000000]\n",
      "\tTraining Loss: 0.820696\n",
      "\tTesting Loss: 0.821107\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [90/10000000000000]\n",
      "\tTraining Loss: 0.821747\n",
      "\tTesting Loss: 0.820481\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [91/10000000000000]\n",
      "\tTraining Loss: 0.821158\n",
      "\tTesting Loss: 0.824496\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [92/10000000000000]\n",
      "\tTraining Loss: 0.820977\n",
      "\tTesting Loss: 0.820941\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [93/10000000000000]\n",
      "\tTraining Loss: 0.820783\n",
      "\tTesting Loss: 0.817478\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [94/10000000000000]\n",
      "\tTraining Loss: 0.819582\n",
      "\tTesting Loss: 0.818356\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [95/10000000000000]\n",
      "\tTraining Loss: 0.820582\n",
      "\tTesting Loss: 0.818446\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [96/10000000000000]\n",
      "\tTraining Loss: 0.821090\n",
      "\tTesting Loss: 0.829846\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [97/10000000000000]\n",
      "\tTraining Loss: 0.819296\n",
      "\tTesting Loss: 0.820074\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [98/10000000000000]\n",
      "\tTraining Loss: 0.819144\n",
      "\tTesting Loss: 0.820206\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [99/10000000000000]\n",
      "\tTraining Loss: 0.819034\n",
      "\tTesting Loss: 0.817381\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [100/10000000000000]\n",
      "\tTraining Loss: 0.820245\n",
      "\tTesting Loss: 0.816108\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [101/10000000000000]\n",
      "\tTraining Loss: 0.819219\n",
      "\tTesting Loss: 0.814616\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [102/10000000000000]\n",
      "\tTraining Loss: 0.819595\n",
      "\tTesting Loss: 0.820610\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [103/10000000000000]\n",
      "\tTraining Loss: 0.819538\n",
      "\tTesting Loss: 0.825028\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [104/10000000000000]\n",
      "\tTraining Loss: 0.819556\n",
      "\tTesting Loss: 0.831751\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [105/10000000000000]\n",
      "\tTraining Loss: 0.822541\n",
      "\tTesting Loss: 0.821162\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [106/10000000000000]\n",
      "\tTraining Loss: 0.819650\n",
      "\tTesting Loss: 0.815622\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [107/10000000000000]\n",
      "\tTraining Loss: 0.817125\n",
      "\tTesting Loss: 0.813212\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [108/10000000000000]\n",
      "\tTraining Loss: 0.817501\n",
      "\tTesting Loss: 0.822980\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [109/10000000000000]\n",
      "\tTraining Loss: 0.816891\n",
      "\tTesting Loss: 0.821462\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [110/10000000000000]\n",
      "\tTraining Loss: 0.817965\n",
      "\tTesting Loss: 0.821091\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [111/10000000000000]\n",
      "\tTraining Loss: 0.817431\n",
      "\tTesting Loss: 0.817787\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [112/10000000000000]\n",
      "\tTraining Loss: 0.817152\n",
      "\tTesting Loss: 0.813586\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [113/10000000000000]\n",
      "\tTraining Loss: 0.817476\n",
      "\tTesting Loss: 0.823503\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [114/10000000000000]\n",
      "\tTraining Loss: 0.819000\n",
      "\tTesting Loss: 0.816735\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [115/10000000000000]\n",
      "\tTraining Loss: 0.818365\n",
      "\tTesting Loss: 0.815394\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [116/10000000000000]\n",
      "\tTraining Loss: 0.817405\n",
      "\tTesting Loss: 0.815459\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [117/10000000000000]\n",
      "\tTraining Loss: 0.815926\n",
      "\tTesting Loss: 0.817668\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [118/10000000000000]\n",
      "\tTraining Loss: 0.816898\n",
      "\tTesting Loss: 0.820491\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [119/10000000000000]\n",
      "\tTraining Loss: 0.815401\n",
      "\tTesting Loss: 0.820909\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [120/10000000000000]\n",
      "\tTraining Loss: 0.819242\n",
      "\tTesting Loss: 0.816691\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [121/10000000000000]\n",
      "\tTraining Loss: 0.817490\n",
      "\tTesting Loss: 0.819332\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [122/10000000000000]\n",
      "\tTraining Loss: 0.815693\n",
      "\tTesting Loss: 0.813255\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [123/10000000000000]\n",
      "\tTraining Loss: 0.815568\n",
      "\tTesting Loss: 0.816157\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [124/10000000000000]\n",
      "\tTraining Loss: 0.815551\n",
      "\tTesting Loss: 0.819757\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [125/10000000000000]\n",
      "\tTraining Loss: 0.816182\n",
      "\tTesting Loss: 0.817343\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [126/10000000000000]\n",
      "\tTraining Loss: 0.816322\n",
      "\tTesting Loss: 0.812339\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [127/10000000000000]\n",
      "\tTraining Loss: 0.816265\n",
      "\tTesting Loss: 0.811204\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [128/10000000000000]\n",
      "\tTraining Loss: 0.814873\n",
      "\tTesting Loss: 0.813454\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [129/10000000000000]\n",
      "\tTraining Loss: 0.814588\n",
      "\tTesting Loss: 0.815825\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [130/10000000000000]\n",
      "\tTraining Loss: 0.815806\n",
      "\tTesting Loss: 0.827874\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [131/10000000000000]\n",
      "\tTraining Loss: 0.816742\n",
      "\tTesting Loss: 0.811940\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [132/10000000000000]\n",
      "\tTraining Loss: 0.816788\n",
      "\tTesting Loss: 0.822754\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [133/10000000000000]\n",
      "\tTraining Loss: 0.814022\n",
      "\tTesting Loss: 0.815405\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [134/10000000000000]\n",
      "\tTraining Loss: 0.817575\n",
      "\tTesting Loss: 0.816215\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [135/10000000000000]\n",
      "\tTraining Loss: 0.816174\n",
      "\tTesting Loss: 0.826241\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [136/10000000000000]\n",
      "\tTraining Loss: 0.817358\n",
      "\tTesting Loss: 0.818630\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [137/10000000000000]\n",
      "\tTraining Loss: 0.816731\n",
      "\tTesting Loss: 0.818081\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [138/10000000000000]\n",
      "\tTraining Loss: 0.816759\n",
      "\tTesting Loss: 0.818909\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [139/10000000000000]\n",
      "\tTraining Loss: 0.813701\n",
      "\tTesting Loss: 0.828661\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [140/10000000000000]\n",
      "\tTraining Loss: 0.815321\n",
      "\tTesting Loss: 0.823685\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [141/10000000000000]\n",
      "\tTraining Loss: 0.815852\n",
      "\tTesting Loss: 0.822399\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [142/10000000000000]\n",
      "\tTraining Loss: 0.814498\n",
      "\tTesting Loss: 0.815912\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [143/10000000000000]\n",
      "\tTraining Loss: 0.815451\n",
      "\tTesting Loss: 0.814052\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [144/10000000000000]\n",
      "\tTraining Loss: 0.829286\n",
      "\tTesting Loss: 0.815075\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [145/10000000000000]\n",
      "\tTraining Loss: 0.816627\n",
      "\tTesting Loss: 0.811762\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [146/10000000000000]\n",
      "\tTraining Loss: 0.815304\n",
      "\tTesting Loss: 0.818040\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [147/10000000000000]\n",
      "\tTraining Loss: 0.813125\n",
      "\tTesting Loss: 0.811595\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [148/10000000000000]\n",
      "\tTraining Loss: 0.814981\n",
      "\tTesting Loss: 0.816298\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [149/10000000000000]\n",
      "\tTraining Loss: 0.812905\n",
      "\tTesting Loss: 0.809152\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [150/10000000000000]\n",
      "\tTraining Loss: 0.813986\n",
      "\tTesting Loss: 0.810068\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [151/10000000000000]\n",
      "\tTraining Loss: 0.812696\n",
      "\tTesting Loss: 0.815910\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [152/10000000000000]\n",
      "\tTraining Loss: 0.813798\n",
      "\tTesting Loss: 0.812800\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [153/10000000000000]\n",
      "\tTraining Loss: 0.812522\n",
      "\tTesting Loss: 0.816328\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [154/10000000000000]\n",
      "\tTraining Loss: 0.814515\n",
      "\tTesting Loss: 0.816159\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [155/10000000000000]\n",
      "\tTraining Loss: 0.814385\n",
      "\tTesting Loss: 0.839976\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [156/10000000000000]\n",
      "\tTraining Loss: 0.820553\n",
      "\tTesting Loss: 0.820485\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [157/10000000000000]\n",
      "\tTraining Loss: 0.812731\n",
      "\tTesting Loss: 0.807641\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [158/10000000000000]\n",
      "\tTraining Loss: 0.812078\n",
      "\tTesting Loss: 0.811679\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [159/10000000000000]\n",
      "\tTraining Loss: 0.814197\n",
      "\tTesting Loss: 0.816257\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [160/10000000000000]\n",
      "\tTraining Loss: 0.816631\n",
      "\tTesting Loss: 0.818153\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [161/10000000000000]\n",
      "\tTraining Loss: 0.813753\n",
      "\tTesting Loss: 0.817796\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [162/10000000000000]\n",
      "\tTraining Loss: 0.814296\n",
      "\tTesting Loss: 0.817499\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [163/10000000000000]\n",
      "\tTraining Loss: 0.812995\n",
      "\tTesting Loss: 0.809476\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [164/10000000000000]\n",
      "\tTraining Loss: 0.812901\n",
      "\tTesting Loss: 0.820487\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [165/10000000000000]\n",
      "\tTraining Loss: 0.813776\n",
      "\tTesting Loss: 0.818464\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [166/10000000000000]\n",
      "\tTraining Loss: 0.812787\n",
      "\tTesting Loss: 0.819027\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [167/10000000000000]\n",
      "\tTraining Loss: 0.811103\n",
      "\tTesting Loss: 0.825519\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [168/10000000000000]\n",
      "\tTraining Loss: 0.812392\n",
      "\tTesting Loss: 0.840891\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [169/10000000000000]\n",
      "\tTraining Loss: 0.813106\n",
      "\tTesting Loss: 0.814972\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [170/10000000000000]\n",
      "\tTraining Loss: 0.812996\n",
      "\tTesting Loss: 0.811040\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [171/10000000000000]\n",
      "\tTraining Loss: 0.810798\n",
      "\tTesting Loss: 0.814128\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [172/10000000000000]\n",
      "\tTraining Loss: 0.811192\n",
      "\tTesting Loss: 0.807552\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [173/10000000000000]\n",
      "\tTraining Loss: 0.812551\n",
      "\tTesting Loss: 0.815155\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [174/10000000000000]\n",
      "\tTraining Loss: 0.811453\n",
      "\tTesting Loss: 0.809339\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [175/10000000000000]\n",
      "\tTraining Loss: 0.810674\n",
      "\tTesting Loss: 0.814421\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [176/10000000000000]\n",
      "\tTraining Loss: 0.810171\n",
      "\tTesting Loss: 0.811118\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [177/10000000000000]\n",
      "\tTraining Loss: 0.812753\n",
      "\tTesting Loss: 0.818602\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [178/10000000000000]\n",
      "\tTraining Loss: 0.813116\n",
      "\tTesting Loss: 0.814079\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [179/10000000000000]\n",
      "\tTraining Loss: 0.809325\n",
      "\tTesting Loss: 0.807534\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [180/10000000000000]\n",
      "\tTraining Loss: 0.811091\n",
      "\tTesting Loss: 0.808131\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [181/10000000000000]\n",
      "\tTraining Loss: 0.813355\n",
      "\tTesting Loss: 0.807908\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [182/10000000000000]\n",
      "\tTraining Loss: 0.810949\n",
      "\tTesting Loss: 0.812138\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [183/10000000000000]\n",
      "\tTraining Loss: 0.811905\n",
      "\tTesting Loss: 0.822854\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [184/10000000000000]\n",
      "\tTraining Loss: 0.812357\n",
      "\tTesting Loss: 0.808513\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [185/10000000000000]\n",
      "\tTraining Loss: 0.810560\n",
      "\tTesting Loss: 0.815332\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [186/10000000000000]\n",
      "\tTraining Loss: 0.814069\n",
      "\tTesting Loss: 0.820783\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [187/10000000000000]\n",
      "\tTraining Loss: 0.814524\n",
      "\tTesting Loss: 0.811996\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [188/10000000000000]\n",
      "\tTraining Loss: 0.813225\n",
      "\tTesting Loss: 0.814107\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [189/10000000000000]\n",
      "\tTraining Loss: 0.811280\n",
      "\tTesting Loss: 0.825382\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [190/10000000000000]\n",
      "\tTraining Loss: 0.816886\n",
      "\tTesting Loss: 0.822601\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [191/10000000000000]\n",
      "\tTraining Loss: 0.812392\n",
      "\tTesting Loss: 0.808280\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [192/10000000000000]\n",
      "\tTraining Loss: 0.811173\n",
      "\tTesting Loss: 0.807942\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [193/10000000000000]\n",
      "\tTraining Loss: 0.809705\n",
      "\tTesting Loss: 0.813184\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [194/10000000000000]\n",
      "\tTraining Loss: 0.813548\n",
      "\tTesting Loss: 0.813926\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [195/10000000000000]\n",
      "\tTraining Loss: 0.810879\n",
      "\tTesting Loss: 0.810946\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [196/10000000000000]\n",
      "\tTraining Loss: 0.809580\n",
      "\tTesting Loss: 0.810019\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [197/10000000000000]\n",
      "\tTraining Loss: 0.813361\n",
      "\tTesting Loss: 0.812378\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [198/10000000000000]\n",
      "\tTraining Loss: 0.814500\n",
      "\tTesting Loss: 0.825966\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [199/10000000000000]\n",
      "\tTraining Loss: 0.817585\n",
      "\tTesting Loss: 0.817453\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [200/10000000000000]\n",
      "\tTraining Loss: 0.813697\n",
      "\tTesting Loss: 0.817890\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [201/10000000000000]\n",
      "\tTraining Loss: 0.816424\n",
      "\tTesting Loss: 0.813617\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [202/10000000000000]\n",
      "\tTraining Loss: 0.813199\n",
      "\tTesting Loss: 0.815103\n",
      "\tLearning Rate: 0.001000000\n",
      "Epoch [203/10000000000000]\n",
      "\tTraining Loss: 0.815117\n",
      "\tTesting Loss: 0.815502\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [204/10000000000000]\n",
      "\tTraining Loss: 0.810455\n",
      "\tTesting Loss: 0.809554\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [205/10000000000000]\n",
      "\tTraining Loss: 0.811743\n",
      "\tTesting Loss: 0.814633\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [206/10000000000000]\n",
      "\tTraining Loss: 0.810666\n",
      "\tTesting Loss: 0.811829\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [207/10000000000000]\n",
      "\tTraining Loss: 0.810407\n",
      "\tTesting Loss: 0.812883\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [208/10000000000000]\n",
      "\tTraining Loss: 0.809725\n",
      "\tTesting Loss: 0.817424\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [209/10000000000000]\n",
      "\tTraining Loss: 0.809621\n",
      "\tTesting Loss: 0.805907\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [210/10000000000000]\n",
      "\tTraining Loss: 0.806343\n",
      "\tTesting Loss: 0.818644\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [211/10000000000000]\n",
      "\tTraining Loss: 0.807925\n",
      "\tTesting Loss: 0.806381\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [212/10000000000000]\n",
      "\tTraining Loss: 0.807240\n",
      "\tTesting Loss: 0.813044\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [213/10000000000000]\n",
      "\tTraining Loss: 0.808939\n",
      "\tTesting Loss: 0.819808\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [214/10000000000000]\n",
      "\tTraining Loss: 0.807838\n",
      "\tTesting Loss: 0.812601\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [215/10000000000000]\n",
      "\tTraining Loss: 0.807476\n",
      "\tTesting Loss: 0.808015\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [216/10000000000000]\n",
      "\tTraining Loss: 0.808284\n",
      "\tTesting Loss: 0.809778\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [217/10000000000000]\n",
      "\tTraining Loss: 0.808237\n",
      "\tTesting Loss: 0.807040\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [218/10000000000000]\n",
      "\tTraining Loss: 0.808600\n",
      "\tTesting Loss: 0.822522\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [219/10000000000000]\n",
      "\tTraining Loss: 0.806036\n",
      "\tTesting Loss: 0.811539\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [220/10000000000000]\n",
      "\tTraining Loss: 0.806904\n",
      "\tTesting Loss: 0.813538\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [221/10000000000000]\n",
      "\tTraining Loss: 0.807431\n",
      "\tTesting Loss: 0.808688\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [222/10000000000000]\n",
      "\tTraining Loss: 0.806576\n",
      "\tTesting Loss: 0.820423\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [223/10000000000000]\n",
      "\tTraining Loss: 0.806173\n",
      "\tTesting Loss: 0.806840\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [224/10000000000000]\n",
      "\tTraining Loss: 0.805217\n",
      "\tTesting Loss: 0.805009\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [225/10000000000000]\n",
      "\tTraining Loss: 0.805951\n",
      "\tTesting Loss: 0.801386\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [226/10000000000000]\n",
      "\tTraining Loss: 0.807179\n",
      "\tTesting Loss: 0.804922\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [227/10000000000000]\n",
      "\tTraining Loss: 0.805354\n",
      "\tTesting Loss: 0.807684\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [228/10000000000000]\n",
      "\tTraining Loss: 0.804921\n",
      "\tTesting Loss: 0.803062\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [229/10000000000000]\n",
      "\tTraining Loss: 0.804003\n",
      "\tTesting Loss: 0.814248\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [230/10000000000000]\n",
      "\tTraining Loss: 0.805286\n",
      "\tTesting Loss: 0.805964\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [231/10000000000000]\n",
      "\tTraining Loss: 0.806418\n",
      "\tTesting Loss: 0.815318\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [232/10000000000000]\n",
      "\tTraining Loss: 0.807585\n",
      "\tTesting Loss: 0.821034\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [233/10000000000000]\n",
      "\tTraining Loss: 0.804576\n",
      "\tTesting Loss: 0.808249\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [234/10000000000000]\n",
      "\tTraining Loss: 0.804719\n",
      "\tTesting Loss: 0.806971\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [235/10000000000000]\n",
      "\tTraining Loss: 0.803932\n",
      "\tTesting Loss: 0.812842\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [236/10000000000000]\n",
      "\tTraining Loss: 0.806042\n",
      "\tTesting Loss: 0.819711\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [237/10000000000000]\n",
      "\tTraining Loss: 0.805171\n",
      "\tTesting Loss: 0.804266\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [238/10000000000000]\n",
      "\tTraining Loss: 0.805862\n",
      "\tTesting Loss: 0.809504\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [239/10000000000000]\n",
      "\tTraining Loss: 0.803695\n",
      "\tTesting Loss: 0.806346\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [240/10000000000000]\n",
      "\tTraining Loss: 0.803463\n",
      "\tTesting Loss: 0.805492\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [241/10000000000000]\n",
      "\tTraining Loss: 0.808257\n",
      "\tTesting Loss: 0.805971\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [242/10000000000000]\n",
      "\tTraining Loss: 0.803349\n",
      "\tTesting Loss: 0.807032\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [243/10000000000000]\n",
      "\tTraining Loss: 0.804672\n",
      "\tTesting Loss: 0.805716\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [244/10000000000000]\n",
      "\tTraining Loss: 0.803205\n",
      "\tTesting Loss: 0.803493\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [245/10000000000000]\n",
      "\tTraining Loss: 0.807827\n",
      "\tTesting Loss: 0.820688\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [246/10000000000000]\n",
      "\tTraining Loss: 0.802794\n",
      "\tTesting Loss: 0.804380\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [247/10000000000000]\n",
      "\tTraining Loss: 0.803157\n",
      "\tTesting Loss: 0.810578\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [248/10000000000000]\n",
      "\tTraining Loss: 0.804734\n",
      "\tTesting Loss: 0.810946\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [249/10000000000000]\n",
      "\tTraining Loss: 0.804032\n",
      "\tTesting Loss: 0.802854\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [250/10000000000000]\n",
      "\tTraining Loss: 0.806273\n",
      "\tTesting Loss: 0.807168\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [251/10000000000000]\n",
      "\tTraining Loss: 0.805692\n",
      "\tTesting Loss: 0.807930\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [252/10000000000000]\n",
      "\tTraining Loss: 0.805166\n",
      "\tTesting Loss: 0.809033\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [253/10000000000000]\n",
      "\tTraining Loss: 0.808605\n",
      "\tTesting Loss: 0.813012\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [254/10000000000000]\n",
      "\tTraining Loss: 0.805620\n",
      "\tTesting Loss: 0.808719\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [255/10000000000000]\n",
      "\tTraining Loss: 0.806493\n",
      "\tTesting Loss: 0.808738\n",
      "\tLearning Rate: 0.000800000\n",
      "Epoch [256/10000000000000]\n",
      "\tTraining Loss: 0.806291\n",
      "\tTesting Loss: 0.809557\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [257/10000000000000]\n",
      "\tTraining Loss: 0.802584\n",
      "\tTesting Loss: 0.805320\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [258/10000000000000]\n",
      "\tTraining Loss: 0.801694\n",
      "\tTesting Loss: 0.807546\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [259/10000000000000]\n",
      "\tTraining Loss: 0.801366\n",
      "\tTesting Loss: 0.804769\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [260/10000000000000]\n",
      "\tTraining Loss: 0.802823\n",
      "\tTesting Loss: 0.805166\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [261/10000000000000]\n",
      "\tTraining Loss: 0.801528\n",
      "\tTesting Loss: 0.806633\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [262/10000000000000]\n",
      "\tTraining Loss: 0.801751\n",
      "\tTesting Loss: 0.802967\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [263/10000000000000]\n",
      "\tTraining Loss: 0.801130\n",
      "\tTesting Loss: 0.800954\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [264/10000000000000]\n",
      "\tTraining Loss: 0.801564\n",
      "\tTesting Loss: 0.800085\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [265/10000000000000]\n",
      "\tTraining Loss: 0.801984\n",
      "\tTesting Loss: 0.807991\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [266/10000000000000]\n",
      "\tTraining Loss: 0.801305\n",
      "\tTesting Loss: 0.804324\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [267/10000000000000]\n",
      "\tTraining Loss: 0.800415\n",
      "\tTesting Loss: 0.804432\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [268/10000000000000]\n",
      "\tTraining Loss: 0.798406\n",
      "\tTesting Loss: 0.802131\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [269/10000000000000]\n",
      "\tTraining Loss: 0.798878\n",
      "\tTesting Loss: 0.798719\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [270/10000000000000]\n",
      "\tTraining Loss: 0.799164\n",
      "\tTesting Loss: 0.807202\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [271/10000000000000]\n",
      "\tTraining Loss: 0.799420\n",
      "\tTesting Loss: 0.800381\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [272/10000000000000]\n",
      "\tTraining Loss: 0.799294\n",
      "\tTesting Loss: 0.799674\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [273/10000000000000]\n",
      "\tTraining Loss: 0.799145\n",
      "\tTesting Loss: 0.800623\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [274/10000000000000]\n",
      "\tTraining Loss: 0.799849\n",
      "\tTesting Loss: 0.805158\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [275/10000000000000]\n",
      "\tTraining Loss: 0.798953\n",
      "\tTesting Loss: 0.802297\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [276/10000000000000]\n",
      "\tTraining Loss: 0.797779\n",
      "\tTesting Loss: 0.800245\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [277/10000000000000]\n",
      "\tTraining Loss: 0.799223\n",
      "\tTesting Loss: 0.803330\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [278/10000000000000]\n",
      "\tTraining Loss: 0.798674\n",
      "\tTesting Loss: 0.798606\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [279/10000000000000]\n",
      "\tTraining Loss: 0.798588\n",
      "\tTesting Loss: 0.801146\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [280/10000000000000]\n",
      "\tTraining Loss: 0.799008\n",
      "\tTesting Loss: 0.799552\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [281/10000000000000]\n",
      "\tTraining Loss: 0.799993\n",
      "\tTesting Loss: 0.802668\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [282/10000000000000]\n",
      "\tTraining Loss: 0.800228\n",
      "\tTesting Loss: 0.808230\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [283/10000000000000]\n",
      "\tTraining Loss: 0.798060\n",
      "\tTesting Loss: 0.803730\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [284/10000000000000]\n",
      "\tTraining Loss: 0.798827\n",
      "\tTesting Loss: 0.806199\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [285/10000000000000]\n",
      "\tTraining Loss: 0.797687\n",
      "\tTesting Loss: 0.801514\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [286/10000000000000]\n",
      "\tTraining Loss: 0.799791\n",
      "\tTesting Loss: 0.801948\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [287/10000000000000]\n",
      "\tTraining Loss: 0.795823\n",
      "\tTesting Loss: 0.802402\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [288/10000000000000]\n",
      "\tTraining Loss: 0.798676\n",
      "\tTesting Loss: 0.801211\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [289/10000000000000]\n",
      "\tTraining Loss: 0.798092\n",
      "\tTesting Loss: 0.800964\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [290/10000000000000]\n",
      "\tTraining Loss: 0.798133\n",
      "\tTesting Loss: 0.804956\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [291/10000000000000]\n",
      "\tTraining Loss: 0.799505\n",
      "\tTesting Loss: 0.803257\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [292/10000000000000]\n",
      "\tTraining Loss: 0.797797\n",
      "\tTesting Loss: 0.812767\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [293/10000000000000]\n",
      "\tTraining Loss: 0.798036\n",
      "\tTesting Loss: 0.799889\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [294/10000000000000]\n",
      "\tTraining Loss: 0.799042\n",
      "\tTesting Loss: 0.813533\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [295/10000000000000]\n",
      "\tTraining Loss: 0.797564\n",
      "\tTesting Loss: 0.807418\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [296/10000000000000]\n",
      "\tTraining Loss: 0.796388\n",
      "\tTesting Loss: 0.801394\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [297/10000000000000]\n",
      "\tTraining Loss: 0.797966\n",
      "\tTesting Loss: 0.805495\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [298/10000000000000]\n",
      "\tTraining Loss: 0.796763\n",
      "\tTesting Loss: 0.802992\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [299/10000000000000]\n",
      "\tTraining Loss: 0.799373\n",
      "\tTesting Loss: 0.803124\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [300/10000000000000]\n",
      "\tTraining Loss: 0.798432\n",
      "\tTesting Loss: 0.803878\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [301/10000000000000]\n",
      "\tTraining Loss: 0.799303\n",
      "\tTesting Loss: 0.798925\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [302/10000000000000]\n",
      "\tTraining Loss: 0.798895\n",
      "\tTesting Loss: 0.798835\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [303/10000000000000]\n",
      "\tTraining Loss: 0.796290\n",
      "\tTesting Loss: 0.798825\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [304/10000000000000]\n",
      "\tTraining Loss: 0.800072\n",
      "\tTesting Loss: 0.802356\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [305/10000000000000]\n",
      "\tTraining Loss: 0.796892\n",
      "\tTesting Loss: 0.799985\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [306/10000000000000]\n",
      "\tTraining Loss: 0.796791\n",
      "\tTesting Loss: 0.801930\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [307/10000000000000]\n",
      "\tTraining Loss: 0.797378\n",
      "\tTesting Loss: 0.800583\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [308/10000000000000]\n",
      "\tTraining Loss: 0.797793\n",
      "\tTesting Loss: 0.799014\n",
      "\tLearning Rate: 0.000640000\n",
      "Epoch [309/10000000000000]\n",
      "\tTraining Loss: 0.797368\n",
      "\tTesting Loss: 0.801743\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [310/10000000000000]\n",
      "\tTraining Loss: 0.793849\n",
      "\tTesting Loss: 0.800482\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [311/10000000000000]\n",
      "\tTraining Loss: 0.793326\n",
      "\tTesting Loss: 0.798577\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [312/10000000000000]\n",
      "\tTraining Loss: 0.795575\n",
      "\tTesting Loss: 0.794480\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [313/10000000000000]\n",
      "\tTraining Loss: 0.794943\n",
      "\tTesting Loss: 0.803462\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [314/10000000000000]\n",
      "\tTraining Loss: 0.793903\n",
      "\tTesting Loss: 0.803187\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [315/10000000000000]\n",
      "\tTraining Loss: 0.795485\n",
      "\tTesting Loss: 0.802923\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [316/10000000000000]\n",
      "\tTraining Loss: 0.793376\n",
      "\tTesting Loss: 0.805408\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [317/10000000000000]\n",
      "\tTraining Loss: 0.793416\n",
      "\tTesting Loss: 0.798746\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [318/10000000000000]\n",
      "\tTraining Loss: 0.793342\n",
      "\tTesting Loss: 0.806744\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [319/10000000000000]\n",
      "\tTraining Loss: 0.795570\n",
      "\tTesting Loss: 0.802401\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [320/10000000000000]\n",
      "\tTraining Loss: 0.793191\n",
      "\tTesting Loss: 0.800101\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [321/10000000000000]\n",
      "\tTraining Loss: 0.793866\n",
      "\tTesting Loss: 0.799724\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [322/10000000000000]\n",
      "\tTraining Loss: 0.793211\n",
      "\tTesting Loss: 0.801381\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [323/10000000000000]\n",
      "\tTraining Loss: 0.793411\n",
      "\tTesting Loss: 0.803299\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [324/10000000000000]\n",
      "\tTraining Loss: 0.793735\n",
      "\tTesting Loss: 0.805026\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [325/10000000000000]\n",
      "\tTraining Loss: 0.791621\n",
      "\tTesting Loss: 0.796061\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [326/10000000000000]\n",
      "\tTraining Loss: 0.793572\n",
      "\tTesting Loss: 0.796649\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [327/10000000000000]\n",
      "\tTraining Loss: 0.793799\n",
      "\tTesting Loss: 0.795092\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [328/10000000000000]\n",
      "\tTraining Loss: 0.793011\n",
      "\tTesting Loss: 0.798141\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [329/10000000000000]\n",
      "\tTraining Loss: 0.793269\n",
      "\tTesting Loss: 0.807327\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [330/10000000000000]\n",
      "\tTraining Loss: 0.793500\n",
      "\tTesting Loss: 0.804127\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [331/10000000000000]\n",
      "\tTraining Loss: 0.792751\n",
      "\tTesting Loss: 0.798722\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [332/10000000000000]\n",
      "\tTraining Loss: 0.792085\n",
      "\tTesting Loss: 0.798646\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [333/10000000000000]\n",
      "\tTraining Loss: 0.792486\n",
      "\tTesting Loss: 0.802056\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [334/10000000000000]\n",
      "\tTraining Loss: 0.794389\n",
      "\tTesting Loss: 0.801653\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [335/10000000000000]\n",
      "\tTraining Loss: 0.792288\n",
      "\tTesting Loss: 0.798336\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [336/10000000000000]\n",
      "\tTraining Loss: 0.794090\n",
      "\tTesting Loss: 0.796985\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [337/10000000000000]\n",
      "\tTraining Loss: 0.792835\n",
      "\tTesting Loss: 0.801399\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [338/10000000000000]\n",
      "\tTraining Loss: 0.792294\n",
      "\tTesting Loss: 0.798634\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [339/10000000000000]\n",
      "\tTraining Loss: 0.796001\n",
      "\tTesting Loss: 0.796079\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [340/10000000000000]\n",
      "\tTraining Loss: 0.791590\n",
      "\tTesting Loss: 0.798708\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [341/10000000000000]\n",
      "\tTraining Loss: 0.791902\n",
      "\tTesting Loss: 0.802754\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [342/10000000000000]\n",
      "\tTraining Loss: 0.792271\n",
      "\tTesting Loss: 0.801680\n",
      "\tLearning Rate: 0.000512000\n",
      "Epoch [343/10000000000000]\n",
      "\tTraining Loss: 0.792308\n",
      "\tTesting Loss: 0.804144\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [344/10000000000000]\n",
      "\tTraining Loss: 0.789873\n",
      "\tTesting Loss: 0.797617\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [345/10000000000000]\n",
      "\tTraining Loss: 0.789812\n",
      "\tTesting Loss: 0.796070\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [346/10000000000000]\n",
      "\tTraining Loss: 0.789388\n",
      "\tTesting Loss: 0.797864\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [347/10000000000000]\n",
      "\tTraining Loss: 0.791720\n",
      "\tTesting Loss: 0.796262\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [348/10000000000000]\n",
      "\tTraining Loss: 0.789578\n",
      "\tTesting Loss: 0.796316\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [349/10000000000000]\n",
      "\tTraining Loss: 0.789666\n",
      "\tTesting Loss: 0.795511\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [350/10000000000000]\n",
      "\tTraining Loss: 0.789536\n",
      "\tTesting Loss: 0.798742\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [351/10000000000000]\n",
      "\tTraining Loss: 0.789119\n",
      "\tTesting Loss: 0.795522\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [352/10000000000000]\n",
      "\tTraining Loss: 0.788279\n",
      "\tTesting Loss: 0.795206\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [353/10000000000000]\n",
      "\tTraining Loss: 0.789566\n",
      "\tTesting Loss: 0.797352\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [354/10000000000000]\n",
      "\tTraining Loss: 0.787871\n",
      "\tTesting Loss: 0.799054\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [355/10000000000000]\n",
      "\tTraining Loss: 0.788857\n",
      "\tTesting Loss: 0.794307\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [356/10000000000000]\n",
      "\tTraining Loss: 0.789849\n",
      "\tTesting Loss: 0.797043\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [357/10000000000000]\n",
      "\tTraining Loss: 0.787342\n",
      "\tTesting Loss: 0.795818\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [358/10000000000000]\n",
      "\tTraining Loss: 0.789780\n",
      "\tTesting Loss: 0.795303\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [359/10000000000000]\n",
      "\tTraining Loss: 0.788915\n",
      "\tTesting Loss: 0.795996\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [360/10000000000000]\n",
      "\tTraining Loss: 0.789286\n",
      "\tTesting Loss: 0.793689\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [361/10000000000000]\n",
      "\tTraining Loss: 0.788362\n",
      "\tTesting Loss: 0.793725\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [362/10000000000000]\n",
      "\tTraining Loss: 0.789602\n",
      "\tTesting Loss: 0.796853\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [363/10000000000000]\n",
      "\tTraining Loss: 0.790318\n",
      "\tTesting Loss: 0.800808\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [364/10000000000000]\n",
      "\tTraining Loss: 0.789440\n",
      "\tTesting Loss: 0.797051\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [365/10000000000000]\n",
      "\tTraining Loss: 0.788448\n",
      "\tTesting Loss: 0.796640\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [366/10000000000000]\n",
      "\tTraining Loss: 0.787833\n",
      "\tTesting Loss: 0.798169\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [367/10000000000000]\n",
      "\tTraining Loss: 0.788591\n",
      "\tTesting Loss: 0.802115\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [368/10000000000000]\n",
      "\tTraining Loss: 0.790900\n",
      "\tTesting Loss: 0.794513\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [369/10000000000000]\n",
      "\tTraining Loss: 0.787820\n",
      "\tTesting Loss: 0.796409\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [370/10000000000000]\n",
      "\tTraining Loss: 0.788346\n",
      "\tTesting Loss: 0.795331\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [371/10000000000000]\n",
      "\tTraining Loss: 0.788565\n",
      "\tTesting Loss: 0.796404\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [372/10000000000000]\n",
      "\tTraining Loss: 0.787187\n",
      "\tTesting Loss: 0.793639\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [373/10000000000000]\n",
      "\tTraining Loss: 0.787501\n",
      "\tTesting Loss: 0.794439\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [374/10000000000000]\n",
      "\tTraining Loss: 0.787363\n",
      "\tTesting Loss: 0.793224\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [375/10000000000000]\n",
      "\tTraining Loss: 0.787330\n",
      "\tTesting Loss: 0.795833\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [376/10000000000000]\n",
      "\tTraining Loss: 0.786935\n",
      "\tTesting Loss: 0.797391\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [377/10000000000000]\n",
      "\tTraining Loss: 0.786934\n",
      "\tTesting Loss: 0.794077\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [378/10000000000000]\n",
      "\tTraining Loss: 0.787320\n",
      "\tTesting Loss: 0.802831\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [379/10000000000000]\n",
      "\tTraining Loss: 0.787248\n",
      "\tTesting Loss: 0.793077\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [380/10000000000000]\n",
      "\tTraining Loss: 0.788410\n",
      "\tTesting Loss: 0.807570\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [381/10000000000000]\n",
      "\tTraining Loss: 0.787909\n",
      "\tTesting Loss: 0.800358\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [382/10000000000000]\n",
      "\tTraining Loss: 0.786452\n",
      "\tTesting Loss: 0.795295\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [383/10000000000000]\n",
      "\tTraining Loss: 0.787148\n",
      "\tTesting Loss: 0.794119\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [384/10000000000000]\n",
      "\tTraining Loss: 0.786106\n",
      "\tTesting Loss: 0.793862\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [385/10000000000000]\n",
      "\tTraining Loss: 0.787097\n",
      "\tTesting Loss: 0.795853\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [386/10000000000000]\n",
      "\tTraining Loss: 0.785980\n",
      "\tTesting Loss: 0.796138\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [387/10000000000000]\n",
      "\tTraining Loss: 0.786306\n",
      "\tTesting Loss: 0.799803\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [388/10000000000000]\n",
      "\tTraining Loss: 0.787780\n",
      "\tTesting Loss: 0.793731\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [389/10000000000000]\n",
      "\tTraining Loss: 0.788240\n",
      "\tTesting Loss: 0.793203\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [390/10000000000000]\n",
      "\tTraining Loss: 0.786365\n",
      "\tTesting Loss: 0.794066\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [391/10000000000000]\n",
      "\tTraining Loss: 0.787174\n",
      "\tTesting Loss: 0.791527\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [392/10000000000000]\n",
      "\tTraining Loss: 0.786151\n",
      "\tTesting Loss: 0.794804\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [393/10000000000000]\n",
      "\tTraining Loss: 0.787725\n",
      "\tTesting Loss: 0.798665\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [394/10000000000000]\n",
      "\tTraining Loss: 0.787100\n",
      "\tTesting Loss: 0.795093\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [395/10000000000000]\n",
      "\tTraining Loss: 0.785809\n",
      "\tTesting Loss: 0.793350\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [396/10000000000000]\n",
      "\tTraining Loss: 0.786689\n",
      "\tTesting Loss: 0.793148\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [397/10000000000000]\n",
      "\tTraining Loss: 0.786491\n",
      "\tTesting Loss: 0.795717\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [398/10000000000000]\n",
      "\tTraining Loss: 0.785496\n",
      "\tTesting Loss: 0.804504\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [399/10000000000000]\n",
      "\tTraining Loss: 0.785715\n",
      "\tTesting Loss: 0.796825\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [400/10000000000000]\n",
      "\tTraining Loss: 0.787677\n",
      "\tTesting Loss: 0.796804\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [401/10000000000000]\n",
      "\tTraining Loss: 0.785782\n",
      "\tTesting Loss: 0.798539\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [402/10000000000000]\n",
      "\tTraining Loss: 0.787120\n",
      "\tTesting Loss: 0.797491\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [403/10000000000000]\n",
      "\tTraining Loss: 0.785452\n",
      "\tTesting Loss: 0.795741\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [404/10000000000000]\n",
      "\tTraining Loss: 0.786640\n",
      "\tTesting Loss: 0.791863\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [405/10000000000000]\n",
      "\tTraining Loss: 0.785963\n",
      "\tTesting Loss: 0.797119\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [406/10000000000000]\n",
      "\tTraining Loss: 0.787582\n",
      "\tTesting Loss: 0.807314\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [407/10000000000000]\n",
      "\tTraining Loss: 0.786178\n",
      "\tTesting Loss: 0.792573\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [408/10000000000000]\n",
      "\tTraining Loss: 0.784676\n",
      "\tTesting Loss: 0.791988\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [409/10000000000000]\n",
      "\tTraining Loss: 0.785922\n",
      "\tTesting Loss: 0.803162\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [410/10000000000000]\n",
      "\tTraining Loss: 0.784760\n",
      "\tTesting Loss: 0.796030\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [411/10000000000000]\n",
      "\tTraining Loss: 0.786453\n",
      "\tTesting Loss: 0.799480\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [412/10000000000000]\n",
      "\tTraining Loss: 0.786579\n",
      "\tTesting Loss: 0.804029\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [413/10000000000000]\n",
      "\tTraining Loss: 0.785335\n",
      "\tTesting Loss: 0.797193\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [414/10000000000000]\n",
      "\tTraining Loss: 0.784914\n",
      "\tTesting Loss: 0.797699\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [415/10000000000000]\n",
      "\tTraining Loss: 0.783923\n",
      "\tTesting Loss: 0.792447\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [416/10000000000000]\n",
      "\tTraining Loss: 0.785460\n",
      "\tTesting Loss: 0.798370\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [417/10000000000000]\n",
      "\tTraining Loss: 0.785748\n",
      "\tTesting Loss: 0.793547\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [418/10000000000000]\n",
      "\tTraining Loss: 0.784399\n",
      "\tTesting Loss: 0.795396\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [419/10000000000000]\n",
      "\tTraining Loss: 0.785170\n",
      "\tTesting Loss: 0.794020\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [420/10000000000000]\n",
      "\tTraining Loss: 0.784296\n",
      "\tTesting Loss: 0.803173\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [421/10000000000000]\n",
      "\tTraining Loss: 0.785798\n",
      "\tTesting Loss: 0.795517\n",
      "\tLearning Rate: 0.000409600\n",
      "Epoch [422/10000000000000]\n",
      "\tTraining Loss: 0.784848\n",
      "\tTesting Loss: 0.794548\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [423/10000000000000]\n",
      "\tTraining Loss: 0.782352\n",
      "\tTesting Loss: 0.795507\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [424/10000000000000]\n",
      "\tTraining Loss: 0.783149\n",
      "\tTesting Loss: 0.791174\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [425/10000000000000]\n",
      "\tTraining Loss: 0.781299\n",
      "\tTesting Loss: 0.791422\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [426/10000000000000]\n",
      "\tTraining Loss: 0.782270\n",
      "\tTesting Loss: 0.794925\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [427/10000000000000]\n",
      "\tTraining Loss: 0.782343\n",
      "\tTesting Loss: 0.800309\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [428/10000000000000]\n",
      "\tTraining Loss: 0.784021\n",
      "\tTesting Loss: 0.804187\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [429/10000000000000]\n",
      "\tTraining Loss: 0.782105\n",
      "\tTesting Loss: 0.792168\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [430/10000000000000]\n",
      "\tTraining Loss: 0.782863\n",
      "\tTesting Loss: 0.796883\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [431/10000000000000]\n",
      "\tTraining Loss: 0.783455\n",
      "\tTesting Loss: 0.796719\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [432/10000000000000]\n",
      "\tTraining Loss: 0.781007\n",
      "\tTesting Loss: 0.792487\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [433/10000000000000]\n",
      "\tTraining Loss: 0.781520\n",
      "\tTesting Loss: 0.791665\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [434/10000000000000]\n",
      "\tTraining Loss: 0.781157\n",
      "\tTesting Loss: 0.794695\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [435/10000000000000]\n",
      "\tTraining Loss: 0.781558\n",
      "\tTesting Loss: 0.793591\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [436/10000000000000]\n",
      "\tTraining Loss: 0.779893\n",
      "\tTesting Loss: 0.790563\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [437/10000000000000]\n",
      "\tTraining Loss: 0.781830\n",
      "\tTesting Loss: 0.792790\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [438/10000000000000]\n",
      "\tTraining Loss: 0.780316\n",
      "\tTesting Loss: 0.794250\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [439/10000000000000]\n",
      "\tTraining Loss: 0.780983\n",
      "\tTesting Loss: 0.792271\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [440/10000000000000]\n",
      "\tTraining Loss: 0.782175\n",
      "\tTesting Loss: 0.791575\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [441/10000000000000]\n",
      "\tTraining Loss: 0.782127\n",
      "\tTesting Loss: 0.794069\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [442/10000000000000]\n",
      "\tTraining Loss: 0.781320\n",
      "\tTesting Loss: 0.794810\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [443/10000000000000]\n",
      "\tTraining Loss: 0.781342\n",
      "\tTesting Loss: 0.792260\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [444/10000000000000]\n",
      "\tTraining Loss: 0.781209\n",
      "\tTesting Loss: 0.790199\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [445/10000000000000]\n",
      "\tTraining Loss: 0.779876\n",
      "\tTesting Loss: 0.790199\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [446/10000000000000]\n",
      "\tTraining Loss: 0.780767\n",
      "\tTesting Loss: 0.791984\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [447/10000000000000]\n",
      "\tTraining Loss: 0.780392\n",
      "\tTesting Loss: 0.791740\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [448/10000000000000]\n",
      "\tTraining Loss: 0.780297\n",
      "\tTesting Loss: 0.792241\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [449/10000000000000]\n",
      "\tTraining Loss: 0.780770\n",
      "\tTesting Loss: 0.792323\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [450/10000000000000]\n",
      "\tTraining Loss: 0.781335\n",
      "\tTesting Loss: 0.800576\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [451/10000000000000]\n",
      "\tTraining Loss: 0.781287\n",
      "\tTesting Loss: 0.792300\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [452/10000000000000]\n",
      "\tTraining Loss: 0.781829\n",
      "\tTesting Loss: 0.792593\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [453/10000000000000]\n",
      "\tTraining Loss: 0.780489\n",
      "\tTesting Loss: 0.793156\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [454/10000000000000]\n",
      "\tTraining Loss: 0.780030\n",
      "\tTesting Loss: 0.795747\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [455/10000000000000]\n",
      "\tTraining Loss: 0.779782\n",
      "\tTesting Loss: 0.792631\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [456/10000000000000]\n",
      "\tTraining Loss: 0.780207\n",
      "\tTesting Loss: 0.794147\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [457/10000000000000]\n",
      "\tTraining Loss: 0.780902\n",
      "\tTesting Loss: 0.791491\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [458/10000000000000]\n",
      "\tTraining Loss: 0.780550\n",
      "\tTesting Loss: 0.791565\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [459/10000000000000]\n",
      "\tTraining Loss: 0.779705\n",
      "\tTesting Loss: 0.791755\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [460/10000000000000]\n",
      "\tTraining Loss: 0.778225\n",
      "\tTesting Loss: 0.790973\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [461/10000000000000]\n",
      "\tTraining Loss: 0.779589\n",
      "\tTesting Loss: 0.790963\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [462/10000000000000]\n",
      "\tTraining Loss: 0.780091\n",
      "\tTesting Loss: 0.793744\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [463/10000000000000]\n",
      "\tTraining Loss: 0.779196\n",
      "\tTesting Loss: 0.796792\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [464/10000000000000]\n",
      "\tTraining Loss: 0.780491\n",
      "\tTesting Loss: 0.798234\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [465/10000000000000]\n",
      "\tTraining Loss: 0.779544\n",
      "\tTesting Loss: 0.793639\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [466/10000000000000]\n",
      "\tTraining Loss: 0.779821\n",
      "\tTesting Loss: 0.788828\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [467/10000000000000]\n",
      "\tTraining Loss: 0.780483\n",
      "\tTesting Loss: 0.789830\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [468/10000000000000]\n",
      "\tTraining Loss: 0.779778\n",
      "\tTesting Loss: 0.791305\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [469/10000000000000]\n",
      "\tTraining Loss: 0.778604\n",
      "\tTesting Loss: 0.791178\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [470/10000000000000]\n",
      "\tTraining Loss: 0.779186\n",
      "\tTesting Loss: 0.796368\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [471/10000000000000]\n",
      "\tTraining Loss: 0.779090\n",
      "\tTesting Loss: 0.792537\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [472/10000000000000]\n",
      "\tTraining Loss: 0.777743\n",
      "\tTesting Loss: 0.801122\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [473/10000000000000]\n",
      "\tTraining Loss: 0.778253\n",
      "\tTesting Loss: 0.799252\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [474/10000000000000]\n",
      "\tTraining Loss: 0.779178\n",
      "\tTesting Loss: 0.797131\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [475/10000000000000]\n",
      "\tTraining Loss: 0.778875\n",
      "\tTesting Loss: 0.792423\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [476/10000000000000]\n",
      "\tTraining Loss: 0.778759\n",
      "\tTesting Loss: 0.793519\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [477/10000000000000]\n",
      "\tTraining Loss: 0.778285\n",
      "\tTesting Loss: 0.793579\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [478/10000000000000]\n",
      "\tTraining Loss: 0.777999\n",
      "\tTesting Loss: 0.790104\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [479/10000000000000]\n",
      "\tTraining Loss: 0.778639\n",
      "\tTesting Loss: 0.792531\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [480/10000000000000]\n",
      "\tTraining Loss: 0.778635\n",
      "\tTesting Loss: 0.791513\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [481/10000000000000]\n",
      "\tTraining Loss: 0.778236\n",
      "\tTesting Loss: 0.792848\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [482/10000000000000]\n",
      "\tTraining Loss: 0.778622\n",
      "\tTesting Loss: 0.789677\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [483/10000000000000]\n",
      "\tTraining Loss: 0.778471\n",
      "\tTesting Loss: 0.792087\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [484/10000000000000]\n",
      "\tTraining Loss: 0.777758\n",
      "\tTesting Loss: 0.792720\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [485/10000000000000]\n",
      "\tTraining Loss: 0.779905\n",
      "\tTesting Loss: 0.790281\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [486/10000000000000]\n",
      "\tTraining Loss: 0.779990\n",
      "\tTesting Loss: 0.792413\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [487/10000000000000]\n",
      "\tTraining Loss: 0.779670\n",
      "\tTesting Loss: 0.796349\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [488/10000000000000]\n",
      "\tTraining Loss: 0.778553\n",
      "\tTesting Loss: 0.789458\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [489/10000000000000]\n",
      "\tTraining Loss: 0.778596\n",
      "\tTesting Loss: 0.794058\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [490/10000000000000]\n",
      "\tTraining Loss: 0.778757\n",
      "\tTesting Loss: 0.793062\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [491/10000000000000]\n",
      "\tTraining Loss: 0.778201\n",
      "\tTesting Loss: 0.791453\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [492/10000000000000]\n",
      "\tTraining Loss: 0.779420\n",
      "\tTesting Loss: 0.791709\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [493/10000000000000]\n",
      "\tTraining Loss: 0.780106\n",
      "\tTesting Loss: 0.794315\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [494/10000000000000]\n",
      "\tTraining Loss: 0.779977\n",
      "\tTesting Loss: 0.795837\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [495/10000000000000]\n",
      "\tTraining Loss: 0.780942\n",
      "\tTesting Loss: 0.797077\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [496/10000000000000]\n",
      "\tTraining Loss: 0.778175\n",
      "\tTesting Loss: 0.790769\n",
      "\tLearning Rate: 0.000327680\n",
      "Epoch [497/10000000000000]\n",
      "\tTraining Loss: 0.778531\n",
      "\tTesting Loss: 0.795450\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [498/10000000000000]\n",
      "\tTraining Loss: 0.776390\n",
      "\tTesting Loss: 0.789647\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [499/10000000000000]\n",
      "\tTraining Loss: 0.776971\n",
      "\tTesting Loss: 0.791231\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [500/10000000000000]\n",
      "\tTraining Loss: 0.776476\n",
      "\tTesting Loss: 0.792723\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [501/10000000000000]\n",
      "\tTraining Loss: 0.776106\n",
      "\tTesting Loss: 0.791450\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [502/10000000000000]\n",
      "\tTraining Loss: 0.776151\n",
      "\tTesting Loss: 0.790521\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [503/10000000000000]\n",
      "\tTraining Loss: 0.775900\n",
      "\tTesting Loss: 0.791489\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [504/10000000000000]\n",
      "\tTraining Loss: 0.776528\n",
      "\tTesting Loss: 0.792121\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [505/10000000000000]\n",
      "\tTraining Loss: 0.776466\n",
      "\tTesting Loss: 0.791900\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [506/10000000000000]\n",
      "\tTraining Loss: 0.777348\n",
      "\tTesting Loss: 0.792409\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [507/10000000000000]\n",
      "\tTraining Loss: 0.776676\n",
      "\tTesting Loss: 0.789737\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [508/10000000000000]\n",
      "\tTraining Loss: 0.776433\n",
      "\tTesting Loss: 0.791998\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [509/10000000000000]\n",
      "\tTraining Loss: 0.775595\n",
      "\tTesting Loss: 0.789539\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [510/10000000000000]\n",
      "\tTraining Loss: 0.775257\n",
      "\tTesting Loss: 0.790487\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [511/10000000000000]\n",
      "\tTraining Loss: 0.776140\n",
      "\tTesting Loss: 0.794683\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [512/10000000000000]\n",
      "\tTraining Loss: 0.775082\n",
      "\tTesting Loss: 0.789306\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [513/10000000000000]\n",
      "\tTraining Loss: 0.775460\n",
      "\tTesting Loss: 0.789225\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [514/10000000000000]\n",
      "\tTraining Loss: 0.775038\n",
      "\tTesting Loss: 0.787817\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [515/10000000000000]\n",
      "\tTraining Loss: 0.774526\n",
      "\tTesting Loss: 0.790211\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [516/10000000000000]\n",
      "\tTraining Loss: 0.775602\n",
      "\tTesting Loss: 0.793716\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [517/10000000000000]\n",
      "\tTraining Loss: 0.774801\n",
      "\tTesting Loss: 0.790286\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [518/10000000000000]\n",
      "\tTraining Loss: 0.774906\n",
      "\tTesting Loss: 0.798392\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [519/10000000000000]\n",
      "\tTraining Loss: 0.774865\n",
      "\tTesting Loss: 0.789098\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [520/10000000000000]\n",
      "\tTraining Loss: 0.774283\n",
      "\tTesting Loss: 0.790087\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [521/10000000000000]\n",
      "\tTraining Loss: 0.774301\n",
      "\tTesting Loss: 0.788646\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [522/10000000000000]\n",
      "\tTraining Loss: 0.774366\n",
      "\tTesting Loss: 0.789002\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [523/10000000000000]\n",
      "\tTraining Loss: 0.774796\n",
      "\tTesting Loss: 0.790147\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [524/10000000000000]\n",
      "\tTraining Loss: 0.775042\n",
      "\tTesting Loss: 0.789536\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [525/10000000000000]\n",
      "\tTraining Loss: 0.775915\n",
      "\tTesting Loss: 0.789799\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [526/10000000000000]\n",
      "\tTraining Loss: 0.773403\n",
      "\tTesting Loss: 0.791010\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [527/10000000000000]\n",
      "\tTraining Loss: 0.774358\n",
      "\tTesting Loss: 0.789580\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [528/10000000000000]\n",
      "\tTraining Loss: 0.774355\n",
      "\tTesting Loss: 0.789973\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [529/10000000000000]\n",
      "\tTraining Loss: 0.774100\n",
      "\tTesting Loss: 0.789662\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [530/10000000000000]\n",
      "\tTraining Loss: 0.774375\n",
      "\tTesting Loss: 0.789668\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [531/10000000000000]\n",
      "\tTraining Loss: 0.774829\n",
      "\tTesting Loss: 0.789531\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [532/10000000000000]\n",
      "\tTraining Loss: 0.774321\n",
      "\tTesting Loss: 0.789244\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [533/10000000000000]\n",
      "\tTraining Loss: 0.773748\n",
      "\tTesting Loss: 0.790555\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [534/10000000000000]\n",
      "\tTraining Loss: 0.774913\n",
      "\tTesting Loss: 0.790830\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [535/10000000000000]\n",
      "\tTraining Loss: 0.773700\n",
      "\tTesting Loss: 0.789776\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [536/10000000000000]\n",
      "\tTraining Loss: 0.773803\n",
      "\tTesting Loss: 0.789386\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [537/10000000000000]\n",
      "\tTraining Loss: 0.773046\n",
      "\tTesting Loss: 0.787257\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [538/10000000000000]\n",
      "\tTraining Loss: 0.772539\n",
      "\tTesting Loss: 0.793737\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [539/10000000000000]\n",
      "\tTraining Loss: 0.773277\n",
      "\tTesting Loss: 0.791512\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [540/10000000000000]\n",
      "\tTraining Loss: 0.773277\n",
      "\tTesting Loss: 0.791398\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [541/10000000000000]\n",
      "\tTraining Loss: 0.773038\n",
      "\tTesting Loss: 0.789175\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [542/10000000000000]\n",
      "\tTraining Loss: 0.773820\n",
      "\tTesting Loss: 0.789468\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [543/10000000000000]\n",
      "\tTraining Loss: 0.772376\n",
      "\tTesting Loss: 0.790146\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [544/10000000000000]\n",
      "\tTraining Loss: 0.775336\n",
      "\tTesting Loss: 0.794892\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [545/10000000000000]\n",
      "\tTraining Loss: 0.775084\n",
      "\tTesting Loss: 0.789779\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [546/10000000000000]\n",
      "\tTraining Loss: 0.773518\n",
      "\tTesting Loss: 0.790196\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [547/10000000000000]\n",
      "\tTraining Loss: 0.774471\n",
      "\tTesting Loss: 0.791348\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [548/10000000000000]\n",
      "\tTraining Loss: 0.775154\n",
      "\tTesting Loss: 0.789059\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [549/10000000000000]\n",
      "\tTraining Loss: 0.774232\n",
      "\tTesting Loss: 0.791648\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [550/10000000000000]\n",
      "\tTraining Loss: 0.773339\n",
      "\tTesting Loss: 0.789118\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [551/10000000000000]\n",
      "\tTraining Loss: 0.774851\n",
      "\tTesting Loss: 0.790092\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [552/10000000000000]\n",
      "\tTraining Loss: 0.774083\n",
      "\tTesting Loss: 0.790691\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [553/10000000000000]\n",
      "\tTraining Loss: 0.774171\n",
      "\tTesting Loss: 0.793777\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [554/10000000000000]\n",
      "\tTraining Loss: 0.775045\n",
      "\tTesting Loss: 0.790624\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [555/10000000000000]\n",
      "\tTraining Loss: 0.774578\n",
      "\tTesting Loss: 0.788697\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [556/10000000000000]\n",
      "\tTraining Loss: 0.776390\n",
      "\tTesting Loss: 0.792110\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [557/10000000000000]\n",
      "\tTraining Loss: 0.774248\n",
      "\tTesting Loss: 0.787394\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [558/10000000000000]\n",
      "\tTraining Loss: 0.773829\n",
      "\tTesting Loss: 0.791233\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [559/10000000000000]\n",
      "\tTraining Loss: 0.773598\n",
      "\tTesting Loss: 0.790345\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [560/10000000000000]\n",
      "\tTraining Loss: 0.773903\n",
      "\tTesting Loss: 0.789185\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [561/10000000000000]\n",
      "\tTraining Loss: 0.773593\n",
      "\tTesting Loss: 0.791913\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [562/10000000000000]\n",
      "\tTraining Loss: 0.773589\n",
      "\tTesting Loss: 0.788993\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [563/10000000000000]\n",
      "\tTraining Loss: 0.772890\n",
      "\tTesting Loss: 0.794297\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [564/10000000000000]\n",
      "\tTraining Loss: 0.773465\n",
      "\tTesting Loss: 0.789207\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [565/10000000000000]\n",
      "\tTraining Loss: 0.773869\n",
      "\tTesting Loss: 0.791889\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [566/10000000000000]\n",
      "\tTraining Loss: 0.772853\n",
      "\tTesting Loss: 0.792814\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [567/10000000000000]\n",
      "\tTraining Loss: 0.773589\n",
      "\tTesting Loss: 0.789576\n",
      "\tLearning Rate: 0.000262144\n",
      "Epoch [568/10000000000000]\n",
      "\tTraining Loss: 0.773475\n",
      "\tTesting Loss: 0.790792\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [569/10000000000000]\n",
      "\tTraining Loss: 0.771399\n",
      "\tTesting Loss: 0.789074\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [570/10000000000000]\n",
      "\tTraining Loss: 0.771395\n",
      "\tTesting Loss: 0.791215\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [571/10000000000000]\n",
      "\tTraining Loss: 0.771805\n",
      "\tTesting Loss: 0.789656\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [572/10000000000000]\n",
      "\tTraining Loss: 0.771793\n",
      "\tTesting Loss: 0.791048\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [573/10000000000000]\n",
      "\tTraining Loss: 0.772269\n",
      "\tTesting Loss: 0.790785\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [574/10000000000000]\n",
      "\tTraining Loss: 0.771880\n",
      "\tTesting Loss: 0.789866\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [575/10000000000000]\n",
      "\tTraining Loss: 0.771238\n",
      "\tTesting Loss: 0.788414\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [576/10000000000000]\n",
      "\tTraining Loss: 0.771350\n",
      "\tTesting Loss: 0.791315\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [577/10000000000000]\n",
      "\tTraining Loss: 0.771338\n",
      "\tTesting Loss: 0.793670\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [578/10000000000000]\n",
      "\tTraining Loss: 0.771639\n",
      "\tTesting Loss: 0.789914\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [579/10000000000000]\n",
      "\tTraining Loss: 0.770876\n",
      "\tTesting Loss: 0.791938\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [580/10000000000000]\n",
      "\tTraining Loss: 0.771472\n",
      "\tTesting Loss: 0.789707\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [581/10000000000000]\n",
      "\tTraining Loss: 0.770805\n",
      "\tTesting Loss: 0.788803\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [582/10000000000000]\n",
      "\tTraining Loss: 0.770694\n",
      "\tTesting Loss: 0.787371\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [583/10000000000000]\n",
      "\tTraining Loss: 0.770109\n",
      "\tTesting Loss: 0.789950\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [584/10000000000000]\n",
      "\tTraining Loss: 0.770815\n",
      "\tTesting Loss: 0.790815\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [585/10000000000000]\n",
      "\tTraining Loss: 0.770661\n",
      "\tTesting Loss: 0.790544\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [586/10000000000000]\n",
      "\tTraining Loss: 0.770867\n",
      "\tTesting Loss: 0.788086\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [587/10000000000000]\n",
      "\tTraining Loss: 0.769920\n",
      "\tTesting Loss: 0.790237\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [588/10000000000000]\n",
      "\tTraining Loss: 0.770470\n",
      "\tTesting Loss: 0.790541\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [589/10000000000000]\n",
      "\tTraining Loss: 0.770403\n",
      "\tTesting Loss: 0.788835\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [590/10000000000000]\n",
      "\tTraining Loss: 0.770149\n",
      "\tTesting Loss: 0.788023\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [591/10000000000000]\n",
      "\tTraining Loss: 0.770249\n",
      "\tTesting Loss: 0.789437\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [592/10000000000000]\n",
      "\tTraining Loss: 0.770127\n",
      "\tTesting Loss: 0.789974\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [593/10000000000000]\n",
      "\tTraining Loss: 0.770358\n",
      "\tTesting Loss: 0.793691\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [594/10000000000000]\n",
      "\tTraining Loss: 0.769766\n",
      "\tTesting Loss: 0.788174\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [595/10000000000000]\n",
      "\tTraining Loss: 0.769980\n",
      "\tTesting Loss: 0.787973\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [596/10000000000000]\n",
      "\tTraining Loss: 0.770209\n",
      "\tTesting Loss: 0.791035\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [597/10000000000000]\n",
      "\tTraining Loss: 0.769512\n",
      "\tTesting Loss: 0.788676\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [598/10000000000000]\n",
      "\tTraining Loss: 0.769781\n",
      "\tTesting Loss: 0.790361\n",
      "\tLearning Rate: 0.000209715\n",
      "Epoch [599/10000000000000]\n",
      "\tTraining Loss: 0.769652\n",
      "\tTesting Loss: 0.789064\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [600/10000000000000]\n",
      "\tTraining Loss: 0.768064\n",
      "\tTesting Loss: 0.790865\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [601/10000000000000]\n",
      "\tTraining Loss: 0.768040\n",
      "\tTesting Loss: 0.788600\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [602/10000000000000]\n",
      "\tTraining Loss: 0.768160\n",
      "\tTesting Loss: 0.789577\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [603/10000000000000]\n",
      "\tTraining Loss: 0.767818\n",
      "\tTesting Loss: 0.788105\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [604/10000000000000]\n",
      "\tTraining Loss: 0.768008\n",
      "\tTesting Loss: 0.788660\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [605/10000000000000]\n",
      "\tTraining Loss: 0.768279\n",
      "\tTesting Loss: 0.788703\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [606/10000000000000]\n",
      "\tTraining Loss: 0.767723\n",
      "\tTesting Loss: 0.790039\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [607/10000000000000]\n",
      "\tTraining Loss: 0.767546\n",
      "\tTesting Loss: 0.788393\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [608/10000000000000]\n",
      "\tTraining Loss: 0.769243\n",
      "\tTesting Loss: 0.790900\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [609/10000000000000]\n",
      "\tTraining Loss: 0.768645\n",
      "\tTesting Loss: 0.788943\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [610/10000000000000]\n",
      "\tTraining Loss: 0.769163\n",
      "\tTesting Loss: 0.789921\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [611/10000000000000]\n",
      "\tTraining Loss: 0.769895\n",
      "\tTesting Loss: 0.789897\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [612/10000000000000]\n",
      "\tTraining Loss: 0.769153\n",
      "\tTesting Loss: 0.789233\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [613/10000000000000]\n",
      "\tTraining Loss: 0.769006\n",
      "\tTesting Loss: 0.789852\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [614/10000000000000]\n",
      "\tTraining Loss: 0.768996\n",
      "\tTesting Loss: 0.789019\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [615/10000000000000]\n",
      "\tTraining Loss: 0.768959\n",
      "\tTesting Loss: 0.791538\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [616/10000000000000]\n",
      "\tTraining Loss: 0.768972\n",
      "\tTesting Loss: 0.791230\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [617/10000000000000]\n",
      "\tTraining Loss: 0.768277\n",
      "\tTesting Loss: 0.790956\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [618/10000000000000]\n",
      "\tTraining Loss: 0.768347\n",
      "\tTesting Loss: 0.789876\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [619/10000000000000]\n",
      "\tTraining Loss: 0.768768\n",
      "\tTesting Loss: 0.790236\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [620/10000000000000]\n",
      "\tTraining Loss: 0.768099\n",
      "\tTesting Loss: 0.789468\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [621/10000000000000]\n",
      "\tTraining Loss: 0.768822\n",
      "\tTesting Loss: 0.792537\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [622/10000000000000]\n",
      "\tTraining Loss: 0.768416\n",
      "\tTesting Loss: 0.789495\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [623/10000000000000]\n",
      "\tTraining Loss: 0.767603\n",
      "\tTesting Loss: 0.789381\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [624/10000000000000]\n",
      "\tTraining Loss: 0.768142\n",
      "\tTesting Loss: 0.788313\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [625/10000000000000]\n",
      "\tTraining Loss: 0.767982\n",
      "\tTesting Loss: 0.787206\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [626/10000000000000]\n",
      "\tTraining Loss: 0.768017\n",
      "\tTesting Loss: 0.789794\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [627/10000000000000]\n",
      "\tTraining Loss: 0.768007\n",
      "\tTesting Loss: 0.790471\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [628/10000000000000]\n",
      "\tTraining Loss: 0.769032\n",
      "\tTesting Loss: 0.789375\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [629/10000000000000]\n",
      "\tTraining Loss: 0.768301\n",
      "\tTesting Loss: 0.791121\n",
      "\tLearning Rate: 0.000167772\n",
      "Epoch [630/10000000000000]\n",
      "\tTraining Loss: 0.768493\n",
      "\tTesting Loss: 0.787900\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [631/10000000000000]\n",
      "\tTraining Loss: 0.767200\n",
      "\tTesting Loss: 0.788510\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [632/10000000000000]\n",
      "\tTraining Loss: 0.767654\n",
      "\tTesting Loss: 0.791949\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [633/10000000000000]\n",
      "\tTraining Loss: 0.766957\n",
      "\tTesting Loss: 0.788537\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [634/10000000000000]\n",
      "\tTraining Loss: 0.767021\n",
      "\tTesting Loss: 0.788058\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [635/10000000000000]\n",
      "\tTraining Loss: 0.766832\n",
      "\tTesting Loss: 0.788893\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [636/10000000000000]\n",
      "\tTraining Loss: 0.768886\n",
      "\tTesting Loss: 0.790498\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [637/10000000000000]\n",
      "\tTraining Loss: 0.767817\n",
      "\tTesting Loss: 0.788595\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [638/10000000000000]\n",
      "\tTraining Loss: 0.767392\n",
      "\tTesting Loss: 0.788571\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [639/10000000000000]\n",
      "\tTraining Loss: 0.767414\n",
      "\tTesting Loss: 0.788769\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [640/10000000000000]\n",
      "\tTraining Loss: 0.767371\n",
      "\tTesting Loss: 0.787397\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [641/10000000000000]\n",
      "\tTraining Loss: 0.767095\n",
      "\tTesting Loss: 0.789079\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [642/10000000000000]\n",
      "\tTraining Loss: 0.766626\n",
      "\tTesting Loss: 0.792786\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [643/10000000000000]\n",
      "\tTraining Loss: 0.766846\n",
      "\tTesting Loss: 0.787551\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [644/10000000000000]\n",
      "\tTraining Loss: 0.767219\n",
      "\tTesting Loss: 0.790094\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [645/10000000000000]\n",
      "\tTraining Loss: 0.767248\n",
      "\tTesting Loss: 0.788092\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [646/10000000000000]\n",
      "\tTraining Loss: 0.767204\n",
      "\tTesting Loss: 0.788666\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [647/10000000000000]\n",
      "\tTraining Loss: 0.767552\n",
      "\tTesting Loss: 0.790361\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [648/10000000000000]\n",
      "\tTraining Loss: 0.767628\n",
      "\tTesting Loss: 0.787899\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [649/10000000000000]\n",
      "\tTraining Loss: 0.766739\n",
      "\tTesting Loss: 0.787708\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [650/10000000000000]\n",
      "\tTraining Loss: 0.767041\n",
      "\tTesting Loss: 0.789182\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [651/10000000000000]\n",
      "\tTraining Loss: 0.766890\n",
      "\tTesting Loss: 0.789192\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [652/10000000000000]\n",
      "\tTraining Loss: 0.766846\n",
      "\tTesting Loss: 0.788907\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [653/10000000000000]\n",
      "\tTraining Loss: 0.766588\n",
      "\tTesting Loss: 0.789474\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [654/10000000000000]\n",
      "\tTraining Loss: 0.766335\n",
      "\tTesting Loss: 0.788921\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [655/10000000000000]\n",
      "\tTraining Loss: 0.765934\n",
      "\tTesting Loss: 0.790490\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [656/10000000000000]\n",
      "\tTraining Loss: 0.766542\n",
      "\tTesting Loss: 0.788078\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [657/10000000000000]\n",
      "\tTraining Loss: 0.766068\n",
      "\tTesting Loss: 0.788991\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [658/10000000000000]\n",
      "\tTraining Loss: 0.766224\n",
      "\tTesting Loss: 0.789210\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [659/10000000000000]\n",
      "\tTraining Loss: 0.766847\n",
      "\tTesting Loss: 0.788921\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [660/10000000000000]\n",
      "\tTraining Loss: 0.766421\n",
      "\tTesting Loss: 0.788234\n",
      "\tLearning Rate: 0.000134218\n",
      "Epoch [661/10000000000000]\n",
      "\tTraining Loss: 0.766967\n",
      "\tTesting Loss: 0.791699\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [662/10000000000000]\n",
      "\tTraining Loss: 0.765769\n",
      "\tTesting Loss: 0.788693\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [663/10000000000000]\n",
      "\tTraining Loss: 0.765288\n",
      "\tTesting Loss: 0.788814\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [664/10000000000000]\n",
      "\tTraining Loss: 0.765015\n",
      "\tTesting Loss: 0.788372\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [665/10000000000000]\n",
      "\tTraining Loss: 0.765139\n",
      "\tTesting Loss: 0.787606\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [666/10000000000000]\n",
      "\tTraining Loss: 0.764729\n",
      "\tTesting Loss: 0.786697\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [667/10000000000000]\n",
      "\tTraining Loss: 0.765192\n",
      "\tTesting Loss: 0.788918\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [668/10000000000000]\n",
      "\tTraining Loss: 0.765166\n",
      "\tTesting Loss: 0.788518\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [669/10000000000000]\n",
      "\tTraining Loss: 0.764730\n",
      "\tTesting Loss: 0.787615\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [670/10000000000000]\n",
      "\tTraining Loss: 0.765069\n",
      "\tTesting Loss: 0.788751\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [671/10000000000000]\n",
      "\tTraining Loss: 0.764936\n",
      "\tTesting Loss: 0.787698\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [672/10000000000000]\n",
      "\tTraining Loss: 0.764532\n",
      "\tTesting Loss: 0.787559\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [673/10000000000000]\n",
      "\tTraining Loss: 0.764240\n",
      "\tTesting Loss: 0.789177\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [674/10000000000000]\n",
      "\tTraining Loss: 0.764545\n",
      "\tTesting Loss: 0.789233\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [675/10000000000000]\n",
      "\tTraining Loss: 0.764586\n",
      "\tTesting Loss: 0.787734\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [676/10000000000000]\n",
      "\tTraining Loss: 0.764258\n",
      "\tTesting Loss: 0.790918\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [677/10000000000000]\n",
      "\tTraining Loss: 0.764166\n",
      "\tTesting Loss: 0.788499\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [678/10000000000000]\n",
      "\tTraining Loss: 0.764330\n",
      "\tTesting Loss: 0.787335\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [679/10000000000000]\n",
      "\tTraining Loss: 0.764750\n",
      "\tTesting Loss: 0.788228\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [680/10000000000000]\n",
      "\tTraining Loss: 0.764270\n",
      "\tTesting Loss: 0.788521\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [681/10000000000000]\n",
      "\tTraining Loss: 0.764260\n",
      "\tTesting Loss: 0.786330\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [682/10000000000000]\n",
      "\tTraining Loss: 0.764190\n",
      "\tTesting Loss: 0.788284\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [683/10000000000000]\n",
      "\tTraining Loss: 0.764497\n",
      "\tTesting Loss: 0.787586\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [684/10000000000000]\n",
      "\tTraining Loss: 0.764631\n",
      "\tTesting Loss: 0.787923\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [685/10000000000000]\n",
      "\tTraining Loss: 0.764170\n",
      "\tTesting Loss: 0.787634\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [686/10000000000000]\n",
      "\tTraining Loss: 0.764326\n",
      "\tTesting Loss: 0.787775\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [687/10000000000000]\n",
      "\tTraining Loss: 0.764384\n",
      "\tTesting Loss: 0.789794\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [688/10000000000000]\n",
      "\tTraining Loss: 0.764143\n",
      "\tTesting Loss: 0.788850\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [689/10000000000000]\n",
      "\tTraining Loss: 0.763906\n",
      "\tTesting Loss: 0.789883\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [690/10000000000000]\n",
      "\tTraining Loss: 0.764020\n",
      "\tTesting Loss: 0.789984\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [691/10000000000000]\n",
      "\tTraining Loss: 0.763526\n",
      "\tTesting Loss: 0.787441\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [692/10000000000000]\n",
      "\tTraining Loss: 0.763515\n",
      "\tTesting Loss: 0.788174\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [693/10000000000000]\n",
      "\tTraining Loss: 0.763805\n",
      "\tTesting Loss: 0.787455\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [694/10000000000000]\n",
      "\tTraining Loss: 0.763732\n",
      "\tTesting Loss: 0.787912\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [695/10000000000000]\n",
      "\tTraining Loss: 0.763436\n",
      "\tTesting Loss: 0.788299\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [696/10000000000000]\n",
      "\tTraining Loss: 0.763954\n",
      "\tTesting Loss: 0.789443\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [697/10000000000000]\n",
      "\tTraining Loss: 0.763313\n",
      "\tTesting Loss: 0.787626\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [698/10000000000000]\n",
      "\tTraining Loss: 0.763259\n",
      "\tTesting Loss: 0.786427\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [699/10000000000000]\n",
      "\tTraining Loss: 0.763950\n",
      "\tTesting Loss: 0.789046\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [700/10000000000000]\n",
      "\tTraining Loss: 0.763734\n",
      "\tTesting Loss: 0.788111\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [701/10000000000000]\n",
      "\tTraining Loss: 0.763994\n",
      "\tTesting Loss: 0.789653\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [702/10000000000000]\n",
      "\tTraining Loss: 0.763749\n",
      "\tTesting Loss: 0.789418\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [703/10000000000000]\n",
      "\tTraining Loss: 0.763600\n",
      "\tTesting Loss: 0.787913\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [704/10000000000000]\n",
      "\tTraining Loss: 0.763540\n",
      "\tTesting Loss: 0.789158\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [705/10000000000000]\n",
      "\tTraining Loss: 0.764049\n",
      "\tTesting Loss: 0.788017\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [706/10000000000000]\n",
      "\tTraining Loss: 0.763073\n",
      "\tTesting Loss: 0.787769\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [707/10000000000000]\n",
      "\tTraining Loss: 0.763452\n",
      "\tTesting Loss: 0.789261\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [708/10000000000000]\n",
      "\tTraining Loss: 0.763695\n",
      "\tTesting Loss: 0.788829\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [709/10000000000000]\n",
      "\tTraining Loss: 0.762775\n",
      "\tTesting Loss: 0.787446\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [710/10000000000000]\n",
      "\tTraining Loss: 0.762856\n",
      "\tTesting Loss: 0.789377\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [711/10000000000000]\n",
      "\tTraining Loss: 0.763132\n",
      "\tTesting Loss: 0.788305\n",
      "\tLearning Rate: 0.000107374\n",
      "Epoch [712/10000000000000]\n",
      "\tTraining Loss: 0.762926\n",
      "\tTesting Loss: 0.788231\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [713/10000000000000]\n",
      "\tTraining Loss: 0.762198\n",
      "\tTesting Loss: 0.787577\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [714/10000000000000]\n",
      "\tTraining Loss: 0.762211\n",
      "\tTesting Loss: 0.788143\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [715/10000000000000]\n",
      "\tTraining Loss: 0.762093\n",
      "\tTesting Loss: 0.787622\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [716/10000000000000]\n",
      "\tTraining Loss: 0.761858\n",
      "\tTesting Loss: 0.787695\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [717/10000000000000]\n",
      "\tTraining Loss: 0.761743\n",
      "\tTesting Loss: 0.787193\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [718/10000000000000]\n",
      "\tTraining Loss: 0.762470\n",
      "\tTesting Loss: 0.787595\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [719/10000000000000]\n",
      "\tTraining Loss: 0.761806\n",
      "\tTesting Loss: 0.788400\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [720/10000000000000]\n",
      "\tTraining Loss: 0.761750\n",
      "\tTesting Loss: 0.788685\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [721/10000000000000]\n",
      "\tTraining Loss: 0.762357\n",
      "\tTesting Loss: 0.787674\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [722/10000000000000]\n",
      "\tTraining Loss: 0.761586\n",
      "\tTesting Loss: 0.787298\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [723/10000000000000]\n",
      "\tTraining Loss: 0.761676\n",
      "\tTesting Loss: 0.787503\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [724/10000000000000]\n",
      "\tTraining Loss: 0.761370\n",
      "\tTesting Loss: 0.787858\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [725/10000000000000]\n",
      "\tTraining Loss: 0.761217\n",
      "\tTesting Loss: 0.789572\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [726/10000000000000]\n",
      "\tTraining Loss: 0.761871\n",
      "\tTesting Loss: 0.788861\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [727/10000000000000]\n",
      "\tTraining Loss: 0.761552\n",
      "\tTesting Loss: 0.788212\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [728/10000000000000]\n",
      "\tTraining Loss: 0.761666\n",
      "\tTesting Loss: 0.788677\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [729/10000000000000]\n",
      "\tTraining Loss: 0.761086\n",
      "\tTesting Loss: 0.787082\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [730/10000000000000]\n",
      "\tTraining Loss: 0.761325\n",
      "\tTesting Loss: 0.788057\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [731/10000000000000]\n",
      "\tTraining Loss: 0.761554\n",
      "\tTesting Loss: 0.787733\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [732/10000000000000]\n",
      "\tTraining Loss: 0.761660\n",
      "\tTesting Loss: 0.788288\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [733/10000000000000]\n",
      "\tTraining Loss: 0.761248\n",
      "\tTesting Loss: 0.787635\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [734/10000000000000]\n",
      "\tTraining Loss: 0.761419\n",
      "\tTesting Loss: 0.788520\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [735/10000000000000]\n",
      "\tTraining Loss: 0.761587\n",
      "\tTesting Loss: 0.787643\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [736/10000000000000]\n",
      "\tTraining Loss: 0.761539\n",
      "\tTesting Loss: 0.787488\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [737/10000000000000]\n",
      "\tTraining Loss: 0.761420\n",
      "\tTesting Loss: 0.788014\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [738/10000000000000]\n",
      "\tTraining Loss: 0.761516\n",
      "\tTesting Loss: 0.789453\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [739/10000000000000]\n",
      "\tTraining Loss: 0.761582\n",
      "\tTesting Loss: 0.787552\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [740/10000000000000]\n",
      "\tTraining Loss: 0.761607\n",
      "\tTesting Loss: 0.787984\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [741/10000000000000]\n",
      "\tTraining Loss: 0.761196\n",
      "\tTesting Loss: 0.788005\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [742/10000000000000]\n",
      "\tTraining Loss: 0.761059\n",
      "\tTesting Loss: 0.788962\n",
      "\tLearning Rate: 0.000085899\n",
      "Epoch [743/10000000000000]\n",
      "\tTraining Loss: 0.761329\n",
      "\tTesting Loss: 0.788718\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [744/10000000000000]\n",
      "\tTraining Loss: 0.760464\n",
      "\tTesting Loss: 0.787743\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [745/10000000000000]\n",
      "\tTraining Loss: 0.760731\n",
      "\tTesting Loss: 0.788345\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [746/10000000000000]\n",
      "\tTraining Loss: 0.760522\n",
      "\tTesting Loss: 0.787656\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [747/10000000000000]\n",
      "\tTraining Loss: 0.760439\n",
      "\tTesting Loss: 0.789399\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [748/10000000000000]\n",
      "\tTraining Loss: 0.760938\n",
      "\tTesting Loss: 0.787952\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [749/10000000000000]\n",
      "\tTraining Loss: 0.761480\n",
      "\tTesting Loss: 0.788248\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [750/10000000000000]\n",
      "\tTraining Loss: 0.761524\n",
      "\tTesting Loss: 0.789072\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [751/10000000000000]\n",
      "\tTraining Loss: 0.761083\n",
      "\tTesting Loss: 0.789614\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [752/10000000000000]\n",
      "\tTraining Loss: 0.761200\n",
      "\tTesting Loss: 0.787774\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [753/10000000000000]\n",
      "\tTraining Loss: 0.760781\n",
      "\tTesting Loss: 0.787570\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [754/10000000000000]\n",
      "\tTraining Loss: 0.760862\n",
      "\tTesting Loss: 0.787263\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [755/10000000000000]\n",
      "\tTraining Loss: 0.760506\n",
      "\tTesting Loss: 0.788082\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [756/10000000000000]\n",
      "\tTraining Loss: 0.760584\n",
      "\tTesting Loss: 0.788975\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [757/10000000000000]\n",
      "\tTraining Loss: 0.760802\n",
      "\tTesting Loss: 0.788038\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [758/10000000000000]\n",
      "\tTraining Loss: 0.760681\n",
      "\tTesting Loss: 0.788700\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [759/10000000000000]\n",
      "\tTraining Loss: 0.760621\n",
      "\tTesting Loss: 0.787528\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [760/10000000000000]\n",
      "\tTraining Loss: 0.760549\n",
      "\tTesting Loss: 0.787986\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [761/10000000000000]\n",
      "\tTraining Loss: 0.760570\n",
      "\tTesting Loss: 0.787422\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [762/10000000000000]\n",
      "\tTraining Loss: 0.760327\n",
      "\tTesting Loss: 0.788007\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [763/10000000000000]\n",
      "\tTraining Loss: 0.760305\n",
      "\tTesting Loss: 0.788030\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [764/10000000000000]\n",
      "\tTraining Loss: 0.760340\n",
      "\tTesting Loss: 0.787865\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [765/10000000000000]\n",
      "\tTraining Loss: 0.760169\n",
      "\tTesting Loss: 0.789616\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [766/10000000000000]\n",
      "\tTraining Loss: 0.759967\n",
      "\tTesting Loss: 0.788605\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [767/10000000000000]\n",
      "\tTraining Loss: 0.760078\n",
      "\tTesting Loss: 0.787817\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [768/10000000000000]\n",
      "\tTraining Loss: 0.759944\n",
      "\tTesting Loss: 0.788313\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [769/10000000000000]\n",
      "\tTraining Loss: 0.760210\n",
      "\tTesting Loss: 0.789129\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [770/10000000000000]\n",
      "\tTraining Loss: 0.760339\n",
      "\tTesting Loss: 0.791417\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [771/10000000000000]\n",
      "\tTraining Loss: 0.760580\n",
      "\tTesting Loss: 0.789311\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [772/10000000000000]\n",
      "\tTraining Loss: 0.760366\n",
      "\tTesting Loss: 0.787981\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [773/10000000000000]\n",
      "\tTraining Loss: 0.760269\n",
      "\tTesting Loss: 0.787853\n",
      "\tLearning Rate: 0.000068719\n",
      "Epoch [774/10000000000000]\n",
      "\tTraining Loss: 0.760130\n",
      "\tTesting Loss: 0.788851\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [775/10000000000000]\n",
      "\tTraining Loss: 0.759545\n",
      "\tTesting Loss: 0.787846\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [776/10000000000000]\n",
      "\tTraining Loss: 0.759600\n",
      "\tTesting Loss: 0.788733\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [777/10000000000000]\n",
      "\tTraining Loss: 0.759497\n",
      "\tTesting Loss: 0.787262\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [778/10000000000000]\n",
      "\tTraining Loss: 0.759506\n",
      "\tTesting Loss: 0.788409\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [779/10000000000000]\n",
      "\tTraining Loss: 0.759584\n",
      "\tTesting Loss: 0.787319\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [780/10000000000000]\n",
      "\tTraining Loss: 0.759455\n",
      "\tTesting Loss: 0.788362\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [781/10000000000000]\n",
      "\tTraining Loss: 0.759450\n",
      "\tTesting Loss: 0.788008\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [782/10000000000000]\n",
      "\tTraining Loss: 0.759344\n",
      "\tTesting Loss: 0.788487\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [783/10000000000000]\n",
      "\tTraining Loss: 0.759251\n",
      "\tTesting Loss: 0.788485\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [784/10000000000000]\n",
      "\tTraining Loss: 0.759352\n",
      "\tTesting Loss: 0.787542\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [785/10000000000000]\n",
      "\tTraining Loss: 0.759480\n",
      "\tTesting Loss: 0.787217\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [786/10000000000000]\n",
      "\tTraining Loss: 0.759240\n",
      "\tTesting Loss: 0.787911\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [787/10000000000000]\n",
      "\tTraining Loss: 0.759363\n",
      "\tTesting Loss: 0.788830\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [788/10000000000000]\n",
      "\tTraining Loss: 0.759012\n",
      "\tTesting Loss: 0.788781\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [789/10000000000000]\n",
      "\tTraining Loss: 0.759345\n",
      "\tTesting Loss: 0.788520\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [790/10000000000000]\n",
      "\tTraining Loss: 0.759359\n",
      "\tTesting Loss: 0.788029\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [791/10000000000000]\n",
      "\tTraining Loss: 0.759110\n",
      "\tTesting Loss: 0.788351\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [792/10000000000000]\n",
      "\tTraining Loss: 0.759143\n",
      "\tTesting Loss: 0.788666\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [793/10000000000000]\n",
      "\tTraining Loss: 0.759223\n",
      "\tTesting Loss: 0.789034\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [794/10000000000000]\n",
      "\tTraining Loss: 0.759358\n",
      "\tTesting Loss: 0.787968\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [795/10000000000000]\n",
      "\tTraining Loss: 0.759252\n",
      "\tTesting Loss: 0.788007\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [796/10000000000000]\n",
      "\tTraining Loss: 0.759228\n",
      "\tTesting Loss: 0.788563\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [797/10000000000000]\n",
      "\tTraining Loss: 0.759603\n",
      "\tTesting Loss: 0.789061\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [798/10000000000000]\n",
      "\tTraining Loss: 0.759134\n",
      "\tTesting Loss: 0.788757\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [799/10000000000000]\n",
      "\tTraining Loss: 0.759577\n",
      "\tTesting Loss: 0.788263\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [800/10000000000000]\n",
      "\tTraining Loss: 0.759346\n",
      "\tTesting Loss: 0.789438\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [801/10000000000000]\n",
      "\tTraining Loss: 0.759183\n",
      "\tTesting Loss: 0.788493\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [802/10000000000000]\n",
      "\tTraining Loss: 0.759045\n",
      "\tTesting Loss: 0.790104\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [803/10000000000000]\n",
      "\tTraining Loss: 0.759077\n",
      "\tTesting Loss: 0.789038\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [804/10000000000000]\n",
      "\tTraining Loss: 0.758999\n",
      "\tTesting Loss: 0.787334\n",
      "\tLearning Rate: 0.000054976\n",
      "Epoch [805/10000000000000]\n",
      "\tTraining Loss: 0.758843\n",
      "\tTesting Loss: 0.788995\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [806/10000000000000]\n",
      "\tTraining Loss: 0.758461\n",
      "\tTesting Loss: 0.789623\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [807/10000000000000]\n",
      "\tTraining Loss: 0.758625\n",
      "\tTesting Loss: 0.788555\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [808/10000000000000]\n",
      "\tTraining Loss: 0.758876\n",
      "\tTesting Loss: 0.788745\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [809/10000000000000]\n",
      "\tTraining Loss: 0.758644\n",
      "\tTesting Loss: 0.788971\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [810/10000000000000]\n",
      "\tTraining Loss: 0.758556\n",
      "\tTesting Loss: 0.787850\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [811/10000000000000]\n",
      "\tTraining Loss: 0.758854\n",
      "\tTesting Loss: 0.788327\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [812/10000000000000]\n",
      "\tTraining Loss: 0.758706\n",
      "\tTesting Loss: 0.788552\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [813/10000000000000]\n",
      "\tTraining Loss: 0.758326\n",
      "\tTesting Loss: 0.789000\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [814/10000000000000]\n",
      "\tTraining Loss: 0.758529\n",
      "\tTesting Loss: 0.789005\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [815/10000000000000]\n",
      "\tTraining Loss: 0.758542\n",
      "\tTesting Loss: 0.786883\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [816/10000000000000]\n",
      "\tTraining Loss: 0.758420\n",
      "\tTesting Loss: 0.789541\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [817/10000000000000]\n",
      "\tTraining Loss: 0.758429\n",
      "\tTesting Loss: 0.787983\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [818/10000000000000]\n",
      "\tTraining Loss: 0.758234\n",
      "\tTesting Loss: 0.790457\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [819/10000000000000]\n",
      "\tTraining Loss: 0.758183\n",
      "\tTesting Loss: 0.790284\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [820/10000000000000]\n",
      "\tTraining Loss: 0.758203\n",
      "\tTesting Loss: 0.788468\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [821/10000000000000]\n",
      "\tTraining Loss: 0.758258\n",
      "\tTesting Loss: 0.788974\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [822/10000000000000]\n",
      "\tTraining Loss: 0.758107\n",
      "\tTesting Loss: 0.789597\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [823/10000000000000]\n",
      "\tTraining Loss: 0.758227\n",
      "\tTesting Loss: 0.788667\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [824/10000000000000]\n",
      "\tTraining Loss: 0.758149\n",
      "\tTesting Loss: 0.788430\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [825/10000000000000]\n",
      "\tTraining Loss: 0.758107\n",
      "\tTesting Loss: 0.788238\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [826/10000000000000]\n",
      "\tTraining Loss: 0.758010\n",
      "\tTesting Loss: 0.788869\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [827/10000000000000]\n",
      "\tTraining Loss: 0.758140\n",
      "\tTesting Loss: 0.788852\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [828/10000000000000]\n",
      "\tTraining Loss: 0.758067\n",
      "\tTesting Loss: 0.788204\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [829/10000000000000]\n",
      "\tTraining Loss: 0.758091\n",
      "\tTesting Loss: 0.788421\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [830/10000000000000]\n",
      "\tTraining Loss: 0.758072\n",
      "\tTesting Loss: 0.788498\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [831/10000000000000]\n",
      "\tTraining Loss: 0.758120\n",
      "\tTesting Loss: 0.789769\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [832/10000000000000]\n",
      "\tTraining Loss: 0.758075\n",
      "\tTesting Loss: 0.789310\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [833/10000000000000]\n",
      "\tTraining Loss: 0.757833\n",
      "\tTesting Loss: 0.788165\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [834/10000000000000]\n",
      "\tTraining Loss: 0.757999\n",
      "\tTesting Loss: 0.790129\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [835/10000000000000]\n",
      "\tTraining Loss: 0.757975\n",
      "\tTesting Loss: 0.788539\n",
      "\tLearning Rate: 0.000043980\n",
      "Epoch [836/10000000000000]\n",
      "\tTraining Loss: 0.757801\n",
      "\tTesting Loss: 0.788242\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [837/10000000000000]\n",
      "\tTraining Loss: 0.757576\n",
      "\tTesting Loss: 0.788745\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [838/10000000000000]\n",
      "\tTraining Loss: 0.757446\n",
      "\tTesting Loss: 0.788507\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [839/10000000000000]\n",
      "\tTraining Loss: 0.757467\n",
      "\tTesting Loss: 0.789175\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [840/10000000000000]\n",
      "\tTraining Loss: 0.757491\n",
      "\tTesting Loss: 0.788989\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [841/10000000000000]\n",
      "\tTraining Loss: 0.757549\n",
      "\tTesting Loss: 0.788733\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [842/10000000000000]\n",
      "\tTraining Loss: 0.757537\n",
      "\tTesting Loss: 0.789057\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [843/10000000000000]\n",
      "\tTraining Loss: 0.757460\n",
      "\tTesting Loss: 0.788343\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [844/10000000000000]\n",
      "\tTraining Loss: 0.757550\n",
      "\tTesting Loss: 0.788481\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [845/10000000000000]\n",
      "\tTraining Loss: 0.757470\n",
      "\tTesting Loss: 0.788729\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [846/10000000000000]\n",
      "\tTraining Loss: 0.757399\n",
      "\tTesting Loss: 0.788652\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [847/10000000000000]\n",
      "\tTraining Loss: 0.757336\n",
      "\tTesting Loss: 0.788371\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [848/10000000000000]\n",
      "\tTraining Loss: 0.757452\n",
      "\tTesting Loss: 0.789260\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [849/10000000000000]\n",
      "\tTraining Loss: 0.757371\n",
      "\tTesting Loss: 0.789339\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [850/10000000000000]\n",
      "\tTraining Loss: 0.757263\n",
      "\tTesting Loss: 0.788658\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [851/10000000000000]\n",
      "\tTraining Loss: 0.757331\n",
      "\tTesting Loss: 0.787877\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [852/10000000000000]\n",
      "\tTraining Loss: 0.757196\n",
      "\tTesting Loss: 0.788764\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [853/10000000000000]\n",
      "\tTraining Loss: 0.757253\n",
      "\tTesting Loss: 0.789752\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [854/10000000000000]\n",
      "\tTraining Loss: 0.757235\n",
      "\tTesting Loss: 0.789231\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [855/10000000000000]\n",
      "\tTraining Loss: 0.757513\n",
      "\tTesting Loss: 0.789893\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [856/10000000000000]\n",
      "\tTraining Loss: 0.757499\n",
      "\tTesting Loss: 0.789349\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [857/10000000000000]\n",
      "\tTraining Loss: 0.757493\n",
      "\tTesting Loss: 0.789200\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [858/10000000000000]\n",
      "\tTraining Loss: 0.757397\n",
      "\tTesting Loss: 0.789445\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [859/10000000000000]\n",
      "\tTraining Loss: 0.757393\n",
      "\tTesting Loss: 0.787854\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [860/10000000000000]\n",
      "\tTraining Loss: 0.757443\n",
      "\tTesting Loss: 0.788105\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [861/10000000000000]\n",
      "\tTraining Loss: 0.757241\n",
      "\tTesting Loss: 0.788492\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [862/10000000000000]\n",
      "\tTraining Loss: 0.757105\n",
      "\tTesting Loss: 0.788396\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [863/10000000000000]\n",
      "\tTraining Loss: 0.757127\n",
      "\tTesting Loss: 0.788795\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [864/10000000000000]\n",
      "\tTraining Loss: 0.757077\n",
      "\tTesting Loss: 0.789054\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [865/10000000000000]\n",
      "\tTraining Loss: 0.757302\n",
      "\tTesting Loss: 0.788948\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [866/10000000000000]\n",
      "\tTraining Loss: 0.757119\n",
      "\tTesting Loss: 0.789371\n",
      "\tLearning Rate: 0.000035184\n",
      "Epoch [867/10000000000000]\n",
      "\tTraining Loss: 0.757110\n",
      "\tTesting Loss: 0.789073\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [868/10000000000000]\n",
      "\tTraining Loss: 0.756910\n",
      "\tTesting Loss: 0.789792\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [869/10000000000000]\n",
      "\tTraining Loss: 0.756756\n",
      "\tTesting Loss: 0.790206\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [870/10000000000000]\n",
      "\tTraining Loss: 0.756753\n",
      "\tTesting Loss: 0.789216\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [871/10000000000000]\n",
      "\tTraining Loss: 0.756872\n",
      "\tTesting Loss: 0.788571\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [872/10000000000000]\n",
      "\tTraining Loss: 0.756774\n",
      "\tTesting Loss: 0.789999\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [873/10000000000000]\n",
      "\tTraining Loss: 0.756791\n",
      "\tTesting Loss: 0.788542\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [874/10000000000000]\n",
      "\tTraining Loss: 0.756720\n",
      "\tTesting Loss: 0.789522\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [875/10000000000000]\n",
      "\tTraining Loss: 0.756678\n",
      "\tTesting Loss: 0.788235\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [876/10000000000000]\n",
      "\tTraining Loss: 0.756714\n",
      "\tTesting Loss: 0.788012\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [877/10000000000000]\n",
      "\tTraining Loss: 0.756718\n",
      "\tTesting Loss: 0.789787\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [878/10000000000000]\n",
      "\tTraining Loss: 0.756728\n",
      "\tTesting Loss: 0.788853\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [879/10000000000000]\n",
      "\tTraining Loss: 0.756774\n",
      "\tTesting Loss: 0.789810\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [880/10000000000000]\n",
      "\tTraining Loss: 0.756601\n",
      "\tTesting Loss: 0.789724\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [881/10000000000000]\n",
      "\tTraining Loss: 0.756660\n",
      "\tTesting Loss: 0.788543\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [882/10000000000000]\n",
      "\tTraining Loss: 0.756668\n",
      "\tTesting Loss: 0.789574\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [883/10000000000000]\n",
      "\tTraining Loss: 0.756624\n",
      "\tTesting Loss: 0.789846\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [884/10000000000000]\n",
      "\tTraining Loss: 0.756590\n",
      "\tTesting Loss: 0.788911\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [885/10000000000000]\n",
      "\tTraining Loss: 0.756518\n",
      "\tTesting Loss: 0.789663\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [886/10000000000000]\n",
      "\tTraining Loss: 0.756610\n",
      "\tTesting Loss: 0.789531\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [887/10000000000000]\n",
      "\tTraining Loss: 0.756560\n",
      "\tTesting Loss: 0.788611\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [888/10000000000000]\n",
      "\tTraining Loss: 0.756514\n",
      "\tTesting Loss: 0.790175\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [889/10000000000000]\n",
      "\tTraining Loss: 0.756514\n",
      "\tTesting Loss: 0.788583\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [890/10000000000000]\n",
      "\tTraining Loss: 0.756419\n",
      "\tTesting Loss: 0.789615\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [891/10000000000000]\n",
      "\tTraining Loss: 0.756612\n",
      "\tTesting Loss: 0.790653\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [892/10000000000000]\n",
      "\tTraining Loss: 0.756535\n",
      "\tTesting Loss: 0.789764\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [893/10000000000000]\n",
      "\tTraining Loss: 0.756571\n",
      "\tTesting Loss: 0.789185\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [894/10000000000000]\n",
      "\tTraining Loss: 0.756467\n",
      "\tTesting Loss: 0.789067\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [895/10000000000000]\n",
      "\tTraining Loss: 0.756413\n",
      "\tTesting Loss: 0.789894\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [896/10000000000000]\n",
      "\tTraining Loss: 0.756368\n",
      "\tTesting Loss: 0.789880\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [897/10000000000000]\n",
      "\tTraining Loss: 0.756431\n",
      "\tTesting Loss: 0.789550\n",
      "\tLearning Rate: 0.000028147\n",
      "Epoch [898/10000000000000]\n",
      "\tTraining Loss: 0.756405\n",
      "\tTesting Loss: 0.790081\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [899/10000000000000]\n",
      "\tTraining Loss: 0.756211\n",
      "\tTesting Loss: 0.789305\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [900/10000000000000]\n",
      "\tTraining Loss: 0.756225\n",
      "\tTesting Loss: 0.789738\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [901/10000000000000]\n",
      "\tTraining Loss: 0.756192\n",
      "\tTesting Loss: 0.789502\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [902/10000000000000]\n",
      "\tTraining Loss: 0.756185\n",
      "\tTesting Loss: 0.790603\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [903/10000000000000]\n",
      "\tTraining Loss: 0.756180\n",
      "\tTesting Loss: 0.789681\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [904/10000000000000]\n",
      "\tTraining Loss: 0.756233\n",
      "\tTesting Loss: 0.790377\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [905/10000000000000]\n",
      "\tTraining Loss: 0.756284\n",
      "\tTesting Loss: 0.789509\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [906/10000000000000]\n",
      "\tTraining Loss: 0.756073\n",
      "\tTesting Loss: 0.789525\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [907/10000000000000]\n",
      "\tTraining Loss: 0.756194\n",
      "\tTesting Loss: 0.790352\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [908/10000000000000]\n",
      "\tTraining Loss: 0.756115\n",
      "\tTesting Loss: 0.790057\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [909/10000000000000]\n",
      "\tTraining Loss: 0.756119\n",
      "\tTesting Loss: 0.790264\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [910/10000000000000]\n",
      "\tTraining Loss: 0.756162\n",
      "\tTesting Loss: 0.789248\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [911/10000000000000]\n",
      "\tTraining Loss: 0.756080\n",
      "\tTesting Loss: 0.789679\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [912/10000000000000]\n",
      "\tTraining Loss: 0.756096\n",
      "\tTesting Loss: 0.789947\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [913/10000000000000]\n",
      "\tTraining Loss: 0.756104\n",
      "\tTesting Loss: 0.789543\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [914/10000000000000]\n",
      "\tTraining Loss: 0.756015\n",
      "\tTesting Loss: 0.789349\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [915/10000000000000]\n",
      "\tTraining Loss: 0.756054\n",
      "\tTesting Loss: 0.789641\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [916/10000000000000]\n",
      "\tTraining Loss: 0.756040\n",
      "\tTesting Loss: 0.789925\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [917/10000000000000]\n",
      "\tTraining Loss: 0.756046\n",
      "\tTesting Loss: 0.789287\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [918/10000000000000]\n",
      "\tTraining Loss: 0.756067\n",
      "\tTesting Loss: 0.789716\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [919/10000000000000]\n",
      "\tTraining Loss: 0.756057\n",
      "\tTesting Loss: 0.789995\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [920/10000000000000]\n",
      "\tTraining Loss: 0.755974\n",
      "\tTesting Loss: 0.788880\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [921/10000000000000]\n",
      "\tTraining Loss: 0.756009\n",
      "\tTesting Loss: 0.789902\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [922/10000000000000]\n",
      "\tTraining Loss: 0.755952\n",
      "\tTesting Loss: 0.788619\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [923/10000000000000]\n",
      "\tTraining Loss: 0.755920\n",
      "\tTesting Loss: 0.789981\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [924/10000000000000]\n",
      "\tTraining Loss: 0.756035\n",
      "\tTesting Loss: 0.789608\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [925/10000000000000]\n",
      "\tTraining Loss: 0.756042\n",
      "\tTesting Loss: 0.789594\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [926/10000000000000]\n",
      "\tTraining Loss: 0.755990\n",
      "\tTesting Loss: 0.790758\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [927/10000000000000]\n",
      "\tTraining Loss: 0.755993\n",
      "\tTesting Loss: 0.789951\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [928/10000000000000]\n",
      "\tTraining Loss: 0.756027\n",
      "\tTesting Loss: 0.789471\n",
      "\tLearning Rate: 0.000022518\n",
      "Epoch [929/10000000000000]\n",
      "\tTraining Loss: 0.755896\n",
      "\tTesting Loss: 0.790289\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [930/10000000000000]\n",
      "\tTraining Loss: 0.755866\n",
      "\tTesting Loss: 0.788726\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [931/10000000000000]\n",
      "\tTraining Loss: 0.755741\n",
      "\tTesting Loss: 0.789302\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [932/10000000000000]\n",
      "\tTraining Loss: 0.755788\n",
      "\tTesting Loss: 0.790042\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [933/10000000000000]\n",
      "\tTraining Loss: 0.755721\n",
      "\tTesting Loss: 0.790396\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [934/10000000000000]\n",
      "\tTraining Loss: 0.755741\n",
      "\tTesting Loss: 0.791035\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [935/10000000000000]\n",
      "\tTraining Loss: 0.755674\n",
      "\tTesting Loss: 0.789902\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [936/10000000000000]\n",
      "\tTraining Loss: 0.755761\n",
      "\tTesting Loss: 0.789738\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [937/10000000000000]\n",
      "\tTraining Loss: 0.755682\n",
      "\tTesting Loss: 0.789325\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [938/10000000000000]\n",
      "\tTraining Loss: 0.755617\n",
      "\tTesting Loss: 0.789091\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [939/10000000000000]\n",
      "\tTraining Loss: 0.755746\n",
      "\tTesting Loss: 0.790557\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [940/10000000000000]\n",
      "\tTraining Loss: 0.755761\n",
      "\tTesting Loss: 0.789990\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [941/10000000000000]\n",
      "\tTraining Loss: 0.755581\n",
      "\tTesting Loss: 0.789756\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [942/10000000000000]\n",
      "\tTraining Loss: 0.755623\n",
      "\tTesting Loss: 0.789336\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [943/10000000000000]\n",
      "\tTraining Loss: 0.755690\n",
      "\tTesting Loss: 0.789485\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [944/10000000000000]\n",
      "\tTraining Loss: 0.755648\n",
      "\tTesting Loss: 0.789987\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [945/10000000000000]\n",
      "\tTraining Loss: 0.755694\n",
      "\tTesting Loss: 0.789798\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [946/10000000000000]\n",
      "\tTraining Loss: 0.755698\n",
      "\tTesting Loss: 0.789697\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [947/10000000000000]\n",
      "\tTraining Loss: 0.755525\n",
      "\tTesting Loss: 0.789985\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [948/10000000000000]\n",
      "\tTraining Loss: 0.755561\n",
      "\tTesting Loss: 0.789717\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [949/10000000000000]\n",
      "\tTraining Loss: 0.755630\n",
      "\tTesting Loss: 0.789874\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [950/10000000000000]\n",
      "\tTraining Loss: 0.755549\n",
      "\tTesting Loss: 0.790779\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [951/10000000000000]\n",
      "\tTraining Loss: 0.755573\n",
      "\tTesting Loss: 0.790375\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [952/10000000000000]\n",
      "\tTraining Loss: 0.755630\n",
      "\tTesting Loss: 0.789932\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [953/10000000000000]\n",
      "\tTraining Loss: 0.755605\n",
      "\tTesting Loss: 0.789864\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [954/10000000000000]\n",
      "\tTraining Loss: 0.755502\n",
      "\tTesting Loss: 0.790840\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [955/10000000000000]\n",
      "\tTraining Loss: 0.755546\n",
      "\tTesting Loss: 0.790454\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [956/10000000000000]\n",
      "\tTraining Loss: 0.755496\n",
      "\tTesting Loss: 0.790195\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [957/10000000000000]\n",
      "\tTraining Loss: 0.755571\n",
      "\tTesting Loss: 0.790117\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [958/10000000000000]\n",
      "\tTraining Loss: 0.755598\n",
      "\tTesting Loss: 0.790098\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [959/10000000000000]\n",
      "\tTraining Loss: 0.755547\n",
      "\tTesting Loss: 0.790417\n",
      "\tLearning Rate: 0.000018014\n",
      "Epoch [960/10000000000000]\n",
      "\tTraining Loss: 0.755511\n",
      "\tTesting Loss: 0.789392\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [961/10000000000000]\n",
      "\tTraining Loss: 0.755460\n",
      "\tTesting Loss: 0.790277\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [962/10000000000000]\n",
      "\tTraining Loss: 0.755395\n",
      "\tTesting Loss: 0.789668\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [963/10000000000000]\n",
      "\tTraining Loss: 0.755394\n",
      "\tTesting Loss: 0.789842\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [964/10000000000000]\n",
      "\tTraining Loss: 0.755397\n",
      "\tTesting Loss: 0.789328\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [965/10000000000000]\n",
      "\tTraining Loss: 0.755345\n",
      "\tTesting Loss: 0.789167\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [966/10000000000000]\n",
      "\tTraining Loss: 0.755302\n",
      "\tTesting Loss: 0.790097\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [967/10000000000000]\n",
      "\tTraining Loss: 0.755361\n",
      "\tTesting Loss: 0.790690\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [968/10000000000000]\n",
      "\tTraining Loss: 0.755352\n",
      "\tTesting Loss: 0.789766\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [969/10000000000000]\n",
      "\tTraining Loss: 0.755353\n",
      "\tTesting Loss: 0.790014\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [970/10000000000000]\n",
      "\tTraining Loss: 0.755333\n",
      "\tTesting Loss: 0.790373\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [971/10000000000000]\n",
      "\tTraining Loss: 0.755314\n",
      "\tTesting Loss: 0.790060\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [972/10000000000000]\n",
      "\tTraining Loss: 0.755325\n",
      "\tTesting Loss: 0.789060\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [973/10000000000000]\n",
      "\tTraining Loss: 0.755332\n",
      "\tTesting Loss: 0.790611\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [974/10000000000000]\n",
      "\tTraining Loss: 0.755282\n",
      "\tTesting Loss: 0.790810\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [975/10000000000000]\n",
      "\tTraining Loss: 0.755288\n",
      "\tTesting Loss: 0.790451\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [976/10000000000000]\n",
      "\tTraining Loss: 0.755252\n",
      "\tTesting Loss: 0.789267\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [977/10000000000000]\n",
      "\tTraining Loss: 0.755336\n",
      "\tTesting Loss: 0.790680\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [978/10000000000000]\n",
      "\tTraining Loss: 0.755284\n",
      "\tTesting Loss: 0.790472\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [979/10000000000000]\n",
      "\tTraining Loss: 0.755274\n",
      "\tTesting Loss: 0.789724\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [980/10000000000000]\n",
      "\tTraining Loss: 0.755227\n",
      "\tTesting Loss: 0.790684\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [981/10000000000000]\n",
      "\tTraining Loss: 0.755294\n",
      "\tTesting Loss: 0.790488\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [982/10000000000000]\n",
      "\tTraining Loss: 0.755318\n",
      "\tTesting Loss: 0.789417\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [983/10000000000000]\n",
      "\tTraining Loss: 0.755287\n",
      "\tTesting Loss: 0.790397\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [984/10000000000000]\n",
      "\tTraining Loss: 0.755293\n",
      "\tTesting Loss: 0.789639\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [985/10000000000000]\n",
      "\tTraining Loss: 0.755224\n",
      "\tTesting Loss: 0.790970\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [986/10000000000000]\n",
      "\tTraining Loss: 0.755208\n",
      "\tTesting Loss: 0.789974\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [987/10000000000000]\n",
      "\tTraining Loss: 0.755206\n",
      "\tTesting Loss: 0.789250\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [988/10000000000000]\n",
      "\tTraining Loss: 0.755174\n",
      "\tTesting Loss: 0.789694\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [989/10000000000000]\n",
      "\tTraining Loss: 0.755213\n",
      "\tTesting Loss: 0.790224\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [990/10000000000000]\n",
      "\tTraining Loss: 0.755136\n",
      "\tTesting Loss: 0.791043\n",
      "\tLearning Rate: 0.000014412\n",
      "Epoch [991/10000000000000]\n",
      "\tTraining Loss: 0.755200\n",
      "\tTesting Loss: 0.790845\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [992/10000000000000]\n",
      "\tTraining Loss: 0.755113\n",
      "\tTesting Loss: 0.789966\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [993/10000000000000]\n",
      "\tTraining Loss: 0.755065\n",
      "\tTesting Loss: 0.790668\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [994/10000000000000]\n",
      "\tTraining Loss: 0.755053\n",
      "\tTesting Loss: 0.790269\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [995/10000000000000]\n",
      "\tTraining Loss: 0.755087\n",
      "\tTesting Loss: 0.790485\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [996/10000000000000]\n",
      "\tTraining Loss: 0.755019\n",
      "\tTesting Loss: 0.789625\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [997/10000000000000]\n",
      "\tTraining Loss: 0.755058\n",
      "\tTesting Loss: 0.789053\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [998/10000000000000]\n",
      "\tTraining Loss: 0.755114\n",
      "\tTesting Loss: 0.791149\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [999/10000000000000]\n",
      "\tTraining Loss: 0.754999\n",
      "\tTesting Loss: 0.789617\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [1000/10000000000000]\n",
      "\tTraining Loss: 0.755005\n",
      "\tTesting Loss: 0.790110\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [1001/10000000000000]\n",
      "\tTraining Loss: 0.755085\n",
      "\tTesting Loss: 0.789974\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [1002/10000000000000]\n",
      "\tTraining Loss: 0.755092\n",
      "\tTesting Loss: 0.790721\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [1003/10000000000000]\n",
      "\tTraining Loss: 0.754985\n",
      "\tTesting Loss: 0.790242\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [1004/10000000000000]\n",
      "\tTraining Loss: 0.754975\n",
      "\tTesting Loss: 0.790903\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [1005/10000000000000]\n",
      "\tTraining Loss: 0.755009\n",
      "\tTesting Loss: 0.790554\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [1006/10000000000000]\n",
      "\tTraining Loss: 0.755020\n",
      "\tTesting Loss: 0.790050\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [1007/10000000000000]\n",
      "\tTraining Loss: 0.755014\n",
      "\tTesting Loss: 0.790135\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [1008/10000000000000]\n",
      "\tTraining Loss: 0.755035\n",
      "\tTesting Loss: 0.791757\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [1009/10000000000000]\n",
      "\tTraining Loss: 0.754955\n",
      "\tTesting Loss: 0.790576\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [1010/10000000000000]\n",
      "\tTraining Loss: 0.755030\n",
      "\tTesting Loss: 0.790055\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [1011/10000000000000]\n",
      "\tTraining Loss: 0.754991\n",
      "\tTesting Loss: 0.790714\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [1012/10000000000000]\n",
      "\tTraining Loss: 0.754993\n",
      "\tTesting Loss: 0.790394\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [1013/10000000000000]\n",
      "\tTraining Loss: 0.755019\n",
      "\tTesting Loss: 0.789600\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [1014/10000000000000]\n",
      "\tTraining Loss: 0.754972\n",
      "\tTesting Loss: 0.791402\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [1015/10000000000000]\n",
      "\tTraining Loss: 0.754981\n",
      "\tTesting Loss: 0.791006\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [1016/10000000000000]\n",
      "\tTraining Loss: 0.754924\n",
      "\tTesting Loss: 0.790313\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [1017/10000000000000]\n",
      "\tTraining Loss: 0.754899\n",
      "\tTesting Loss: 0.790079\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [1018/10000000000000]\n",
      "\tTraining Loss: 0.754966\n",
      "\tTesting Loss: 0.789912\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [1019/10000000000000]\n",
      "\tTraining Loss: 0.754900\n",
      "\tTesting Loss: 0.790489\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [1020/10000000000000]\n",
      "\tTraining Loss: 0.754905\n",
      "\tTesting Loss: 0.790660\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [1021/10000000000000]\n",
      "\tTraining Loss: 0.754919\n",
      "\tTesting Loss: 0.790618\n",
      "\tLearning Rate: 0.000011529\n",
      "Epoch [1022/10000000000000]\n",
      "\tTraining Loss: 0.754883\n",
      "\tTesting Loss: 0.790986\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1023/10000000000000]\n",
      "\tTraining Loss: 0.754812\n",
      "\tTesting Loss: 0.790501\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1024/10000000000000]\n",
      "\tTraining Loss: 0.754862\n",
      "\tTesting Loss: 0.789124\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1025/10000000000000]\n",
      "\tTraining Loss: 0.754816\n",
      "\tTesting Loss: 0.789910\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1026/10000000000000]\n",
      "\tTraining Loss: 0.754796\n",
      "\tTesting Loss: 0.790262\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1027/10000000000000]\n",
      "\tTraining Loss: 0.754826\n",
      "\tTesting Loss: 0.790225\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1028/10000000000000]\n",
      "\tTraining Loss: 0.754795\n",
      "\tTesting Loss: 0.789470\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1029/10000000000000]\n",
      "\tTraining Loss: 0.754847\n",
      "\tTesting Loss: 0.790465\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1030/10000000000000]\n",
      "\tTraining Loss: 0.754787\n",
      "\tTesting Loss: 0.790820\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1031/10000000000000]\n",
      "\tTraining Loss: 0.754763\n",
      "\tTesting Loss: 0.790653\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1032/10000000000000]\n",
      "\tTraining Loss: 0.754807\n",
      "\tTesting Loss: 0.789485\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1033/10000000000000]\n",
      "\tTraining Loss: 0.754756\n",
      "\tTesting Loss: 0.789804\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1034/10000000000000]\n",
      "\tTraining Loss: 0.754825\n",
      "\tTesting Loss: 0.790416\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1035/10000000000000]\n",
      "\tTraining Loss: 0.754840\n",
      "\tTesting Loss: 0.790160\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1036/10000000000000]\n",
      "\tTraining Loss: 0.754813\n",
      "\tTesting Loss: 0.789674\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1037/10000000000000]\n",
      "\tTraining Loss: 0.754719\n",
      "\tTesting Loss: 0.790358\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1038/10000000000000]\n",
      "\tTraining Loss: 0.754782\n",
      "\tTesting Loss: 0.789064\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1039/10000000000000]\n",
      "\tTraining Loss: 0.754763\n",
      "\tTesting Loss: 0.790896\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1040/10000000000000]\n",
      "\tTraining Loss: 0.754732\n",
      "\tTesting Loss: 0.790146\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1041/10000000000000]\n",
      "\tTraining Loss: 0.754717\n",
      "\tTesting Loss: 0.789565\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1042/10000000000000]\n",
      "\tTraining Loss: 0.754743\n",
      "\tTesting Loss: 0.790086\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1043/10000000000000]\n",
      "\tTraining Loss: 0.754694\n",
      "\tTesting Loss: 0.790405\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1044/10000000000000]\n",
      "\tTraining Loss: 0.754713\n",
      "\tTesting Loss: 0.790094\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1045/10000000000000]\n",
      "\tTraining Loss: 0.754737\n",
      "\tTesting Loss: 0.790794\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1046/10000000000000]\n",
      "\tTraining Loss: 0.754644\n",
      "\tTesting Loss: 0.789796\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1047/10000000000000]\n",
      "\tTraining Loss: 0.754600\n",
      "\tTesting Loss: 0.790641\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1048/10000000000000]\n",
      "\tTraining Loss: 0.754802\n",
      "\tTesting Loss: 0.790139\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1049/10000000000000]\n",
      "\tTraining Loss: 0.754637\n",
      "\tTesting Loss: 0.790688\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1050/10000000000000]\n",
      "\tTraining Loss: 0.754754\n",
      "\tTesting Loss: 0.789822\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1051/10000000000000]\n",
      "\tTraining Loss: 0.754681\n",
      "\tTesting Loss: 0.790901\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1052/10000000000000]\n",
      "\tTraining Loss: 0.754696\n",
      "\tTesting Loss: 0.791107\n",
      "\tLearning Rate: 0.000009223\n",
      "Epoch [1053/10000000000000]\n",
      "\tTraining Loss: 0.754689\n",
      "\tTesting Loss: 0.790441\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1054/10000000000000]\n",
      "\tTraining Loss: 0.754630\n",
      "\tTesting Loss: 0.789268\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1055/10000000000000]\n",
      "\tTraining Loss: 0.754666\n",
      "\tTesting Loss: 0.790167\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1056/10000000000000]\n",
      "\tTraining Loss: 0.754635\n",
      "\tTesting Loss: 0.790391\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1057/10000000000000]\n",
      "\tTraining Loss: 0.754582\n",
      "\tTesting Loss: 0.791365\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1058/10000000000000]\n",
      "\tTraining Loss: 0.754609\n",
      "\tTesting Loss: 0.790515\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1059/10000000000000]\n",
      "\tTraining Loss: 0.754613\n",
      "\tTesting Loss: 0.789249\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1060/10000000000000]\n",
      "\tTraining Loss: 0.754598\n",
      "\tTesting Loss: 0.789594\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1061/10000000000000]\n",
      "\tTraining Loss: 0.754637\n",
      "\tTesting Loss: 0.789881\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1062/10000000000000]\n",
      "\tTraining Loss: 0.754528\n",
      "\tTesting Loss: 0.790689\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1063/10000000000000]\n",
      "\tTraining Loss: 0.754659\n",
      "\tTesting Loss: 0.790605\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1064/10000000000000]\n",
      "\tTraining Loss: 0.754499\n",
      "\tTesting Loss: 0.790842\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1065/10000000000000]\n",
      "\tTraining Loss: 0.754566\n",
      "\tTesting Loss: 0.789980\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1066/10000000000000]\n",
      "\tTraining Loss: 0.754594\n",
      "\tTesting Loss: 0.790090\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1067/10000000000000]\n",
      "\tTraining Loss: 0.754568\n",
      "\tTesting Loss: 0.790656\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1068/10000000000000]\n",
      "\tTraining Loss: 0.754553\n",
      "\tTesting Loss: 0.791454\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1069/10000000000000]\n",
      "\tTraining Loss: 0.754615\n",
      "\tTesting Loss: 0.791400\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1070/10000000000000]\n",
      "\tTraining Loss: 0.754519\n",
      "\tTesting Loss: 0.790097\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1071/10000000000000]\n",
      "\tTraining Loss: 0.754573\n",
      "\tTesting Loss: 0.789885\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1072/10000000000000]\n",
      "\tTraining Loss: 0.754563\n",
      "\tTesting Loss: 0.789460\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1073/10000000000000]\n",
      "\tTraining Loss: 0.754558\n",
      "\tTesting Loss: 0.790821\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1074/10000000000000]\n",
      "\tTraining Loss: 0.754553\n",
      "\tTesting Loss: 0.789984\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1075/10000000000000]\n",
      "\tTraining Loss: 0.754511\n",
      "\tTesting Loss: 0.789730\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1076/10000000000000]\n",
      "\tTraining Loss: 0.754477\n",
      "\tTesting Loss: 0.789200\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1077/10000000000000]\n",
      "\tTraining Loss: 0.754561\n",
      "\tTesting Loss: 0.791639\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1078/10000000000000]\n",
      "\tTraining Loss: 0.754481\n",
      "\tTesting Loss: 0.790421\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1079/10000000000000]\n",
      "\tTraining Loss: 0.754549\n",
      "\tTesting Loss: 0.790590\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1080/10000000000000]\n",
      "\tTraining Loss: 0.754509\n",
      "\tTesting Loss: 0.791181\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1081/10000000000000]\n",
      "\tTraining Loss: 0.754487\n",
      "\tTesting Loss: 0.790475\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1082/10000000000000]\n",
      "\tTraining Loss: 0.754506\n",
      "\tTesting Loss: 0.789952\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1083/10000000000000]\n",
      "\tTraining Loss: 0.754471\n",
      "\tTesting Loss: 0.790158\n",
      "\tLearning Rate: 0.000007379\n",
      "Epoch [1084/10000000000000]\n",
      "\tTraining Loss: 0.754486\n",
      "\tTesting Loss: 0.790266\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1085/10000000000000]\n",
      "\tTraining Loss: 0.754455\n",
      "\tTesting Loss: 0.790251\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1086/10000000000000]\n",
      "\tTraining Loss: 0.754422\n",
      "\tTesting Loss: 0.790942\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1087/10000000000000]\n",
      "\tTraining Loss: 0.754380\n",
      "\tTesting Loss: 0.790508\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1088/10000000000000]\n",
      "\tTraining Loss: 0.754427\n",
      "\tTesting Loss: 0.790806\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1089/10000000000000]\n",
      "\tTraining Loss: 0.754407\n",
      "\tTesting Loss: 0.791110\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1090/10000000000000]\n",
      "\tTraining Loss: 0.754443\n",
      "\tTesting Loss: 0.790137\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1091/10000000000000]\n",
      "\tTraining Loss: 0.754443\n",
      "\tTesting Loss: 0.790283\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1092/10000000000000]\n",
      "\tTraining Loss: 0.754423\n",
      "\tTesting Loss: 0.789696\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1093/10000000000000]\n",
      "\tTraining Loss: 0.754454\n",
      "\tTesting Loss: 0.789845\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1094/10000000000000]\n",
      "\tTraining Loss: 0.754418\n",
      "\tTesting Loss: 0.790900\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1095/10000000000000]\n",
      "\tTraining Loss: 0.754413\n",
      "\tTesting Loss: 0.790892\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1096/10000000000000]\n",
      "\tTraining Loss: 0.754361\n",
      "\tTesting Loss: 0.790641\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1097/10000000000000]\n",
      "\tTraining Loss: 0.754398\n",
      "\tTesting Loss: 0.790210\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1098/10000000000000]\n",
      "\tTraining Loss: 0.754346\n",
      "\tTesting Loss: 0.790717\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1099/10000000000000]\n",
      "\tTraining Loss: 0.754412\n",
      "\tTesting Loss: 0.789955\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1100/10000000000000]\n",
      "\tTraining Loss: 0.754300\n",
      "\tTesting Loss: 0.791221\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1101/10000000000000]\n",
      "\tTraining Loss: 0.754369\n",
      "\tTesting Loss: 0.790025\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1102/10000000000000]\n",
      "\tTraining Loss: 0.754412\n",
      "\tTesting Loss: 0.790370\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1103/10000000000000]\n",
      "\tTraining Loss: 0.754380\n",
      "\tTesting Loss: 0.790130\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1104/10000000000000]\n",
      "\tTraining Loss: 0.754340\n",
      "\tTesting Loss: 0.790058\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1105/10000000000000]\n",
      "\tTraining Loss: 0.754304\n",
      "\tTesting Loss: 0.790470\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1106/10000000000000]\n",
      "\tTraining Loss: 0.754354\n",
      "\tTesting Loss: 0.790528\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1107/10000000000000]\n",
      "\tTraining Loss: 0.754331\n",
      "\tTesting Loss: 0.791431\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1108/10000000000000]\n",
      "\tTraining Loss: 0.754364\n",
      "\tTesting Loss: 0.791262\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1109/10000000000000]\n",
      "\tTraining Loss: 0.754296\n",
      "\tTesting Loss: 0.791128\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1110/10000000000000]\n",
      "\tTraining Loss: 0.754295\n",
      "\tTesting Loss: 0.789735\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1111/10000000000000]\n",
      "\tTraining Loss: 0.754347\n",
      "\tTesting Loss: 0.790238\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1112/10000000000000]\n",
      "\tTraining Loss: 0.754383\n",
      "\tTesting Loss: 0.791145\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1113/10000000000000]\n",
      "\tTraining Loss: 0.754324\n",
      "\tTesting Loss: 0.790684\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1114/10000000000000]\n",
      "\tTraining Loss: 0.754318\n",
      "\tTesting Loss: 0.791538\n",
      "\tLearning Rate: 0.000005903\n",
      "Epoch [1115/10000000000000]\n",
      "\tTraining Loss: 0.754307\n",
      "\tTesting Loss: 0.790529\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1116/10000000000000]\n",
      "\tTraining Loss: 0.754268\n",
      "\tTesting Loss: 0.792013\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1117/10000000000000]\n",
      "\tTraining Loss: 0.754317\n",
      "\tTesting Loss: 0.790434\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1118/10000000000000]\n",
      "\tTraining Loss: 0.754276\n",
      "\tTesting Loss: 0.790711\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1119/10000000000000]\n",
      "\tTraining Loss: 0.754271\n",
      "\tTesting Loss: 0.790391\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1120/10000000000000]\n",
      "\tTraining Loss: 0.754340\n",
      "\tTesting Loss: 0.790828\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1121/10000000000000]\n",
      "\tTraining Loss: 0.754289\n",
      "\tTesting Loss: 0.791438\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1122/10000000000000]\n",
      "\tTraining Loss: 0.754290\n",
      "\tTesting Loss: 0.791231\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1123/10000000000000]\n",
      "\tTraining Loss: 0.754282\n",
      "\tTesting Loss: 0.791211\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1124/10000000000000]\n",
      "\tTraining Loss: 0.754303\n",
      "\tTesting Loss: 0.791306\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1125/10000000000000]\n",
      "\tTraining Loss: 0.754242\n",
      "\tTesting Loss: 0.790007\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1126/10000000000000]\n",
      "\tTraining Loss: 0.754240\n",
      "\tTesting Loss: 0.790772\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1127/10000000000000]\n",
      "\tTraining Loss: 0.754265\n",
      "\tTesting Loss: 0.791065\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1128/10000000000000]\n",
      "\tTraining Loss: 0.754236\n",
      "\tTesting Loss: 0.790352\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1129/10000000000000]\n",
      "\tTraining Loss: 0.754265\n",
      "\tTesting Loss: 0.789745\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1130/10000000000000]\n",
      "\tTraining Loss: 0.754219\n",
      "\tTesting Loss: 0.790660\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1131/10000000000000]\n",
      "\tTraining Loss: 0.754256\n",
      "\tTesting Loss: 0.789982\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1132/10000000000000]\n",
      "\tTraining Loss: 0.754291\n",
      "\tTesting Loss: 0.791278\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1133/10000000000000]\n",
      "\tTraining Loss: 0.754174\n",
      "\tTesting Loss: 0.790270\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1134/10000000000000]\n",
      "\tTraining Loss: 0.754162\n",
      "\tTesting Loss: 0.790759\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1135/10000000000000]\n",
      "\tTraining Loss: 0.754300\n",
      "\tTesting Loss: 0.791422\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1136/10000000000000]\n",
      "\tTraining Loss: 0.754224\n",
      "\tTesting Loss: 0.790258\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1137/10000000000000]\n",
      "\tTraining Loss: 0.754213\n",
      "\tTesting Loss: 0.790647\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1138/10000000000000]\n",
      "\tTraining Loss: 0.754202\n",
      "\tTesting Loss: 0.790891\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1139/10000000000000]\n",
      "\tTraining Loss: 0.754180\n",
      "\tTesting Loss: 0.790243\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1140/10000000000000]\n",
      "\tTraining Loss: 0.754261\n",
      "\tTesting Loss: 0.790410\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1141/10000000000000]\n",
      "\tTraining Loss: 0.754214\n",
      "\tTesting Loss: 0.790891\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1142/10000000000000]\n",
      "\tTraining Loss: 0.754227\n",
      "\tTesting Loss: 0.791219\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1143/10000000000000]\n",
      "\tTraining Loss: 0.754182\n",
      "\tTesting Loss: 0.791829\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1144/10000000000000]\n",
      "\tTraining Loss: 0.754199\n",
      "\tTesting Loss: 0.790206\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1145/10000000000000]\n",
      "\tTraining Loss: 0.754215\n",
      "\tTesting Loss: 0.790648\n",
      "\tLearning Rate: 0.000004722\n",
      "Epoch [1146/10000000000000]\n",
      "\tTraining Loss: 0.754230\n",
      "\tTesting Loss: 0.790899\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1147/10000000000000]\n",
      "\tTraining Loss: 0.754211\n",
      "\tTesting Loss: 0.791605\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1148/10000000000000]\n",
      "\tTraining Loss: 0.754180\n",
      "\tTesting Loss: 0.790610\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1149/10000000000000]\n",
      "\tTraining Loss: 0.754166\n",
      "\tTesting Loss: 0.790599\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1150/10000000000000]\n",
      "\tTraining Loss: 0.754127\n",
      "\tTesting Loss: 0.790651\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1151/10000000000000]\n",
      "\tTraining Loss: 0.754120\n",
      "\tTesting Loss: 0.790405\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1152/10000000000000]\n",
      "\tTraining Loss: 0.754173\n",
      "\tTesting Loss: 0.789445\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1153/10000000000000]\n",
      "\tTraining Loss: 0.754099\n",
      "\tTesting Loss: 0.790315\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1154/10000000000000]\n",
      "\tTraining Loss: 0.754226\n",
      "\tTesting Loss: 0.791201\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1155/10000000000000]\n",
      "\tTraining Loss: 0.754181\n",
      "\tTesting Loss: 0.790889\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1156/10000000000000]\n",
      "\tTraining Loss: 0.754083\n",
      "\tTesting Loss: 0.790431\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1157/10000000000000]\n",
      "\tTraining Loss: 0.754144\n",
      "\tTesting Loss: 0.791631\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1158/10000000000000]\n",
      "\tTraining Loss: 0.754185\n",
      "\tTesting Loss: 0.791065\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1159/10000000000000]\n",
      "\tTraining Loss: 0.754117\n",
      "\tTesting Loss: 0.790049\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1160/10000000000000]\n",
      "\tTraining Loss: 0.754142\n",
      "\tTesting Loss: 0.790690\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1161/10000000000000]\n",
      "\tTraining Loss: 0.754149\n",
      "\tTesting Loss: 0.790881\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1162/10000000000000]\n",
      "\tTraining Loss: 0.754160\n",
      "\tTesting Loss: 0.790349\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1163/10000000000000]\n",
      "\tTraining Loss: 0.754116\n",
      "\tTesting Loss: 0.790331\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1164/10000000000000]\n",
      "\tTraining Loss: 0.754121\n",
      "\tTesting Loss: 0.790246\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1165/10000000000000]\n",
      "\tTraining Loss: 0.754129\n",
      "\tTesting Loss: 0.790845\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1166/10000000000000]\n",
      "\tTraining Loss: 0.754120\n",
      "\tTesting Loss: 0.790107\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1167/10000000000000]\n",
      "\tTraining Loss: 0.754095\n",
      "\tTesting Loss: 0.790039\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1168/10000000000000]\n",
      "\tTraining Loss: 0.754099\n",
      "\tTesting Loss: 0.791451\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1169/10000000000000]\n",
      "\tTraining Loss: 0.754109\n",
      "\tTesting Loss: 0.790795\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1170/10000000000000]\n",
      "\tTraining Loss: 0.754129\n",
      "\tTesting Loss: 0.790377\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1171/10000000000000]\n",
      "\tTraining Loss: 0.754073\n",
      "\tTesting Loss: 0.790203\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1172/10000000000000]\n",
      "\tTraining Loss: 0.754082\n",
      "\tTesting Loss: 0.789728\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1173/10000000000000]\n",
      "\tTraining Loss: 0.754122\n",
      "\tTesting Loss: 0.791479\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1174/10000000000000]\n",
      "\tTraining Loss: 0.754088\n",
      "\tTesting Loss: 0.790740\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1175/10000000000000]\n",
      "\tTraining Loss: 0.754140\n",
      "\tTesting Loss: 0.791204\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1176/10000000000000]\n",
      "\tTraining Loss: 0.754125\n",
      "\tTesting Loss: 0.791111\n",
      "\tLearning Rate: 0.000003778\n",
      "Epoch [1177/10000000000000]\n",
      "\tTraining Loss: 0.754093\n",
      "\tTesting Loss: 0.790862\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1178/10000000000000]\n",
      "\tTraining Loss: 0.754078\n",
      "\tTesting Loss: 0.790707\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1179/10000000000000]\n",
      "\tTraining Loss: 0.754066\n",
      "\tTesting Loss: 0.789969\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1180/10000000000000]\n",
      "\tTraining Loss: 0.754059\n",
      "\tTesting Loss: 0.790567\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1181/10000000000000]\n",
      "\tTraining Loss: 0.754020\n",
      "\tTesting Loss: 0.790155\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1182/10000000000000]\n",
      "\tTraining Loss: 0.754134\n",
      "\tTesting Loss: 0.791272\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1183/10000000000000]\n",
      "\tTraining Loss: 0.754048\n",
      "\tTesting Loss: 0.791060\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1184/10000000000000]\n",
      "\tTraining Loss: 0.753976\n",
      "\tTesting Loss: 0.790942\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1185/10000000000000]\n",
      "\tTraining Loss: 0.754075\n",
      "\tTesting Loss: 0.791010\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1186/10000000000000]\n",
      "\tTraining Loss: 0.754066\n",
      "\tTesting Loss: 0.791242\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1187/10000000000000]\n",
      "\tTraining Loss: 0.754080\n",
      "\tTesting Loss: 0.790612\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1188/10000000000000]\n",
      "\tTraining Loss: 0.754055\n",
      "\tTesting Loss: 0.790960\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1189/10000000000000]\n",
      "\tTraining Loss: 0.754041\n",
      "\tTesting Loss: 0.789863\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1190/10000000000000]\n",
      "\tTraining Loss: 0.754064\n",
      "\tTesting Loss: 0.790558\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1191/10000000000000]\n",
      "\tTraining Loss: 0.754029\n",
      "\tTesting Loss: 0.790886\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1192/10000000000000]\n",
      "\tTraining Loss: 0.754015\n",
      "\tTesting Loss: 0.791382\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1193/10000000000000]\n",
      "\tTraining Loss: 0.754058\n",
      "\tTesting Loss: 0.790251\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1194/10000000000000]\n",
      "\tTraining Loss: 0.754043\n",
      "\tTesting Loss: 0.790366\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1195/10000000000000]\n",
      "\tTraining Loss: 0.754032\n",
      "\tTesting Loss: 0.791010\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1196/10000000000000]\n",
      "\tTraining Loss: 0.754039\n",
      "\tTesting Loss: 0.790681\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1197/10000000000000]\n",
      "\tTraining Loss: 0.754004\n",
      "\tTesting Loss: 0.790905\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1198/10000000000000]\n",
      "\tTraining Loss: 0.754015\n",
      "\tTesting Loss: 0.790844\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1199/10000000000000]\n",
      "\tTraining Loss: 0.754028\n",
      "\tTesting Loss: 0.790046\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1200/10000000000000]\n",
      "\tTraining Loss: 0.753973\n",
      "\tTesting Loss: 0.790190\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1201/10000000000000]\n",
      "\tTraining Loss: 0.754054\n",
      "\tTesting Loss: 0.791209\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1202/10000000000000]\n",
      "\tTraining Loss: 0.753927\n",
      "\tTesting Loss: 0.791016\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1203/10000000000000]\n",
      "\tTraining Loss: 0.754016\n",
      "\tTesting Loss: 0.789822\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1204/10000000000000]\n",
      "\tTraining Loss: 0.754073\n",
      "\tTesting Loss: 0.790702\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1205/10000000000000]\n",
      "\tTraining Loss: 0.754057\n",
      "\tTesting Loss: 0.791493\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1206/10000000000000]\n",
      "\tTraining Loss: 0.753997\n",
      "\tTesting Loss: 0.791055\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1207/10000000000000]\n",
      "\tTraining Loss: 0.754023\n",
      "\tTesting Loss: 0.790362\n",
      "\tLearning Rate: 0.000003022\n",
      "Epoch [1208/10000000000000]\n",
      "\tTraining Loss: 0.753993\n",
      "\tTesting Loss: 0.789794\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1209/10000000000000]\n",
      "\tTraining Loss: 0.754016\n",
      "\tTesting Loss: 0.790805\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1210/10000000000000]\n",
      "\tTraining Loss: 0.754034\n",
      "\tTesting Loss: 0.791122\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1211/10000000000000]\n",
      "\tTraining Loss: 0.753962\n",
      "\tTesting Loss: 0.789958\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1212/10000000000000]\n",
      "\tTraining Loss: 0.753962\n",
      "\tTesting Loss: 0.791175\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1213/10000000000000]\n",
      "\tTraining Loss: 0.754009\n",
      "\tTesting Loss: 0.791058\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1214/10000000000000]\n",
      "\tTraining Loss: 0.754011\n",
      "\tTesting Loss: 0.791147\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1215/10000000000000]\n",
      "\tTraining Loss: 0.754017\n",
      "\tTesting Loss: 0.790701\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1216/10000000000000]\n",
      "\tTraining Loss: 0.753983\n",
      "\tTesting Loss: 0.791299\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1217/10000000000000]\n",
      "\tTraining Loss: 0.753954\n",
      "\tTesting Loss: 0.790644\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1218/10000000000000]\n",
      "\tTraining Loss: 0.753979\n",
      "\tTesting Loss: 0.790382\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1219/10000000000000]\n",
      "\tTraining Loss: 0.753953\n",
      "\tTesting Loss: 0.790014\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1220/10000000000000]\n",
      "\tTraining Loss: 0.753979\n",
      "\tTesting Loss: 0.790423\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1221/10000000000000]\n",
      "\tTraining Loss: 0.753985\n",
      "\tTesting Loss: 0.790723\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1222/10000000000000]\n",
      "\tTraining Loss: 0.753966\n",
      "\tTesting Loss: 0.790986\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1223/10000000000000]\n",
      "\tTraining Loss: 0.753995\n",
      "\tTesting Loss: 0.790175\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1224/10000000000000]\n",
      "\tTraining Loss: 0.753959\n",
      "\tTesting Loss: 0.791169\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1225/10000000000000]\n",
      "\tTraining Loss: 0.753923\n",
      "\tTesting Loss: 0.790444\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1226/10000000000000]\n",
      "\tTraining Loss: 0.753959\n",
      "\tTesting Loss: 0.792362\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1227/10000000000000]\n",
      "\tTraining Loss: 0.753965\n",
      "\tTesting Loss: 0.790976\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1228/10000000000000]\n",
      "\tTraining Loss: 0.753968\n",
      "\tTesting Loss: 0.791802\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1229/10000000000000]\n",
      "\tTraining Loss: 0.753967\n",
      "\tTesting Loss: 0.790764\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1230/10000000000000]\n",
      "\tTraining Loss: 0.753928\n",
      "\tTesting Loss: 0.790667\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1231/10000000000000]\n",
      "\tTraining Loss: 0.753948\n",
      "\tTesting Loss: 0.790101\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1232/10000000000000]\n",
      "\tTraining Loss: 0.753997\n",
      "\tTesting Loss: 0.790708\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1233/10000000000000]\n",
      "\tTraining Loss: 0.754005\n",
      "\tTesting Loss: 0.791812\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1234/10000000000000]\n",
      "\tTraining Loss: 0.753985\n",
      "\tTesting Loss: 0.790601\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1235/10000000000000]\n",
      "\tTraining Loss: 0.753919\n",
      "\tTesting Loss: 0.790869\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1236/10000000000000]\n",
      "\tTraining Loss: 0.753959\n",
      "\tTesting Loss: 0.790485\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1237/10000000000000]\n",
      "\tTraining Loss: 0.753941\n",
      "\tTesting Loss: 0.790772\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1238/10000000000000]\n",
      "\tTraining Loss: 0.753990\n",
      "\tTesting Loss: 0.790816\n",
      "\tLearning Rate: 0.000002418\n",
      "Epoch [1239/10000000000000]\n",
      "\tTraining Loss: 0.753941\n",
      "\tTesting Loss: 0.790824\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1240/10000000000000]\n",
      "\tTraining Loss: 0.753915\n",
      "\tTesting Loss: 0.791460\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1241/10000000000000]\n",
      "\tTraining Loss: 0.753968\n",
      "\tTesting Loss: 0.790904\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1242/10000000000000]\n",
      "\tTraining Loss: 0.753893\n",
      "\tTesting Loss: 0.790825\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1243/10000000000000]\n",
      "\tTraining Loss: 0.753937\n",
      "\tTesting Loss: 0.790359\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1244/10000000000000]\n",
      "\tTraining Loss: 0.753938\n",
      "\tTesting Loss: 0.790534\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1245/10000000000000]\n",
      "\tTraining Loss: 0.753910\n",
      "\tTesting Loss: 0.790533\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1246/10000000000000]\n",
      "\tTraining Loss: 0.753885\n",
      "\tTesting Loss: 0.790614\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1247/10000000000000]\n",
      "\tTraining Loss: 0.753874\n",
      "\tTesting Loss: 0.790050\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1248/10000000000000]\n",
      "\tTraining Loss: 0.753935\n",
      "\tTesting Loss: 0.790642\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1249/10000000000000]\n",
      "\tTraining Loss: 0.753960\n",
      "\tTesting Loss: 0.792180\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1250/10000000000000]\n",
      "\tTraining Loss: 0.753917\n",
      "\tTesting Loss: 0.790867\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1251/10000000000000]\n",
      "\tTraining Loss: 0.753911\n",
      "\tTesting Loss: 0.791173\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1252/10000000000000]\n",
      "\tTraining Loss: 0.753877\n",
      "\tTesting Loss: 0.790549\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1253/10000000000000]\n",
      "\tTraining Loss: 0.753883\n",
      "\tTesting Loss: 0.791052\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1254/10000000000000]\n",
      "\tTraining Loss: 0.753930\n",
      "\tTesting Loss: 0.790263\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1255/10000000000000]\n",
      "\tTraining Loss: 0.753896\n",
      "\tTesting Loss: 0.790447\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1256/10000000000000]\n",
      "\tTraining Loss: 0.753926\n",
      "\tTesting Loss: 0.790969\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1257/10000000000000]\n",
      "\tTraining Loss: 0.753931\n",
      "\tTesting Loss: 0.790865\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1258/10000000000000]\n",
      "\tTraining Loss: 0.753978\n",
      "\tTesting Loss: 0.791216\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1259/10000000000000]\n",
      "\tTraining Loss: 0.753877\n",
      "\tTesting Loss: 0.790915\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1260/10000000000000]\n",
      "\tTraining Loss: 0.753844\n",
      "\tTesting Loss: 0.790480\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1261/10000000000000]\n",
      "\tTraining Loss: 0.753897\n",
      "\tTesting Loss: 0.792305\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1262/10000000000000]\n",
      "\tTraining Loss: 0.753928\n",
      "\tTesting Loss: 0.789861\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1263/10000000000000]\n",
      "\tTraining Loss: 0.753934\n",
      "\tTesting Loss: 0.792162\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1264/10000000000000]\n",
      "\tTraining Loss: 0.753838\n",
      "\tTesting Loss: 0.791000\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1265/10000000000000]\n",
      "\tTraining Loss: 0.753881\n",
      "\tTesting Loss: 0.790476\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1266/10000000000000]\n",
      "\tTraining Loss: 0.753899\n",
      "\tTesting Loss: 0.790806\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1267/10000000000000]\n",
      "\tTraining Loss: 0.753897\n",
      "\tTesting Loss: 0.791089\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1268/10000000000000]\n",
      "\tTraining Loss: 0.753889\n",
      "\tTesting Loss: 0.791492\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1269/10000000000000]\n",
      "\tTraining Loss: 0.753946\n",
      "\tTesting Loss: 0.791430\n",
      "\tLearning Rate: 0.000001934\n",
      "Epoch [1270/10000000000000]\n",
      "\tTraining Loss: 0.753899\n",
      "\tTesting Loss: 0.790749\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1271/10000000000000]\n",
      "\tTraining Loss: 0.753903\n",
      "\tTesting Loss: 0.790932\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1272/10000000000000]\n",
      "\tTraining Loss: 0.753875\n",
      "\tTesting Loss: 0.791550\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1273/10000000000000]\n",
      "\tTraining Loss: 0.753870\n",
      "\tTesting Loss: 0.791004\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1274/10000000000000]\n",
      "\tTraining Loss: 0.753847\n",
      "\tTesting Loss: 0.791027\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1275/10000000000000]\n",
      "\tTraining Loss: 0.753864\n",
      "\tTesting Loss: 0.791434\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1276/10000000000000]\n",
      "\tTraining Loss: 0.753855\n",
      "\tTesting Loss: 0.790998\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1277/10000000000000]\n",
      "\tTraining Loss: 0.753874\n",
      "\tTesting Loss: 0.790839\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1278/10000000000000]\n",
      "\tTraining Loss: 0.753907\n",
      "\tTesting Loss: 0.790911\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1279/10000000000000]\n",
      "\tTraining Loss: 0.753871\n",
      "\tTesting Loss: 0.791181\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1280/10000000000000]\n",
      "\tTraining Loss: 0.753934\n",
      "\tTesting Loss: 0.790602\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1281/10000000000000]\n",
      "\tTraining Loss: 0.753916\n",
      "\tTesting Loss: 0.790196\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1282/10000000000000]\n",
      "\tTraining Loss: 0.753906\n",
      "\tTesting Loss: 0.791217\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1283/10000000000000]\n",
      "\tTraining Loss: 0.753864\n",
      "\tTesting Loss: 0.791178\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1284/10000000000000]\n",
      "\tTraining Loss: 0.753860\n",
      "\tTesting Loss: 0.790752\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1285/10000000000000]\n",
      "\tTraining Loss: 0.753851\n",
      "\tTesting Loss: 0.790520\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1286/10000000000000]\n",
      "\tTraining Loss: 0.753895\n",
      "\tTesting Loss: 0.791144\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1287/10000000000000]\n",
      "\tTraining Loss: 0.753888\n",
      "\tTesting Loss: 0.790635\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1288/10000000000000]\n",
      "\tTraining Loss: 0.753831\n",
      "\tTesting Loss: 0.790701\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1289/10000000000000]\n",
      "\tTraining Loss: 0.753839\n",
      "\tTesting Loss: 0.790979\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1290/10000000000000]\n",
      "\tTraining Loss: 0.753868\n",
      "\tTesting Loss: 0.791245\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1291/10000000000000]\n",
      "\tTraining Loss: 0.753916\n",
      "\tTesting Loss: 0.790403\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1292/10000000000000]\n",
      "\tTraining Loss: 0.753872\n",
      "\tTesting Loss: 0.790983\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1293/10000000000000]\n",
      "\tTraining Loss: 0.753892\n",
      "\tTesting Loss: 0.791012\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1294/10000000000000]\n",
      "\tTraining Loss: 0.753824\n",
      "\tTesting Loss: 0.790947\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1295/10000000000000]\n",
      "\tTraining Loss: 0.753850\n",
      "\tTesting Loss: 0.791030\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1296/10000000000000]\n",
      "\tTraining Loss: 0.753795\n",
      "\tTesting Loss: 0.791139\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1297/10000000000000]\n",
      "\tTraining Loss: 0.753808\n",
      "\tTesting Loss: 0.790918\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1298/10000000000000]\n",
      "\tTraining Loss: 0.753876\n",
      "\tTesting Loss: 0.791173\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1299/10000000000000]\n",
      "\tTraining Loss: 0.753854\n",
      "\tTesting Loss: 0.791282\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1300/10000000000000]\n",
      "\tTraining Loss: 0.753861\n",
      "\tTesting Loss: 0.790919\n",
      "\tLearning Rate: 0.000001547\n",
      "Epoch [1301/10000000000000]\n",
      "\tTraining Loss: 0.753864\n",
      "\tTesting Loss: 0.791132\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1302/10000000000000]\n",
      "\tTraining Loss: 0.753857\n",
      "\tTesting Loss: 0.790663\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1303/10000000000000]\n",
      "\tTraining Loss: 0.753888\n",
      "\tTesting Loss: 0.790712\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1304/10000000000000]\n",
      "\tTraining Loss: 0.753838\n",
      "\tTesting Loss: 0.791458\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1305/10000000000000]\n",
      "\tTraining Loss: 0.753805\n",
      "\tTesting Loss: 0.790851\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1306/10000000000000]\n",
      "\tTraining Loss: 0.753859\n",
      "\tTesting Loss: 0.790045\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1307/10000000000000]\n",
      "\tTraining Loss: 0.753869\n",
      "\tTesting Loss: 0.790715\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1308/10000000000000]\n",
      "\tTraining Loss: 0.753781\n",
      "\tTesting Loss: 0.790717\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1309/10000000000000]\n",
      "\tTraining Loss: 0.753797\n",
      "\tTesting Loss: 0.790656\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1310/10000000000000]\n",
      "\tTraining Loss: 0.753830\n",
      "\tTesting Loss: 0.790671\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1311/10000000000000]\n",
      "\tTraining Loss: 0.753793\n",
      "\tTesting Loss: 0.790405\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1312/10000000000000]\n",
      "\tTraining Loss: 0.753845\n",
      "\tTesting Loss: 0.791036\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1313/10000000000000]\n",
      "\tTraining Loss: 0.753826\n",
      "\tTesting Loss: 0.790099\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1314/10000000000000]\n",
      "\tTraining Loss: 0.753845\n",
      "\tTesting Loss: 0.790703\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1315/10000000000000]\n",
      "\tTraining Loss: 0.753796\n",
      "\tTesting Loss: 0.790876\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1316/10000000000000]\n",
      "\tTraining Loss: 0.753858\n",
      "\tTesting Loss: 0.791098\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1317/10000000000000]\n",
      "\tTraining Loss: 0.753751\n",
      "\tTesting Loss: 0.790914\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1318/10000000000000]\n",
      "\tTraining Loss: 0.753866\n",
      "\tTesting Loss: 0.790553\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1319/10000000000000]\n",
      "\tTraining Loss: 0.753807\n",
      "\tTesting Loss: 0.790788\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1320/10000000000000]\n",
      "\tTraining Loss: 0.753816\n",
      "\tTesting Loss: 0.790657\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1321/10000000000000]\n",
      "\tTraining Loss: 0.753858\n",
      "\tTesting Loss: 0.790326\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1322/10000000000000]\n",
      "\tTraining Loss: 0.753856\n",
      "\tTesting Loss: 0.790573\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1323/10000000000000]\n",
      "\tTraining Loss: 0.753828\n",
      "\tTesting Loss: 0.790086\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1324/10000000000000]\n",
      "\tTraining Loss: 0.753798\n",
      "\tTesting Loss: 0.790491\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1325/10000000000000]\n",
      "\tTraining Loss: 0.753867\n",
      "\tTesting Loss: 0.790790\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1326/10000000000000]\n",
      "\tTraining Loss: 0.753758\n",
      "\tTesting Loss: 0.789573\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1327/10000000000000]\n",
      "\tTraining Loss: 0.753781\n",
      "\tTesting Loss: 0.790223\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1328/10000000000000]\n",
      "\tTraining Loss: 0.753823\n",
      "\tTesting Loss: 0.791988\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1329/10000000000000]\n",
      "\tTraining Loss: 0.753823\n",
      "\tTesting Loss: 0.791292\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1330/10000000000000]\n",
      "\tTraining Loss: 0.753833\n",
      "\tTesting Loss: 0.790632\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1331/10000000000000]\n",
      "\tTraining Loss: 0.753821\n",
      "\tTesting Loss: 0.791295\n",
      "\tLearning Rate: 0.000001238\n",
      "Epoch [1332/10000000000000]\n",
      "\tTraining Loss: 0.753882\n",
      "\tTesting Loss: 0.791231\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1333/10000000000000]\n",
      "\tTraining Loss: 0.753780\n",
      "\tTesting Loss: 0.790695\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1334/10000000000000]\n",
      "\tTraining Loss: 0.753761\n",
      "\tTesting Loss: 0.791112\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1335/10000000000000]\n",
      "\tTraining Loss: 0.753714\n",
      "\tTesting Loss: 0.791892\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1336/10000000000000]\n",
      "\tTraining Loss: 0.753814\n",
      "\tTesting Loss: 0.789856\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1337/10000000000000]\n",
      "\tTraining Loss: 0.753786\n",
      "\tTesting Loss: 0.790567\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1338/10000000000000]\n",
      "\tTraining Loss: 0.753765\n",
      "\tTesting Loss: 0.792238\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1339/10000000000000]\n",
      "\tTraining Loss: 0.753862\n",
      "\tTesting Loss: 0.791496\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1340/10000000000000]\n",
      "\tTraining Loss: 0.753781\n",
      "\tTesting Loss: 0.791453\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1341/10000000000000]\n",
      "\tTraining Loss: 0.753810\n",
      "\tTesting Loss: 0.791365\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1342/10000000000000]\n",
      "\tTraining Loss: 0.753817\n",
      "\tTesting Loss: 0.791366\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1343/10000000000000]\n",
      "\tTraining Loss: 0.753847\n",
      "\tTesting Loss: 0.790980\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1344/10000000000000]\n",
      "\tTraining Loss: 0.753789\n",
      "\tTesting Loss: 0.790478\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1345/10000000000000]\n",
      "\tTraining Loss: 0.753756\n",
      "\tTesting Loss: 0.790931\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1346/10000000000000]\n",
      "\tTraining Loss: 0.753780\n",
      "\tTesting Loss: 0.790861\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1347/10000000000000]\n",
      "\tTraining Loss: 0.753785\n",
      "\tTesting Loss: 0.790591\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1348/10000000000000]\n",
      "\tTraining Loss: 0.753786\n",
      "\tTesting Loss: 0.790926\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1349/10000000000000]\n",
      "\tTraining Loss: 0.753788\n",
      "\tTesting Loss: 0.791439\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1350/10000000000000]\n",
      "\tTraining Loss: 0.753813\n",
      "\tTesting Loss: 0.790522\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1351/10000000000000]\n",
      "\tTraining Loss: 0.753783\n",
      "\tTesting Loss: 0.791067\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1352/10000000000000]\n",
      "\tTraining Loss: 0.753799\n",
      "\tTesting Loss: 0.791280\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1353/10000000000000]\n",
      "\tTraining Loss: 0.753831\n",
      "\tTesting Loss: 0.790754\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1354/10000000000000]\n",
      "\tTraining Loss: 0.753747\n",
      "\tTesting Loss: 0.790774\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1355/10000000000000]\n",
      "\tTraining Loss: 0.753847\n",
      "\tTesting Loss: 0.790819\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1356/10000000000000]\n",
      "\tTraining Loss: 0.753786\n",
      "\tTesting Loss: 0.790419\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1357/10000000000000]\n",
      "\tTraining Loss: 0.753783\n",
      "\tTesting Loss: 0.790901\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1358/10000000000000]\n",
      "\tTraining Loss: 0.753776\n",
      "\tTesting Loss: 0.791136\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1359/10000000000000]\n",
      "\tTraining Loss: 0.753775\n",
      "\tTesting Loss: 0.791632\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1360/10000000000000]\n",
      "\tTraining Loss: 0.753772\n",
      "\tTesting Loss: 0.791385\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1361/10000000000000]\n",
      "\tTraining Loss: 0.753776\n",
      "\tTesting Loss: 0.790373\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1362/10000000000000]\n",
      "\tTraining Loss: 0.753822\n",
      "\tTesting Loss: 0.790708\n",
      "\tLearning Rate: 0.000000990\n",
      "Epoch [1363/10000000000000]\n",
      "\tTraining Loss: 0.753809\n",
      "\tTesting Loss: 0.791196\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1364/10000000000000]\n",
      "\tTraining Loss: 0.753765\n",
      "\tTesting Loss: 0.791243\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1365/10000000000000]\n",
      "\tTraining Loss: 0.753768\n",
      "\tTesting Loss: 0.791211\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1366/10000000000000]\n",
      "\tTraining Loss: 0.753793\n",
      "\tTesting Loss: 0.790238\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1367/10000000000000]\n",
      "\tTraining Loss: 0.753853\n",
      "\tTesting Loss: 0.791306\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1368/10000000000000]\n",
      "\tTraining Loss: 0.753763\n",
      "\tTesting Loss: 0.791343\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1369/10000000000000]\n",
      "\tTraining Loss: 0.753788\n",
      "\tTesting Loss: 0.791104\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1370/10000000000000]\n",
      "\tTraining Loss: 0.753802\n",
      "\tTesting Loss: 0.790647\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1371/10000000000000]\n",
      "\tTraining Loss: 0.753810\n",
      "\tTesting Loss: 0.790980\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1372/10000000000000]\n",
      "\tTraining Loss: 0.753806\n",
      "\tTesting Loss: 0.790965\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1373/10000000000000]\n",
      "\tTraining Loss: 0.753777\n",
      "\tTesting Loss: 0.791006\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1374/10000000000000]\n",
      "\tTraining Loss: 0.753786\n",
      "\tTesting Loss: 0.790832\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1375/10000000000000]\n",
      "\tTraining Loss: 0.753771\n",
      "\tTesting Loss: 0.790743\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1376/10000000000000]\n",
      "\tTraining Loss: 0.753721\n",
      "\tTesting Loss: 0.790299\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1377/10000000000000]\n",
      "\tTraining Loss: 0.753783\n",
      "\tTesting Loss: 0.791002\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1378/10000000000000]\n",
      "\tTraining Loss: 0.753739\n",
      "\tTesting Loss: 0.791440\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1379/10000000000000]\n",
      "\tTraining Loss: 0.753795\n",
      "\tTesting Loss: 0.790687\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1380/10000000000000]\n",
      "\tTraining Loss: 0.753790\n",
      "\tTesting Loss: 0.790880\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1381/10000000000000]\n",
      "\tTraining Loss: 0.753761\n",
      "\tTesting Loss: 0.791163\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1382/10000000000000]\n",
      "\tTraining Loss: 0.753728\n",
      "\tTesting Loss: 0.791174\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1383/10000000000000]\n",
      "\tTraining Loss: 0.753758\n",
      "\tTesting Loss: 0.791046\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1384/10000000000000]\n",
      "\tTraining Loss: 0.753746\n",
      "\tTesting Loss: 0.791336\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1385/10000000000000]\n",
      "\tTraining Loss: 0.753810\n",
      "\tTesting Loss: 0.790500\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1386/10000000000000]\n",
      "\tTraining Loss: 0.753785\n",
      "\tTesting Loss: 0.792020\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1387/10000000000000]\n",
      "\tTraining Loss: 0.753809\n",
      "\tTesting Loss: 0.791470\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1388/10000000000000]\n",
      "\tTraining Loss: 0.753747\n",
      "\tTesting Loss: 0.791133\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1389/10000000000000]\n",
      "\tTraining Loss: 0.753738\n",
      "\tTesting Loss: 0.790747\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1390/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.790660\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1391/10000000000000]\n",
      "\tTraining Loss: 0.753729\n",
      "\tTesting Loss: 0.790255\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1392/10000000000000]\n",
      "\tTraining Loss: 0.753756\n",
      "\tTesting Loss: 0.790835\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1393/10000000000000]\n",
      "\tTraining Loss: 0.753752\n",
      "\tTesting Loss: 0.790735\n",
      "\tLearning Rate: 0.000000792\n",
      "Epoch [1394/10000000000000]\n",
      "\tTraining Loss: 0.753711\n",
      "\tTesting Loss: 0.791140\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1395/10000000000000]\n",
      "\tTraining Loss: 0.753799\n",
      "\tTesting Loss: 0.790159\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1396/10000000000000]\n",
      "\tTraining Loss: 0.753744\n",
      "\tTesting Loss: 0.791037\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1397/10000000000000]\n",
      "\tTraining Loss: 0.753811\n",
      "\tTesting Loss: 0.790578\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1398/10000000000000]\n",
      "\tTraining Loss: 0.753697\n",
      "\tTesting Loss: 0.790795\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1399/10000000000000]\n",
      "\tTraining Loss: 0.753777\n",
      "\tTesting Loss: 0.791048\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1400/10000000000000]\n",
      "\tTraining Loss: 0.753730\n",
      "\tTesting Loss: 0.790881\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1401/10000000000000]\n",
      "\tTraining Loss: 0.753786\n",
      "\tTesting Loss: 0.791404\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1402/10000000000000]\n",
      "\tTraining Loss: 0.753787\n",
      "\tTesting Loss: 0.791692\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1403/10000000000000]\n",
      "\tTraining Loss: 0.753800\n",
      "\tTesting Loss: 0.791705\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1404/10000000000000]\n",
      "\tTraining Loss: 0.753758\n",
      "\tTesting Loss: 0.790439\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1405/10000000000000]\n",
      "\tTraining Loss: 0.753728\n",
      "\tTesting Loss: 0.790704\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1406/10000000000000]\n",
      "\tTraining Loss: 0.753756\n",
      "\tTesting Loss: 0.790762\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1407/10000000000000]\n",
      "\tTraining Loss: 0.753776\n",
      "\tTesting Loss: 0.791198\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1408/10000000000000]\n",
      "\tTraining Loss: 0.753727\n",
      "\tTesting Loss: 0.790451\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1409/10000000000000]\n",
      "\tTraining Loss: 0.753771\n",
      "\tTesting Loss: 0.790943\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1410/10000000000000]\n",
      "\tTraining Loss: 0.753748\n",
      "\tTesting Loss: 0.790357\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1411/10000000000000]\n",
      "\tTraining Loss: 0.753721\n",
      "\tTesting Loss: 0.791379\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1412/10000000000000]\n",
      "\tTraining Loss: 0.753789\n",
      "\tTesting Loss: 0.790421\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1413/10000000000000]\n",
      "\tTraining Loss: 0.753799\n",
      "\tTesting Loss: 0.790959\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1414/10000000000000]\n",
      "\tTraining Loss: 0.753706\n",
      "\tTesting Loss: 0.790368\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1415/10000000000000]\n",
      "\tTraining Loss: 0.753776\n",
      "\tTesting Loss: 0.790692\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1416/10000000000000]\n",
      "\tTraining Loss: 0.753736\n",
      "\tTesting Loss: 0.791288\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1417/10000000000000]\n",
      "\tTraining Loss: 0.753722\n",
      "\tTesting Loss: 0.791559\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1418/10000000000000]\n",
      "\tTraining Loss: 0.753773\n",
      "\tTesting Loss: 0.791342\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1419/10000000000000]\n",
      "\tTraining Loss: 0.753767\n",
      "\tTesting Loss: 0.792063\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1420/10000000000000]\n",
      "\tTraining Loss: 0.753718\n",
      "\tTesting Loss: 0.790928\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1421/10000000000000]\n",
      "\tTraining Loss: 0.753772\n",
      "\tTesting Loss: 0.791112\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1422/10000000000000]\n",
      "\tTraining Loss: 0.753763\n",
      "\tTesting Loss: 0.791181\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1423/10000000000000]\n",
      "\tTraining Loss: 0.753767\n",
      "\tTesting Loss: 0.791779\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1424/10000000000000]\n",
      "\tTraining Loss: 0.753831\n",
      "\tTesting Loss: 0.790455\n",
      "\tLearning Rate: 0.000000634\n",
      "Epoch [1425/10000000000000]\n",
      "\tTraining Loss: 0.753801\n",
      "\tTesting Loss: 0.791160\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1426/10000000000000]\n",
      "\tTraining Loss: 0.753738\n",
      "\tTesting Loss: 0.791363\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1427/10000000000000]\n",
      "\tTraining Loss: 0.753763\n",
      "\tTesting Loss: 0.791495\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1428/10000000000000]\n",
      "\tTraining Loss: 0.753726\n",
      "\tTesting Loss: 0.790307\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1429/10000000000000]\n",
      "\tTraining Loss: 0.753677\n",
      "\tTesting Loss: 0.791153\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1430/10000000000000]\n",
      "\tTraining Loss: 0.753773\n",
      "\tTesting Loss: 0.790282\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1431/10000000000000]\n",
      "\tTraining Loss: 0.753711\n",
      "\tTesting Loss: 0.791292\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1432/10000000000000]\n",
      "\tTraining Loss: 0.753763\n",
      "\tTesting Loss: 0.791230\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1433/10000000000000]\n",
      "\tTraining Loss: 0.753749\n",
      "\tTesting Loss: 0.789882\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1434/10000000000000]\n",
      "\tTraining Loss: 0.753742\n",
      "\tTesting Loss: 0.791006\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1435/10000000000000]\n",
      "\tTraining Loss: 0.753729\n",
      "\tTesting Loss: 0.791077\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1436/10000000000000]\n",
      "\tTraining Loss: 0.753739\n",
      "\tTesting Loss: 0.790555\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1437/10000000000000]\n",
      "\tTraining Loss: 0.753710\n",
      "\tTesting Loss: 0.789791\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1438/10000000000000]\n",
      "\tTraining Loss: 0.753760\n",
      "\tTesting Loss: 0.791795\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1439/10000000000000]\n",
      "\tTraining Loss: 0.753729\n",
      "\tTesting Loss: 0.790502\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1440/10000000000000]\n",
      "\tTraining Loss: 0.753713\n",
      "\tTesting Loss: 0.790900\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1441/10000000000000]\n",
      "\tTraining Loss: 0.753756\n",
      "\tTesting Loss: 0.791032\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1442/10000000000000]\n",
      "\tTraining Loss: 0.753763\n",
      "\tTesting Loss: 0.790183\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1443/10000000000000]\n",
      "\tTraining Loss: 0.753729\n",
      "\tTesting Loss: 0.790924\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1444/10000000000000]\n",
      "\tTraining Loss: 0.753763\n",
      "\tTesting Loss: 0.790597\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1445/10000000000000]\n",
      "\tTraining Loss: 0.753724\n",
      "\tTesting Loss: 0.791833\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1446/10000000000000]\n",
      "\tTraining Loss: 0.753725\n",
      "\tTesting Loss: 0.791136\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1447/10000000000000]\n",
      "\tTraining Loss: 0.753766\n",
      "\tTesting Loss: 0.791305\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1448/10000000000000]\n",
      "\tTraining Loss: 0.753739\n",
      "\tTesting Loss: 0.790607\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1449/10000000000000]\n",
      "\tTraining Loss: 0.753742\n",
      "\tTesting Loss: 0.790487\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1450/10000000000000]\n",
      "\tTraining Loss: 0.753744\n",
      "\tTesting Loss: 0.790333\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1451/10000000000000]\n",
      "\tTraining Loss: 0.753774\n",
      "\tTesting Loss: 0.791419\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1452/10000000000000]\n",
      "\tTraining Loss: 0.753719\n",
      "\tTesting Loss: 0.790748\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1453/10000000000000]\n",
      "\tTraining Loss: 0.753702\n",
      "\tTesting Loss: 0.791651\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1454/10000000000000]\n",
      "\tTraining Loss: 0.753762\n",
      "\tTesting Loss: 0.790924\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1455/10000000000000]\n",
      "\tTraining Loss: 0.753751\n",
      "\tTesting Loss: 0.791201\n",
      "\tLearning Rate: 0.000000507\n",
      "Epoch [1456/10000000000000]\n",
      "\tTraining Loss: 0.753719\n",
      "\tTesting Loss: 0.791530\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1457/10000000000000]\n",
      "\tTraining Loss: 0.753741\n",
      "\tTesting Loss: 0.790387\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1458/10000000000000]\n",
      "\tTraining Loss: 0.753742\n",
      "\tTesting Loss: 0.790879\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1459/10000000000000]\n",
      "\tTraining Loss: 0.753662\n",
      "\tTesting Loss: 0.791326\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1460/10000000000000]\n",
      "\tTraining Loss: 0.753708\n",
      "\tTesting Loss: 0.790235\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1461/10000000000000]\n",
      "\tTraining Loss: 0.753724\n",
      "\tTesting Loss: 0.790366\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1462/10000000000000]\n",
      "\tTraining Loss: 0.753687\n",
      "\tTesting Loss: 0.791036\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1463/10000000000000]\n",
      "\tTraining Loss: 0.753735\n",
      "\tTesting Loss: 0.791798\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1464/10000000000000]\n",
      "\tTraining Loss: 0.753744\n",
      "\tTesting Loss: 0.790499\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1465/10000000000000]\n",
      "\tTraining Loss: 0.753795\n",
      "\tTesting Loss: 0.791921\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1466/10000000000000]\n",
      "\tTraining Loss: 0.753699\n",
      "\tTesting Loss: 0.790794\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1467/10000000000000]\n",
      "\tTraining Loss: 0.753741\n",
      "\tTesting Loss: 0.790692\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1468/10000000000000]\n",
      "\tTraining Loss: 0.753774\n",
      "\tTesting Loss: 0.790224\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1469/10000000000000]\n",
      "\tTraining Loss: 0.753735\n",
      "\tTesting Loss: 0.790395\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1470/10000000000000]\n",
      "\tTraining Loss: 0.753745\n",
      "\tTesting Loss: 0.790896\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1471/10000000000000]\n",
      "\tTraining Loss: 0.753719\n",
      "\tTesting Loss: 0.790339\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1472/10000000000000]\n",
      "\tTraining Loss: 0.753714\n",
      "\tTesting Loss: 0.790948\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1473/10000000000000]\n",
      "\tTraining Loss: 0.753687\n",
      "\tTesting Loss: 0.790916\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1474/10000000000000]\n",
      "\tTraining Loss: 0.753691\n",
      "\tTesting Loss: 0.790782\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1475/10000000000000]\n",
      "\tTraining Loss: 0.753697\n",
      "\tTesting Loss: 0.791347\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1476/10000000000000]\n",
      "\tTraining Loss: 0.753725\n",
      "\tTesting Loss: 0.791450\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1477/10000000000000]\n",
      "\tTraining Loss: 0.753704\n",
      "\tTesting Loss: 0.791703\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1478/10000000000000]\n",
      "\tTraining Loss: 0.753717\n",
      "\tTesting Loss: 0.790978\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1479/10000000000000]\n",
      "\tTraining Loss: 0.753777\n",
      "\tTesting Loss: 0.791440\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1480/10000000000000]\n",
      "\tTraining Loss: 0.753736\n",
      "\tTesting Loss: 0.791176\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1481/10000000000000]\n",
      "\tTraining Loss: 0.753710\n",
      "\tTesting Loss: 0.790669\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1482/10000000000000]\n",
      "\tTraining Loss: 0.753708\n",
      "\tTesting Loss: 0.791992\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1483/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.791179\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1484/10000000000000]\n",
      "\tTraining Loss: 0.753729\n",
      "\tTesting Loss: 0.790563\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1485/10000000000000]\n",
      "\tTraining Loss: 0.753633\n",
      "\tTesting Loss: 0.791824\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1486/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.791022\n",
      "\tLearning Rate: 0.000000406\n",
      "Epoch [1487/10000000000000]\n",
      "\tTraining Loss: 0.753762\n",
      "\tTesting Loss: 0.790623\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1488/10000000000000]\n",
      "\tTraining Loss: 0.753681\n",
      "\tTesting Loss: 0.790195\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1489/10000000000000]\n",
      "\tTraining Loss: 0.753728\n",
      "\tTesting Loss: 0.790663\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1490/10000000000000]\n",
      "\tTraining Loss: 0.753715\n",
      "\tTesting Loss: 0.790748\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1491/10000000000000]\n",
      "\tTraining Loss: 0.753748\n",
      "\tTesting Loss: 0.791026\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1492/10000000000000]\n",
      "\tTraining Loss: 0.753662\n",
      "\tTesting Loss: 0.791658\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1493/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.790354\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1494/10000000000000]\n",
      "\tTraining Loss: 0.753707\n",
      "\tTesting Loss: 0.790664\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1495/10000000000000]\n",
      "\tTraining Loss: 0.753662\n",
      "\tTesting Loss: 0.791248\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1496/10000000000000]\n",
      "\tTraining Loss: 0.753725\n",
      "\tTesting Loss: 0.791268\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1497/10000000000000]\n",
      "\tTraining Loss: 0.753697\n",
      "\tTesting Loss: 0.791202\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1498/10000000000000]\n",
      "\tTraining Loss: 0.753699\n",
      "\tTesting Loss: 0.790653\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1499/10000000000000]\n",
      "\tTraining Loss: 0.753751\n",
      "\tTesting Loss: 0.791611\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1500/10000000000000]\n",
      "\tTraining Loss: 0.753740\n",
      "\tTesting Loss: 0.791607\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1501/10000000000000]\n",
      "\tTraining Loss: 0.753782\n",
      "\tTesting Loss: 0.790163\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1502/10000000000000]\n",
      "\tTraining Loss: 0.753726\n",
      "\tTesting Loss: 0.791543\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1503/10000000000000]\n",
      "\tTraining Loss: 0.753731\n",
      "\tTesting Loss: 0.791265\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1504/10000000000000]\n",
      "\tTraining Loss: 0.753728\n",
      "\tTesting Loss: 0.791097\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1505/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.790285\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1506/10000000000000]\n",
      "\tTraining Loss: 0.753738\n",
      "\tTesting Loss: 0.790859\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1507/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.790021\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1508/10000000000000]\n",
      "\tTraining Loss: 0.753743\n",
      "\tTesting Loss: 0.791094\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1509/10000000000000]\n",
      "\tTraining Loss: 0.753756\n",
      "\tTesting Loss: 0.791340\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1510/10000000000000]\n",
      "\tTraining Loss: 0.753751\n",
      "\tTesting Loss: 0.791074\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1511/10000000000000]\n",
      "\tTraining Loss: 0.753719\n",
      "\tTesting Loss: 0.790894\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1512/10000000000000]\n",
      "\tTraining Loss: 0.753817\n",
      "\tTesting Loss: 0.790986\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1513/10000000000000]\n",
      "\tTraining Loss: 0.753690\n",
      "\tTesting Loss: 0.790973\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1514/10000000000000]\n",
      "\tTraining Loss: 0.753727\n",
      "\tTesting Loss: 0.790501\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1515/10000000000000]\n",
      "\tTraining Loss: 0.753773\n",
      "\tTesting Loss: 0.791887\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1516/10000000000000]\n",
      "\tTraining Loss: 0.753664\n",
      "\tTesting Loss: 0.791156\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1517/10000000000000]\n",
      "\tTraining Loss: 0.753718\n",
      "\tTesting Loss: 0.791031\n",
      "\tLearning Rate: 0.000000325\n",
      "Epoch [1518/10000000000000]\n",
      "\tTraining Loss: 0.753706\n",
      "\tTesting Loss: 0.791302\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1519/10000000000000]\n",
      "\tTraining Loss: 0.753719\n",
      "\tTesting Loss: 0.790571\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1520/10000000000000]\n",
      "\tTraining Loss: 0.753711\n",
      "\tTesting Loss: 0.790960\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1521/10000000000000]\n",
      "\tTraining Loss: 0.753700\n",
      "\tTesting Loss: 0.791597\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1522/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.790630\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1523/10000000000000]\n",
      "\tTraining Loss: 0.753684\n",
      "\tTesting Loss: 0.790834\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1524/10000000000000]\n",
      "\tTraining Loss: 0.753723\n",
      "\tTesting Loss: 0.790633\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1525/10000000000000]\n",
      "\tTraining Loss: 0.753713\n",
      "\tTesting Loss: 0.791545\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1526/10000000000000]\n",
      "\tTraining Loss: 0.753746\n",
      "\tTesting Loss: 0.790103\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1527/10000000000000]\n",
      "\tTraining Loss: 0.753699\n",
      "\tTesting Loss: 0.791372\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1528/10000000000000]\n",
      "\tTraining Loss: 0.753680\n",
      "\tTesting Loss: 0.792522\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1529/10000000000000]\n",
      "\tTraining Loss: 0.753717\n",
      "\tTesting Loss: 0.790572\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1530/10000000000000]\n",
      "\tTraining Loss: 0.753704\n",
      "\tTesting Loss: 0.790419\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1531/10000000000000]\n",
      "\tTraining Loss: 0.753704\n",
      "\tTesting Loss: 0.791065\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1532/10000000000000]\n",
      "\tTraining Loss: 0.753732\n",
      "\tTesting Loss: 0.790653\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1533/10000000000000]\n",
      "\tTraining Loss: 0.753680\n",
      "\tTesting Loss: 0.790943\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1534/10000000000000]\n",
      "\tTraining Loss: 0.753704\n",
      "\tTesting Loss: 0.791289\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1535/10000000000000]\n",
      "\tTraining Loss: 0.753730\n",
      "\tTesting Loss: 0.790074\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1536/10000000000000]\n",
      "\tTraining Loss: 0.753730\n",
      "\tTesting Loss: 0.790947\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1537/10000000000000]\n",
      "\tTraining Loss: 0.753730\n",
      "\tTesting Loss: 0.790405\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1538/10000000000000]\n",
      "\tTraining Loss: 0.753731\n",
      "\tTesting Loss: 0.791384\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1539/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.791270\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1540/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.791710\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1541/10000000000000]\n",
      "\tTraining Loss: 0.753693\n",
      "\tTesting Loss: 0.791588\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1542/10000000000000]\n",
      "\tTraining Loss: 0.753696\n",
      "\tTesting Loss: 0.791391\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1543/10000000000000]\n",
      "\tTraining Loss: 0.753754\n",
      "\tTesting Loss: 0.791559\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1544/10000000000000]\n",
      "\tTraining Loss: 0.753653\n",
      "\tTesting Loss: 0.791476\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1545/10000000000000]\n",
      "\tTraining Loss: 0.753695\n",
      "\tTesting Loss: 0.790788\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1546/10000000000000]\n",
      "\tTraining Loss: 0.753657\n",
      "\tTesting Loss: 0.791787\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1547/10000000000000]\n",
      "\tTraining Loss: 0.753712\n",
      "\tTesting Loss: 0.790930\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1548/10000000000000]\n",
      "\tTraining Loss: 0.753737\n",
      "\tTesting Loss: 0.791558\n",
      "\tLearning Rate: 0.000000260\n",
      "Epoch [1549/10000000000000]\n",
      "\tTraining Loss: 0.753734\n",
      "\tTesting Loss: 0.789878\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1550/10000000000000]\n",
      "\tTraining Loss: 0.753714\n",
      "\tTesting Loss: 0.791101\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1551/10000000000000]\n",
      "\tTraining Loss: 0.753770\n",
      "\tTesting Loss: 0.791107\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1552/10000000000000]\n",
      "\tTraining Loss: 0.753713\n",
      "\tTesting Loss: 0.789979\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1553/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.791195\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1554/10000000000000]\n",
      "\tTraining Loss: 0.753654\n",
      "\tTesting Loss: 0.790250\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1555/10000000000000]\n",
      "\tTraining Loss: 0.753684\n",
      "\tTesting Loss: 0.792013\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1556/10000000000000]\n",
      "\tTraining Loss: 0.753789\n",
      "\tTesting Loss: 0.790733\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1557/10000000000000]\n",
      "\tTraining Loss: 0.753718\n",
      "\tTesting Loss: 0.791714\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1558/10000000000000]\n",
      "\tTraining Loss: 0.753767\n",
      "\tTesting Loss: 0.790714\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1559/10000000000000]\n",
      "\tTraining Loss: 0.753691\n",
      "\tTesting Loss: 0.790468\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1560/10000000000000]\n",
      "\tTraining Loss: 0.753725\n",
      "\tTesting Loss: 0.791525\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1561/10000000000000]\n",
      "\tTraining Loss: 0.753687\n",
      "\tTesting Loss: 0.790830\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1562/10000000000000]\n",
      "\tTraining Loss: 0.753686\n",
      "\tTesting Loss: 0.792046\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1563/10000000000000]\n",
      "\tTraining Loss: 0.753685\n",
      "\tTesting Loss: 0.790393\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1564/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.790625\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1565/10000000000000]\n",
      "\tTraining Loss: 0.753679\n",
      "\tTesting Loss: 0.791418\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1566/10000000000000]\n",
      "\tTraining Loss: 0.753712\n",
      "\tTesting Loss: 0.789905\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1567/10000000000000]\n",
      "\tTraining Loss: 0.753749\n",
      "\tTesting Loss: 0.790856\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1568/10000000000000]\n",
      "\tTraining Loss: 0.753687\n",
      "\tTesting Loss: 0.790674\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1569/10000000000000]\n",
      "\tTraining Loss: 0.753713\n",
      "\tTesting Loss: 0.791315\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1570/10000000000000]\n",
      "\tTraining Loss: 0.753670\n",
      "\tTesting Loss: 0.791100\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1571/10000000000000]\n",
      "\tTraining Loss: 0.753700\n",
      "\tTesting Loss: 0.791452\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1572/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.790415\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1573/10000000000000]\n",
      "\tTraining Loss: 0.753668\n",
      "\tTesting Loss: 0.790707\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1574/10000000000000]\n",
      "\tTraining Loss: 0.753729\n",
      "\tTesting Loss: 0.791623\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1575/10000000000000]\n",
      "\tTraining Loss: 0.753720\n",
      "\tTesting Loss: 0.791646\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1576/10000000000000]\n",
      "\tTraining Loss: 0.753695\n",
      "\tTesting Loss: 0.790694\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1577/10000000000000]\n",
      "\tTraining Loss: 0.753751\n",
      "\tTesting Loss: 0.790264\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1578/10000000000000]\n",
      "\tTraining Loss: 0.753686\n",
      "\tTesting Loss: 0.790786\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1579/10000000000000]\n",
      "\tTraining Loss: 0.753623\n",
      "\tTesting Loss: 0.791446\n",
      "\tLearning Rate: 0.000000208\n",
      "Epoch [1580/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.791097\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1581/10000000000000]\n",
      "\tTraining Loss: 0.753700\n",
      "\tTesting Loss: 0.790742\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1582/10000000000000]\n",
      "\tTraining Loss: 0.753714\n",
      "\tTesting Loss: 0.790883\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1583/10000000000000]\n",
      "\tTraining Loss: 0.753758\n",
      "\tTesting Loss: 0.791267\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1584/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.790630\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1585/10000000000000]\n",
      "\tTraining Loss: 0.753706\n",
      "\tTesting Loss: 0.790709\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1586/10000000000000]\n",
      "\tTraining Loss: 0.753753\n",
      "\tTesting Loss: 0.790666\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1587/10000000000000]\n",
      "\tTraining Loss: 0.753691\n",
      "\tTesting Loss: 0.790480\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1588/10000000000000]\n",
      "\tTraining Loss: 0.753764\n",
      "\tTesting Loss: 0.791573\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1589/10000000000000]\n",
      "\tTraining Loss: 0.753723\n",
      "\tTesting Loss: 0.790127\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1590/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.791852\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1591/10000000000000]\n",
      "\tTraining Loss: 0.753723\n",
      "\tTesting Loss: 0.790997\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1592/10000000000000]\n",
      "\tTraining Loss: 0.753726\n",
      "\tTesting Loss: 0.790523\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1593/10000000000000]\n",
      "\tTraining Loss: 0.753710\n",
      "\tTesting Loss: 0.789884\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1594/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.790610\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1595/10000000000000]\n",
      "\tTraining Loss: 0.753729\n",
      "\tTesting Loss: 0.791663\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1596/10000000000000]\n",
      "\tTraining Loss: 0.753666\n",
      "\tTesting Loss: 0.791382\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1597/10000000000000]\n",
      "\tTraining Loss: 0.753710\n",
      "\tTesting Loss: 0.790856\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1598/10000000000000]\n",
      "\tTraining Loss: 0.753740\n",
      "\tTesting Loss: 0.791871\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1599/10000000000000]\n",
      "\tTraining Loss: 0.753694\n",
      "\tTesting Loss: 0.791307\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1600/10000000000000]\n",
      "\tTraining Loss: 0.753699\n",
      "\tTesting Loss: 0.791176\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1601/10000000000000]\n",
      "\tTraining Loss: 0.753687\n",
      "\tTesting Loss: 0.791196\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1602/10000000000000]\n",
      "\tTraining Loss: 0.753676\n",
      "\tTesting Loss: 0.790933\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1603/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.790811\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1604/10000000000000]\n",
      "\tTraining Loss: 0.753746\n",
      "\tTesting Loss: 0.790958\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1605/10000000000000]\n",
      "\tTraining Loss: 0.753703\n",
      "\tTesting Loss: 0.790351\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1606/10000000000000]\n",
      "\tTraining Loss: 0.753695\n",
      "\tTesting Loss: 0.791730\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1607/10000000000000]\n",
      "\tTraining Loss: 0.753715\n",
      "\tTesting Loss: 0.790710\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1608/10000000000000]\n",
      "\tTraining Loss: 0.753681\n",
      "\tTesting Loss: 0.791557\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1609/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.791210\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1610/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.791678\n",
      "\tLearning Rate: 0.000000166\n",
      "Epoch [1611/10000000000000]\n",
      "\tTraining Loss: 0.753638\n",
      "\tTesting Loss: 0.791280\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1612/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.790503\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1613/10000000000000]\n",
      "\tTraining Loss: 0.753701\n",
      "\tTesting Loss: 0.791144\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1614/10000000000000]\n",
      "\tTraining Loss: 0.753772\n",
      "\tTesting Loss: 0.790987\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1615/10000000000000]\n",
      "\tTraining Loss: 0.753706\n",
      "\tTesting Loss: 0.789615\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1616/10000000000000]\n",
      "\tTraining Loss: 0.753708\n",
      "\tTesting Loss: 0.791337\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1617/10000000000000]\n",
      "\tTraining Loss: 0.753709\n",
      "\tTesting Loss: 0.791170\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1618/10000000000000]\n",
      "\tTraining Loss: 0.753644\n",
      "\tTesting Loss: 0.790934\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1619/10000000000000]\n",
      "\tTraining Loss: 0.753690\n",
      "\tTesting Loss: 0.791581\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1620/10000000000000]\n",
      "\tTraining Loss: 0.753661\n",
      "\tTesting Loss: 0.790889\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1621/10000000000000]\n",
      "\tTraining Loss: 0.753741\n",
      "\tTesting Loss: 0.791430\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1622/10000000000000]\n",
      "\tTraining Loss: 0.753728\n",
      "\tTesting Loss: 0.791339\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1623/10000000000000]\n",
      "\tTraining Loss: 0.753686\n",
      "\tTesting Loss: 0.791384\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1624/10000000000000]\n",
      "\tTraining Loss: 0.753609\n",
      "\tTesting Loss: 0.791405\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1625/10000000000000]\n",
      "\tTraining Loss: 0.753648\n",
      "\tTesting Loss: 0.791205\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1626/10000000000000]\n",
      "\tTraining Loss: 0.753646\n",
      "\tTesting Loss: 0.791187\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1627/10000000000000]\n",
      "\tTraining Loss: 0.753654\n",
      "\tTesting Loss: 0.791148\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1628/10000000000000]\n",
      "\tTraining Loss: 0.753701\n",
      "\tTesting Loss: 0.790590\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1629/10000000000000]\n",
      "\tTraining Loss: 0.753700\n",
      "\tTesting Loss: 0.790887\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1630/10000000000000]\n",
      "\tTraining Loss: 0.753758\n",
      "\tTesting Loss: 0.791999\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1631/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.791488\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1632/10000000000000]\n",
      "\tTraining Loss: 0.753664\n",
      "\tTesting Loss: 0.791310\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1633/10000000000000]\n",
      "\tTraining Loss: 0.753696\n",
      "\tTesting Loss: 0.791013\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1634/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.791542\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1635/10000000000000]\n",
      "\tTraining Loss: 0.753722\n",
      "\tTesting Loss: 0.790918\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1636/10000000000000]\n",
      "\tTraining Loss: 0.753662\n",
      "\tTesting Loss: 0.791302\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1637/10000000000000]\n",
      "\tTraining Loss: 0.753709\n",
      "\tTesting Loss: 0.791426\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1638/10000000000000]\n",
      "\tTraining Loss: 0.753764\n",
      "\tTesting Loss: 0.791702\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1639/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.790216\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1640/10000000000000]\n",
      "\tTraining Loss: 0.753697\n",
      "\tTesting Loss: 0.790795\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1641/10000000000000]\n",
      "\tTraining Loss: 0.753686\n",
      "\tTesting Loss: 0.791632\n",
      "\tLearning Rate: 0.000000133\n",
      "Epoch [1642/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.790777\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1643/10000000000000]\n",
      "\tTraining Loss: 0.753685\n",
      "\tTesting Loss: 0.791953\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1644/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.791033\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1645/10000000000000]\n",
      "\tTraining Loss: 0.753747\n",
      "\tTesting Loss: 0.790956\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1646/10000000000000]\n",
      "\tTraining Loss: 0.753679\n",
      "\tTesting Loss: 0.790634\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1647/10000000000000]\n",
      "\tTraining Loss: 0.753737\n",
      "\tTesting Loss: 0.791722\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1648/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.791426\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1649/10000000000000]\n",
      "\tTraining Loss: 0.753691\n",
      "\tTesting Loss: 0.790517\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1650/10000000000000]\n",
      "\tTraining Loss: 0.753697\n",
      "\tTesting Loss: 0.791494\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1651/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.791537\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1652/10000000000000]\n",
      "\tTraining Loss: 0.753727\n",
      "\tTesting Loss: 0.789659\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1653/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.790792\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1654/10000000000000]\n",
      "\tTraining Loss: 0.753655\n",
      "\tTesting Loss: 0.790626\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1655/10000000000000]\n",
      "\tTraining Loss: 0.753707\n",
      "\tTesting Loss: 0.790997\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1656/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.789867\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1657/10000000000000]\n",
      "\tTraining Loss: 0.753717\n",
      "\tTesting Loss: 0.790406\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1658/10000000000000]\n",
      "\tTraining Loss: 0.753626\n",
      "\tTesting Loss: 0.791613\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1659/10000000000000]\n",
      "\tTraining Loss: 0.753718\n",
      "\tTesting Loss: 0.791567\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1660/10000000000000]\n",
      "\tTraining Loss: 0.753757\n",
      "\tTesting Loss: 0.790987\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1661/10000000000000]\n",
      "\tTraining Loss: 0.753701\n",
      "\tTesting Loss: 0.790549\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1662/10000000000000]\n",
      "\tTraining Loss: 0.753715\n",
      "\tTesting Loss: 0.790892\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1663/10000000000000]\n",
      "\tTraining Loss: 0.753695\n",
      "\tTesting Loss: 0.790345\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1664/10000000000000]\n",
      "\tTraining Loss: 0.753703\n",
      "\tTesting Loss: 0.791767\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1665/10000000000000]\n",
      "\tTraining Loss: 0.753641\n",
      "\tTesting Loss: 0.789720\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1666/10000000000000]\n",
      "\tTraining Loss: 0.753703\n",
      "\tTesting Loss: 0.790492\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1667/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.790238\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1668/10000000000000]\n",
      "\tTraining Loss: 0.753765\n",
      "\tTesting Loss: 0.790798\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1669/10000000000000]\n",
      "\tTraining Loss: 0.753746\n",
      "\tTesting Loss: 0.790773\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1670/10000000000000]\n",
      "\tTraining Loss: 0.753713\n",
      "\tTesting Loss: 0.791366\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1671/10000000000000]\n",
      "\tTraining Loss: 0.753646\n",
      "\tTesting Loss: 0.790600\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1672/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.791122\n",
      "\tLearning Rate: 0.000000106\n",
      "Epoch [1673/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.790704\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1674/10000000000000]\n",
      "\tTraining Loss: 0.753732\n",
      "\tTesting Loss: 0.790260\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1675/10000000000000]\n",
      "\tTraining Loss: 0.753669\n",
      "\tTesting Loss: 0.791525\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1676/10000000000000]\n",
      "\tTraining Loss: 0.753720\n",
      "\tTesting Loss: 0.791225\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1677/10000000000000]\n",
      "\tTraining Loss: 0.753703\n",
      "\tTesting Loss: 0.791334\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1678/10000000000000]\n",
      "\tTraining Loss: 0.753660\n",
      "\tTesting Loss: 0.791244\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1679/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.790589\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1680/10000000000000]\n",
      "\tTraining Loss: 0.753705\n",
      "\tTesting Loss: 0.791392\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1681/10000000000000]\n",
      "\tTraining Loss: 0.753648\n",
      "\tTesting Loss: 0.791214\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1682/10000000000000]\n",
      "\tTraining Loss: 0.753643\n",
      "\tTesting Loss: 0.790409\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1683/10000000000000]\n",
      "\tTraining Loss: 0.753710\n",
      "\tTesting Loss: 0.790547\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1684/10000000000000]\n",
      "\tTraining Loss: 0.753703\n",
      "\tTesting Loss: 0.791195\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1685/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.790501\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1686/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.791262\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1687/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.791250\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1688/10000000000000]\n",
      "\tTraining Loss: 0.753751\n",
      "\tTesting Loss: 0.790988\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1689/10000000000000]\n",
      "\tTraining Loss: 0.753695\n",
      "\tTesting Loss: 0.791311\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1690/10000000000000]\n",
      "\tTraining Loss: 0.753694\n",
      "\tTesting Loss: 0.790975\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1691/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.792085\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1692/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.791516\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1693/10000000000000]\n",
      "\tTraining Loss: 0.753676\n",
      "\tTesting Loss: 0.790928\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1694/10000000000000]\n",
      "\tTraining Loss: 0.753668\n",
      "\tTesting Loss: 0.791035\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1695/10000000000000]\n",
      "\tTraining Loss: 0.753730\n",
      "\tTesting Loss: 0.791277\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1696/10000000000000]\n",
      "\tTraining Loss: 0.753594\n",
      "\tTesting Loss: 0.791673\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1697/10000000000000]\n",
      "\tTraining Loss: 0.753720\n",
      "\tTesting Loss: 0.792271\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1698/10000000000000]\n",
      "\tTraining Loss: 0.753697\n",
      "\tTesting Loss: 0.790423\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1699/10000000000000]\n",
      "\tTraining Loss: 0.753709\n",
      "\tTesting Loss: 0.790948\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1700/10000000000000]\n",
      "\tTraining Loss: 0.753648\n",
      "\tTesting Loss: 0.791352\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1701/10000000000000]\n",
      "\tTraining Loss: 0.753704\n",
      "\tTesting Loss: 0.790393\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1702/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.791483\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1703/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.791387\n",
      "\tLearning Rate: 0.000000085\n",
      "Epoch [1704/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.790394\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1705/10000000000000]\n",
      "\tTraining Loss: 0.753634\n",
      "\tTesting Loss: 0.791438\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1706/10000000000000]\n",
      "\tTraining Loss: 0.753664\n",
      "\tTesting Loss: 0.790626\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1707/10000000000000]\n",
      "\tTraining Loss: 0.753634\n",
      "\tTesting Loss: 0.791687\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1708/10000000000000]\n",
      "\tTraining Loss: 0.753712\n",
      "\tTesting Loss: 0.791563\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1709/10000000000000]\n",
      "\tTraining Loss: 0.753718\n",
      "\tTesting Loss: 0.791503\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1710/10000000000000]\n",
      "\tTraining Loss: 0.753684\n",
      "\tTesting Loss: 0.789818\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1711/10000000000000]\n",
      "\tTraining Loss: 0.753735\n",
      "\tTesting Loss: 0.791030\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1712/10000000000000]\n",
      "\tTraining Loss: 0.753696\n",
      "\tTesting Loss: 0.790549\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1713/10000000000000]\n",
      "\tTraining Loss: 0.753705\n",
      "\tTesting Loss: 0.790630\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1714/10000000000000]\n",
      "\tTraining Loss: 0.753712\n",
      "\tTesting Loss: 0.790525\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1715/10000000000000]\n",
      "\tTraining Loss: 0.753677\n",
      "\tTesting Loss: 0.790759\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1716/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.790852\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1717/10000000000000]\n",
      "\tTraining Loss: 0.753731\n",
      "\tTesting Loss: 0.791716\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1718/10000000000000]\n",
      "\tTraining Loss: 0.753701\n",
      "\tTesting Loss: 0.790738\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1719/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.790710\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1720/10000000000000]\n",
      "\tTraining Loss: 0.753693\n",
      "\tTesting Loss: 0.791700\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1721/10000000000000]\n",
      "\tTraining Loss: 0.753679\n",
      "\tTesting Loss: 0.790921\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1722/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.791514\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1723/10000000000000]\n",
      "\tTraining Loss: 0.753676\n",
      "\tTesting Loss: 0.791017\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1724/10000000000000]\n",
      "\tTraining Loss: 0.753622\n",
      "\tTesting Loss: 0.791766\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1725/10000000000000]\n",
      "\tTraining Loss: 0.753608\n",
      "\tTesting Loss: 0.791009\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1726/10000000000000]\n",
      "\tTraining Loss: 0.753697\n",
      "\tTesting Loss: 0.791226\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1727/10000000000000]\n",
      "\tTraining Loss: 0.753729\n",
      "\tTesting Loss: 0.790129\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1728/10000000000000]\n",
      "\tTraining Loss: 0.753690\n",
      "\tTesting Loss: 0.790790\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1729/10000000000000]\n",
      "\tTraining Loss: 0.753669\n",
      "\tTesting Loss: 0.791006\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1730/10000000000000]\n",
      "\tTraining Loss: 0.753718\n",
      "\tTesting Loss: 0.791453\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1731/10000000000000]\n",
      "\tTraining Loss: 0.753664\n",
      "\tTesting Loss: 0.791331\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1732/10000000000000]\n",
      "\tTraining Loss: 0.753710\n",
      "\tTesting Loss: 0.791548\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1733/10000000000000]\n",
      "\tTraining Loss: 0.753670\n",
      "\tTesting Loss: 0.791075\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1734/10000000000000]\n",
      "\tTraining Loss: 0.753717\n",
      "\tTesting Loss: 0.792081\n",
      "\tLearning Rate: 0.000000068\n",
      "Epoch [1735/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.791135\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1736/10000000000000]\n",
      "\tTraining Loss: 0.753718\n",
      "\tTesting Loss: 0.791238\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1737/10000000000000]\n",
      "\tTraining Loss: 0.753612\n",
      "\tTesting Loss: 0.790301\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1738/10000000000000]\n",
      "\tTraining Loss: 0.753661\n",
      "\tTesting Loss: 0.791538\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1739/10000000000000]\n",
      "\tTraining Loss: 0.753696\n",
      "\tTesting Loss: 0.791628\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1740/10000000000000]\n",
      "\tTraining Loss: 0.753694\n",
      "\tTesting Loss: 0.791238\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1741/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.790765\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1742/10000000000000]\n",
      "\tTraining Loss: 0.753686\n",
      "\tTesting Loss: 0.791017\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1743/10000000000000]\n",
      "\tTraining Loss: 0.753622\n",
      "\tTesting Loss: 0.791660\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1744/10000000000000]\n",
      "\tTraining Loss: 0.753703\n",
      "\tTesting Loss: 0.790016\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1745/10000000000000]\n",
      "\tTraining Loss: 0.753603\n",
      "\tTesting Loss: 0.791278\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1746/10000000000000]\n",
      "\tTraining Loss: 0.753708\n",
      "\tTesting Loss: 0.791412\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1747/10000000000000]\n",
      "\tTraining Loss: 0.753669\n",
      "\tTesting Loss: 0.791167\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1748/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.790648\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1749/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.790647\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1750/10000000000000]\n",
      "\tTraining Loss: 0.753721\n",
      "\tTesting Loss: 0.791101\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1751/10000000000000]\n",
      "\tTraining Loss: 0.753680\n",
      "\tTesting Loss: 0.791017\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1752/10000000000000]\n",
      "\tTraining Loss: 0.753696\n",
      "\tTesting Loss: 0.790490\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1753/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.791975\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1754/10000000000000]\n",
      "\tTraining Loss: 0.753689\n",
      "\tTesting Loss: 0.791014\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1755/10000000000000]\n",
      "\tTraining Loss: 0.753622\n",
      "\tTesting Loss: 0.791554\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1756/10000000000000]\n",
      "\tTraining Loss: 0.753679\n",
      "\tTesting Loss: 0.790083\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1757/10000000000000]\n",
      "\tTraining Loss: 0.753689\n",
      "\tTesting Loss: 0.790933\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1758/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.789847\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1759/10000000000000]\n",
      "\tTraining Loss: 0.753719\n",
      "\tTesting Loss: 0.790666\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1760/10000000000000]\n",
      "\tTraining Loss: 0.753670\n",
      "\tTesting Loss: 0.791836\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1761/10000000000000]\n",
      "\tTraining Loss: 0.753706\n",
      "\tTesting Loss: 0.791263\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1762/10000000000000]\n",
      "\tTraining Loss: 0.753738\n",
      "\tTesting Loss: 0.790933\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1763/10000000000000]\n",
      "\tTraining Loss: 0.753648\n",
      "\tTesting Loss: 0.790967\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1764/10000000000000]\n",
      "\tTraining Loss: 0.753720\n",
      "\tTesting Loss: 0.791254\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1765/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.790449\n",
      "\tLearning Rate: 0.000000054\n",
      "Epoch [1766/10000000000000]\n",
      "\tTraining Loss: 0.753620\n",
      "\tTesting Loss: 0.791394\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1767/10000000000000]\n",
      "\tTraining Loss: 0.753724\n",
      "\tTesting Loss: 0.791293\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1768/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.791282\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1769/10000000000000]\n",
      "\tTraining Loss: 0.753618\n",
      "\tTesting Loss: 0.790622\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1770/10000000000000]\n",
      "\tTraining Loss: 0.753700\n",
      "\tTesting Loss: 0.790847\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1771/10000000000000]\n",
      "\tTraining Loss: 0.753669\n",
      "\tTesting Loss: 0.791057\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1772/10000000000000]\n",
      "\tTraining Loss: 0.753647\n",
      "\tTesting Loss: 0.791330\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1773/10000000000000]\n",
      "\tTraining Loss: 0.753725\n",
      "\tTesting Loss: 0.791479\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1774/10000000000000]\n",
      "\tTraining Loss: 0.753625\n",
      "\tTesting Loss: 0.791682\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1775/10000000000000]\n",
      "\tTraining Loss: 0.753666\n",
      "\tTesting Loss: 0.791165\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1776/10000000000000]\n",
      "\tTraining Loss: 0.753677\n",
      "\tTesting Loss: 0.791707\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1777/10000000000000]\n",
      "\tTraining Loss: 0.753681\n",
      "\tTesting Loss: 0.790473\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1778/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.790262\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1779/10000000000000]\n",
      "\tTraining Loss: 0.753607\n",
      "\tTesting Loss: 0.791621\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1780/10000000000000]\n",
      "\tTraining Loss: 0.753702\n",
      "\tTesting Loss: 0.791516\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1781/10000000000000]\n",
      "\tTraining Loss: 0.753666\n",
      "\tTesting Loss: 0.791167\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1782/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.791946\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1783/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.790207\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1784/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.791293\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1785/10000000000000]\n",
      "\tTraining Loss: 0.753694\n",
      "\tTesting Loss: 0.791309\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1786/10000000000000]\n",
      "\tTraining Loss: 0.753735\n",
      "\tTesting Loss: 0.790958\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1787/10000000000000]\n",
      "\tTraining Loss: 0.753756\n",
      "\tTesting Loss: 0.791191\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1788/10000000000000]\n",
      "\tTraining Loss: 0.753732\n",
      "\tTesting Loss: 0.790949\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1789/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.791543\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1790/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.791635\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1791/10000000000000]\n",
      "\tTraining Loss: 0.753721\n",
      "\tTesting Loss: 0.791422\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1792/10000000000000]\n",
      "\tTraining Loss: 0.753598\n",
      "\tTesting Loss: 0.791281\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1793/10000000000000]\n",
      "\tTraining Loss: 0.753727\n",
      "\tTesting Loss: 0.790981\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1794/10000000000000]\n",
      "\tTraining Loss: 0.753720\n",
      "\tTesting Loss: 0.790916\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1795/10000000000000]\n",
      "\tTraining Loss: 0.753706\n",
      "\tTesting Loss: 0.791496\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1796/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.791016\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1797/10000000000000]\n",
      "\tTraining Loss: 0.753655\n",
      "\tTesting Loss: 0.791174\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1798/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.791957\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1799/10000000000000]\n",
      "\tTraining Loss: 0.753668\n",
      "\tTesting Loss: 0.791123\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1800/10000000000000]\n",
      "\tTraining Loss: 0.753680\n",
      "\tTesting Loss: 0.791915\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1801/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.791080\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1802/10000000000000]\n",
      "\tTraining Loss: 0.753738\n",
      "\tTesting Loss: 0.791209\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1803/10000000000000]\n",
      "\tTraining Loss: 0.753608\n",
      "\tTesting Loss: 0.791122\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1804/10000000000000]\n",
      "\tTraining Loss: 0.753641\n",
      "\tTesting Loss: 0.791183\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1805/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.791083\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1806/10000000000000]\n",
      "\tTraining Loss: 0.753720\n",
      "\tTesting Loss: 0.791242\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1807/10000000000000]\n",
      "\tTraining Loss: 0.753636\n",
      "\tTesting Loss: 0.790118\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1808/10000000000000]\n",
      "\tTraining Loss: 0.753714\n",
      "\tTesting Loss: 0.791541\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1809/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.790679\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1810/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.791245\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1811/10000000000000]\n",
      "\tTraining Loss: 0.753685\n",
      "\tTesting Loss: 0.791252\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1812/10000000000000]\n",
      "\tTraining Loss: 0.753694\n",
      "\tTesting Loss: 0.790524\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1813/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.790751\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1814/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.791312\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1815/10000000000000]\n",
      "\tTraining Loss: 0.753637\n",
      "\tTesting Loss: 0.790949\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1816/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.792483\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1817/10000000000000]\n",
      "\tTraining Loss: 0.753684\n",
      "\tTesting Loss: 0.791431\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1818/10000000000000]\n",
      "\tTraining Loss: 0.753724\n",
      "\tTesting Loss: 0.790506\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1819/10000000000000]\n",
      "\tTraining Loss: 0.753645\n",
      "\tTesting Loss: 0.790570\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1820/10000000000000]\n",
      "\tTraining Loss: 0.753716\n",
      "\tTesting Loss: 0.790345\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1821/10000000000000]\n",
      "\tTraining Loss: 0.753734\n",
      "\tTesting Loss: 0.790416\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1822/10000000000000]\n",
      "\tTraining Loss: 0.753744\n",
      "\tTesting Loss: 0.791496\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1823/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.791346\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1824/10000000000000]\n",
      "\tTraining Loss: 0.753699\n",
      "\tTesting Loss: 0.790776\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1825/10000000000000]\n",
      "\tTraining Loss: 0.753647\n",
      "\tTesting Loss: 0.790594\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1826/10000000000000]\n",
      "\tTraining Loss: 0.753735\n",
      "\tTesting Loss: 0.790726\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1827/10000000000000]\n",
      "\tTraining Loss: 0.753589\n",
      "\tTesting Loss: 0.791360\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1828/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.790682\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1829/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.790236\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1830/10000000000000]\n",
      "\tTraining Loss: 0.753694\n",
      "\tTesting Loss: 0.790640\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1831/10000000000000]\n",
      "\tTraining Loss: 0.753714\n",
      "\tTesting Loss: 0.791561\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1832/10000000000000]\n",
      "\tTraining Loss: 0.753766\n",
      "\tTesting Loss: 0.791297\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1833/10000000000000]\n",
      "\tTraining Loss: 0.753664\n",
      "\tTesting Loss: 0.791164\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1834/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.790952\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1835/10000000000000]\n",
      "\tTraining Loss: 0.753677\n",
      "\tTesting Loss: 0.790792\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1836/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.790893\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1837/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.790717\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1838/10000000000000]\n",
      "\tTraining Loss: 0.753659\n",
      "\tTesting Loss: 0.790846\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1839/10000000000000]\n",
      "\tTraining Loss: 0.753739\n",
      "\tTesting Loss: 0.791434\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1840/10000000000000]\n",
      "\tTraining Loss: 0.753754\n",
      "\tTesting Loss: 0.790496\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1841/10000000000000]\n",
      "\tTraining Loss: 0.753652\n",
      "\tTesting Loss: 0.790340\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1842/10000000000000]\n",
      "\tTraining Loss: 0.753659\n",
      "\tTesting Loss: 0.791175\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1843/10000000000000]\n",
      "\tTraining Loss: 0.753713\n",
      "\tTesting Loss: 0.791733\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1844/10000000000000]\n",
      "\tTraining Loss: 0.753700\n",
      "\tTesting Loss: 0.791813\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1845/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.790710\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1846/10000000000000]\n",
      "\tTraining Loss: 0.753668\n",
      "\tTesting Loss: 0.790756\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1847/10000000000000]\n",
      "\tTraining Loss: 0.753654\n",
      "\tTesting Loss: 0.791031\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1848/10000000000000]\n",
      "\tTraining Loss: 0.753653\n",
      "\tTesting Loss: 0.790953\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1849/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.791252\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1850/10000000000000]\n",
      "\tTraining Loss: 0.753694\n",
      "\tTesting Loss: 0.790195\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1851/10000000000000]\n",
      "\tTraining Loss: 0.753670\n",
      "\tTesting Loss: 0.790579\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1852/10000000000000]\n",
      "\tTraining Loss: 0.753723\n",
      "\tTesting Loss: 0.791084\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1853/10000000000000]\n",
      "\tTraining Loss: 0.753694\n",
      "\tTesting Loss: 0.791690\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1854/10000000000000]\n",
      "\tTraining Loss: 0.753744\n",
      "\tTesting Loss: 0.791913\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1855/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.791835\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1856/10000000000000]\n",
      "\tTraining Loss: 0.753690\n",
      "\tTesting Loss: 0.791478\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1857/10000000000000]\n",
      "\tTraining Loss: 0.753674\n",
      "\tTesting Loss: 0.791208\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1858/10000000000000]\n",
      "\tTraining Loss: 0.753711\n",
      "\tTesting Loss: 0.791303\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1859/10000000000000]\n",
      "\tTraining Loss: 0.753602\n",
      "\tTesting Loss: 0.791174\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1860/10000000000000]\n",
      "\tTraining Loss: 0.753617\n",
      "\tTesting Loss: 0.790559\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1861/10000000000000]\n",
      "\tTraining Loss: 0.753685\n",
      "\tTesting Loss: 0.790904\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1862/10000000000000]\n",
      "\tTraining Loss: 0.753730\n",
      "\tTesting Loss: 0.790055\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1863/10000000000000]\n",
      "\tTraining Loss: 0.753694\n",
      "\tTesting Loss: 0.791359\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1864/10000000000000]\n",
      "\tTraining Loss: 0.753725\n",
      "\tTesting Loss: 0.790612\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1865/10000000000000]\n",
      "\tTraining Loss: 0.753623\n",
      "\tTesting Loss: 0.791308\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1866/10000000000000]\n",
      "\tTraining Loss: 0.753659\n",
      "\tTesting Loss: 0.790316\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1867/10000000000000]\n",
      "\tTraining Loss: 0.753709\n",
      "\tTesting Loss: 0.790738\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1868/10000000000000]\n",
      "\tTraining Loss: 0.753690\n",
      "\tTesting Loss: 0.791208\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1869/10000000000000]\n",
      "\tTraining Loss: 0.753633\n",
      "\tTesting Loss: 0.790046\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1870/10000000000000]\n",
      "\tTraining Loss: 0.753655\n",
      "\tTesting Loss: 0.790713\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1871/10000000000000]\n",
      "\tTraining Loss: 0.753644\n",
      "\tTesting Loss: 0.790399\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1872/10000000000000]\n",
      "\tTraining Loss: 0.753729\n",
      "\tTesting Loss: 0.790635\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1873/10000000000000]\n",
      "\tTraining Loss: 0.753618\n",
      "\tTesting Loss: 0.790881\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1874/10000000000000]\n",
      "\tTraining Loss: 0.753685\n",
      "\tTesting Loss: 0.792193\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1875/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.791516\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1876/10000000000000]\n",
      "\tTraining Loss: 0.753722\n",
      "\tTesting Loss: 0.790662\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1877/10000000000000]\n",
      "\tTraining Loss: 0.753654\n",
      "\tTesting Loss: 0.790537\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1878/10000000000000]\n",
      "\tTraining Loss: 0.753740\n",
      "\tTesting Loss: 0.790630\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1879/10000000000000]\n",
      "\tTraining Loss: 0.753645\n",
      "\tTesting Loss: 0.791084\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1880/10000000000000]\n",
      "\tTraining Loss: 0.753666\n",
      "\tTesting Loss: 0.791278\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1881/10000000000000]\n",
      "\tTraining Loss: 0.753685\n",
      "\tTesting Loss: 0.790966\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1882/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.790739\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1883/10000000000000]\n",
      "\tTraining Loss: 0.753705\n",
      "\tTesting Loss: 0.791271\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1884/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.791710\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1885/10000000000000]\n",
      "\tTraining Loss: 0.753660\n",
      "\tTesting Loss: 0.790494\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1886/10000000000000]\n",
      "\tTraining Loss: 0.753722\n",
      "\tTesting Loss: 0.791636\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1887/10000000000000]\n",
      "\tTraining Loss: 0.753717\n",
      "\tTesting Loss: 0.790722\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1888/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.791836\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1889/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.790708\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1890/10000000000000]\n",
      "\tTraining Loss: 0.753634\n",
      "\tTesting Loss: 0.790452\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1891/10000000000000]\n",
      "\tTraining Loss: 0.753674\n",
      "\tTesting Loss: 0.790933\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1892/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.791282\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1893/10000000000000]\n",
      "\tTraining Loss: 0.753729\n",
      "\tTesting Loss: 0.790945\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1894/10000000000000]\n",
      "\tTraining Loss: 0.753662\n",
      "\tTesting Loss: 0.791166\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1895/10000000000000]\n",
      "\tTraining Loss: 0.753654\n",
      "\tTesting Loss: 0.791294\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1896/10000000000000]\n",
      "\tTraining Loss: 0.753645\n",
      "\tTesting Loss: 0.790275\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1897/10000000000000]\n",
      "\tTraining Loss: 0.753708\n",
      "\tTesting Loss: 0.791806\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1898/10000000000000]\n",
      "\tTraining Loss: 0.753709\n",
      "\tTesting Loss: 0.790670\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1899/10000000000000]\n",
      "\tTraining Loss: 0.753701\n",
      "\tTesting Loss: 0.791219\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1900/10000000000000]\n",
      "\tTraining Loss: 0.753704\n",
      "\tTesting Loss: 0.791176\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1901/10000000000000]\n",
      "\tTraining Loss: 0.753730\n",
      "\tTesting Loss: 0.790945\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1902/10000000000000]\n",
      "\tTraining Loss: 0.753657\n",
      "\tTesting Loss: 0.791614\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1903/10000000000000]\n",
      "\tTraining Loss: 0.753746\n",
      "\tTesting Loss: 0.792128\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1904/10000000000000]\n",
      "\tTraining Loss: 0.753637\n",
      "\tTesting Loss: 0.791657\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1905/10000000000000]\n",
      "\tTraining Loss: 0.753631\n",
      "\tTesting Loss: 0.791598\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1906/10000000000000]\n",
      "\tTraining Loss: 0.753676\n",
      "\tTesting Loss: 0.790074\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1907/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.791386\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1908/10000000000000]\n",
      "\tTraining Loss: 0.753666\n",
      "\tTesting Loss: 0.790770\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1909/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.791482\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1910/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.792220\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1911/10000000000000]\n",
      "\tTraining Loss: 0.753704\n",
      "\tTesting Loss: 0.790689\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1912/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.791711\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1913/10000000000000]\n",
      "\tTraining Loss: 0.753714\n",
      "\tTesting Loss: 0.790856\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1914/10000000000000]\n",
      "\tTraining Loss: 0.753701\n",
      "\tTesting Loss: 0.791628\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1915/10000000000000]\n",
      "\tTraining Loss: 0.753686\n",
      "\tTesting Loss: 0.791325\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1916/10000000000000]\n",
      "\tTraining Loss: 0.753746\n",
      "\tTesting Loss: 0.790458\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1917/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.791330\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1918/10000000000000]\n",
      "\tTraining Loss: 0.753703\n",
      "\tTesting Loss: 0.791270\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1919/10000000000000]\n",
      "\tTraining Loss: 0.753684\n",
      "\tTesting Loss: 0.792262\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1920/10000000000000]\n",
      "\tTraining Loss: 0.753643\n",
      "\tTesting Loss: 0.790780\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1921/10000000000000]\n",
      "\tTraining Loss: 0.753646\n",
      "\tTesting Loss: 0.790146\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1922/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.790153\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1923/10000000000000]\n",
      "\tTraining Loss: 0.753622\n",
      "\tTesting Loss: 0.791293\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1924/10000000000000]\n",
      "\tTraining Loss: 0.753695\n",
      "\tTesting Loss: 0.791227\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1925/10000000000000]\n",
      "\tTraining Loss: 0.753645\n",
      "\tTesting Loss: 0.791093\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1926/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.791112\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1927/10000000000000]\n",
      "\tTraining Loss: 0.753767\n",
      "\tTesting Loss: 0.791706\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1928/10000000000000]\n",
      "\tTraining Loss: 0.753723\n",
      "\tTesting Loss: 0.791084\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1929/10000000000000]\n",
      "\tTraining Loss: 0.753690\n",
      "\tTesting Loss: 0.791544\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1930/10000000000000]\n",
      "\tTraining Loss: 0.753661\n",
      "\tTesting Loss: 0.790566\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1931/10000000000000]\n",
      "\tTraining Loss: 0.753689\n",
      "\tTesting Loss: 0.790984\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1932/10000000000000]\n",
      "\tTraining Loss: 0.753674\n",
      "\tTesting Loss: 0.790680\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1933/10000000000000]\n",
      "\tTraining Loss: 0.753681\n",
      "\tTesting Loss: 0.791847\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1934/10000000000000]\n",
      "\tTraining Loss: 0.753695\n",
      "\tTesting Loss: 0.791587\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1935/10000000000000]\n",
      "\tTraining Loss: 0.753720\n",
      "\tTesting Loss: 0.790985\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1936/10000000000000]\n",
      "\tTraining Loss: 0.753766\n",
      "\tTesting Loss: 0.791069\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1937/10000000000000]\n",
      "\tTraining Loss: 0.753674\n",
      "\tTesting Loss: 0.791138\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1938/10000000000000]\n",
      "\tTraining Loss: 0.753632\n",
      "\tTesting Loss: 0.790565\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1939/10000000000000]\n",
      "\tTraining Loss: 0.753736\n",
      "\tTesting Loss: 0.790833\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1940/10000000000000]\n",
      "\tTraining Loss: 0.753707\n",
      "\tTesting Loss: 0.791485\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1941/10000000000000]\n",
      "\tTraining Loss: 0.753719\n",
      "\tTesting Loss: 0.790769\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1942/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.791556\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1943/10000000000000]\n",
      "\tTraining Loss: 0.753641\n",
      "\tTesting Loss: 0.791555\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1944/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.790472\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1945/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.789832\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1946/10000000000000]\n",
      "\tTraining Loss: 0.753680\n",
      "\tTesting Loss: 0.790799\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1947/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.790798\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1948/10000000000000]\n",
      "\tTraining Loss: 0.753685\n",
      "\tTesting Loss: 0.791124\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1949/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.790022\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1950/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.790637\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1951/10000000000000]\n",
      "\tTraining Loss: 0.753701\n",
      "\tTesting Loss: 0.791215\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1952/10000000000000]\n",
      "\tTraining Loss: 0.753695\n",
      "\tTesting Loss: 0.791323\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1953/10000000000000]\n",
      "\tTraining Loss: 0.753721\n",
      "\tTesting Loss: 0.791386\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1954/10000000000000]\n",
      "\tTraining Loss: 0.753604\n",
      "\tTesting Loss: 0.791248\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1955/10000000000000]\n",
      "\tTraining Loss: 0.753696\n",
      "\tTesting Loss: 0.790794\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1956/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.791134\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1957/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.790424\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1958/10000000000000]\n",
      "\tTraining Loss: 0.753685\n",
      "\tTesting Loss: 0.791824\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1959/10000000000000]\n",
      "\tTraining Loss: 0.753638\n",
      "\tTesting Loss: 0.791581\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1960/10000000000000]\n",
      "\tTraining Loss: 0.753666\n",
      "\tTesting Loss: 0.791142\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1961/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.790568\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1962/10000000000000]\n",
      "\tTraining Loss: 0.753715\n",
      "\tTesting Loss: 0.791294\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1963/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.791287\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1964/10000000000000]\n",
      "\tTraining Loss: 0.753725\n",
      "\tTesting Loss: 0.791334\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1965/10000000000000]\n",
      "\tTraining Loss: 0.753695\n",
      "\tTesting Loss: 0.791303\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1966/10000000000000]\n",
      "\tTraining Loss: 0.753642\n",
      "\tTesting Loss: 0.791182\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1967/10000000000000]\n",
      "\tTraining Loss: 0.753659\n",
      "\tTesting Loss: 0.790473\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1968/10000000000000]\n",
      "\tTraining Loss: 0.753690\n",
      "\tTesting Loss: 0.790814\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1969/10000000000000]\n",
      "\tTraining Loss: 0.753629\n",
      "\tTesting Loss: 0.791466\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1970/10000000000000]\n",
      "\tTraining Loss: 0.753731\n",
      "\tTesting Loss: 0.790363\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1971/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.790809\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1972/10000000000000]\n",
      "\tTraining Loss: 0.753695\n",
      "\tTesting Loss: 0.790942\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1973/10000000000000]\n",
      "\tTraining Loss: 0.753687\n",
      "\tTesting Loss: 0.790806\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1974/10000000000000]\n",
      "\tTraining Loss: 0.753704\n",
      "\tTesting Loss: 0.790423\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1975/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.791538\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1976/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.790038\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1977/10000000000000]\n",
      "\tTraining Loss: 0.753669\n",
      "\tTesting Loss: 0.791300\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1978/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.790426\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1979/10000000000000]\n",
      "\tTraining Loss: 0.753705\n",
      "\tTesting Loss: 0.790762\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1980/10000000000000]\n",
      "\tTraining Loss: 0.753669\n",
      "\tTesting Loss: 0.790568\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1981/10000000000000]\n",
      "\tTraining Loss: 0.753704\n",
      "\tTesting Loss: 0.790732\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1982/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.791090\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1983/10000000000000]\n",
      "\tTraining Loss: 0.753679\n",
      "\tTesting Loss: 0.791368\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1984/10000000000000]\n",
      "\tTraining Loss: 0.753639\n",
      "\tTesting Loss: 0.791094\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1985/10000000000000]\n",
      "\tTraining Loss: 0.753590\n",
      "\tTesting Loss: 0.789635\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1986/10000000000000]\n",
      "\tTraining Loss: 0.753754\n",
      "\tTesting Loss: 0.791351\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1987/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.791569\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1988/10000000000000]\n",
      "\tTraining Loss: 0.753648\n",
      "\tTesting Loss: 0.790769\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1989/10000000000000]\n",
      "\tTraining Loss: 0.753629\n",
      "\tTesting Loss: 0.790992\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1990/10000000000000]\n",
      "\tTraining Loss: 0.753709\n",
      "\tTesting Loss: 0.791063\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1991/10000000000000]\n",
      "\tTraining Loss: 0.753728\n",
      "\tTesting Loss: 0.791622\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1992/10000000000000]\n",
      "\tTraining Loss: 0.753631\n",
      "\tTesting Loss: 0.790991\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1993/10000000000000]\n",
      "\tTraining Loss: 0.753709\n",
      "\tTesting Loss: 0.791355\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1994/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.790949\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1995/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.791109\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1996/10000000000000]\n",
      "\tTraining Loss: 0.753718\n",
      "\tTesting Loss: 0.791845\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1997/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.791012\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1998/10000000000000]\n",
      "\tTraining Loss: 0.753634\n",
      "\tTesting Loss: 0.790943\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [1999/10000000000000]\n",
      "\tTraining Loss: 0.753736\n",
      "\tTesting Loss: 0.790645\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2000/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.791057\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2001/10000000000000]\n",
      "\tTraining Loss: 0.753669\n",
      "\tTesting Loss: 0.791149\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2002/10000000000000]\n",
      "\tTraining Loss: 0.753646\n",
      "\tTesting Loss: 0.791198\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2003/10000000000000]\n",
      "\tTraining Loss: 0.753752\n",
      "\tTesting Loss: 0.790812\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2004/10000000000000]\n",
      "\tTraining Loss: 0.753679\n",
      "\tTesting Loss: 0.790467\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2005/10000000000000]\n",
      "\tTraining Loss: 0.753668\n",
      "\tTesting Loss: 0.791263\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2006/10000000000000]\n",
      "\tTraining Loss: 0.753653\n",
      "\tTesting Loss: 0.791220\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2007/10000000000000]\n",
      "\tTraining Loss: 0.753680\n",
      "\tTesting Loss: 0.790844\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2008/10000000000000]\n",
      "\tTraining Loss: 0.753724\n",
      "\tTesting Loss: 0.791468\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2009/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.790170\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2010/10000000000000]\n",
      "\tTraining Loss: 0.753657\n",
      "\tTesting Loss: 0.791350\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2011/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.790467\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2012/10000000000000]\n",
      "\tTraining Loss: 0.753668\n",
      "\tTesting Loss: 0.791750\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2013/10000000000000]\n",
      "\tTraining Loss: 0.753686\n",
      "\tTesting Loss: 0.791502\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2014/10000000000000]\n",
      "\tTraining Loss: 0.753728\n",
      "\tTesting Loss: 0.791177\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2015/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.790600\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2016/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.790853\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2017/10000000000000]\n",
      "\tTraining Loss: 0.753676\n",
      "\tTesting Loss: 0.791042\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2018/10000000000000]\n",
      "\tTraining Loss: 0.753646\n",
      "\tTesting Loss: 0.791206\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2019/10000000000000]\n",
      "\tTraining Loss: 0.753728\n",
      "\tTesting Loss: 0.790504\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2020/10000000000000]\n",
      "\tTraining Loss: 0.753634\n",
      "\tTesting Loss: 0.790457\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2021/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.791039\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2022/10000000000000]\n",
      "\tTraining Loss: 0.753652\n",
      "\tTesting Loss: 0.791388\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2023/10000000000000]\n",
      "\tTraining Loss: 0.753608\n",
      "\tTesting Loss: 0.790726\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2024/10000000000000]\n",
      "\tTraining Loss: 0.753641\n",
      "\tTesting Loss: 0.790877\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2025/10000000000000]\n",
      "\tTraining Loss: 0.753653\n",
      "\tTesting Loss: 0.790706\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2026/10000000000000]\n",
      "\tTraining Loss: 0.753645\n",
      "\tTesting Loss: 0.791270\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2027/10000000000000]\n",
      "\tTraining Loss: 0.753677\n",
      "\tTesting Loss: 0.791436\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2028/10000000000000]\n",
      "\tTraining Loss: 0.753743\n",
      "\tTesting Loss: 0.791090\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2029/10000000000000]\n",
      "\tTraining Loss: 0.753614\n",
      "\tTesting Loss: 0.790443\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2030/10000000000000]\n",
      "\tTraining Loss: 0.753681\n",
      "\tTesting Loss: 0.789863\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2031/10000000000000]\n",
      "\tTraining Loss: 0.753696\n",
      "\tTesting Loss: 0.791055\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2032/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.791666\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2033/10000000000000]\n",
      "\tTraining Loss: 0.753662\n",
      "\tTesting Loss: 0.791071\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2034/10000000000000]\n",
      "\tTraining Loss: 0.753621\n",
      "\tTesting Loss: 0.791246\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2035/10000000000000]\n",
      "\tTraining Loss: 0.753716\n",
      "\tTesting Loss: 0.790966\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2036/10000000000000]\n",
      "\tTraining Loss: 0.753690\n",
      "\tTesting Loss: 0.790750\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2037/10000000000000]\n",
      "\tTraining Loss: 0.753686\n",
      "\tTesting Loss: 0.791582\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2038/10000000000000]\n",
      "\tTraining Loss: 0.753639\n",
      "\tTesting Loss: 0.791639\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2039/10000000000000]\n",
      "\tTraining Loss: 0.753615\n",
      "\tTesting Loss: 0.790881\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2040/10000000000000]\n",
      "\tTraining Loss: 0.753621\n",
      "\tTesting Loss: 0.791684\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2041/10000000000000]\n",
      "\tTraining Loss: 0.753652\n",
      "\tTesting Loss: 0.790955\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2042/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.791660\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2043/10000000000000]\n",
      "\tTraining Loss: 0.753669\n",
      "\tTesting Loss: 0.790553\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2044/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.790953\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2045/10000000000000]\n",
      "\tTraining Loss: 0.753666\n",
      "\tTesting Loss: 0.791404\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2046/10000000000000]\n",
      "\tTraining Loss: 0.753645\n",
      "\tTesting Loss: 0.791378\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2047/10000000000000]\n",
      "\tTraining Loss: 0.753645\n",
      "\tTesting Loss: 0.790825\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2048/10000000000000]\n",
      "\tTraining Loss: 0.753721\n",
      "\tTesting Loss: 0.791286\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2049/10000000000000]\n",
      "\tTraining Loss: 0.753740\n",
      "\tTesting Loss: 0.791016\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2050/10000000000000]\n",
      "\tTraining Loss: 0.753653\n",
      "\tTesting Loss: 0.790681\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2051/10000000000000]\n",
      "\tTraining Loss: 0.753660\n",
      "\tTesting Loss: 0.791139\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2052/10000000000000]\n",
      "\tTraining Loss: 0.753674\n",
      "\tTesting Loss: 0.792118\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2053/10000000000000]\n",
      "\tTraining Loss: 0.753685\n",
      "\tTesting Loss: 0.792205\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2054/10000000000000]\n",
      "\tTraining Loss: 0.753636\n",
      "\tTesting Loss: 0.789911\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2055/10000000000000]\n",
      "\tTraining Loss: 0.753668\n",
      "\tTesting Loss: 0.790645\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2056/10000000000000]\n",
      "\tTraining Loss: 0.753645\n",
      "\tTesting Loss: 0.790795\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2057/10000000000000]\n",
      "\tTraining Loss: 0.753657\n",
      "\tTesting Loss: 0.791267\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2058/10000000000000]\n",
      "\tTraining Loss: 0.753597\n",
      "\tTesting Loss: 0.791630\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2059/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.790846\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2060/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.791362\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2061/10000000000000]\n",
      "\tTraining Loss: 0.753736\n",
      "\tTesting Loss: 0.791448\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2062/10000000000000]\n",
      "\tTraining Loss: 0.753703\n",
      "\tTesting Loss: 0.791653\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2063/10000000000000]\n",
      "\tTraining Loss: 0.753705\n",
      "\tTesting Loss: 0.791111\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2064/10000000000000]\n",
      "\tTraining Loss: 0.753670\n",
      "\tTesting Loss: 0.790862\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2065/10000000000000]\n",
      "\tTraining Loss: 0.753674\n",
      "\tTesting Loss: 0.790099\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2066/10000000000000]\n",
      "\tTraining Loss: 0.753705\n",
      "\tTesting Loss: 0.791197\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2067/10000000000000]\n",
      "\tTraining Loss: 0.753687\n",
      "\tTesting Loss: 0.791487\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2068/10000000000000]\n",
      "\tTraining Loss: 0.753721\n",
      "\tTesting Loss: 0.791718\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2069/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.791952\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2070/10000000000000]\n",
      "\tTraining Loss: 0.753681\n",
      "\tTesting Loss: 0.791756\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2071/10000000000000]\n",
      "\tTraining Loss: 0.753634\n",
      "\tTesting Loss: 0.789702\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2072/10000000000000]\n",
      "\tTraining Loss: 0.753655\n",
      "\tTesting Loss: 0.791925\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2073/10000000000000]\n",
      "\tTraining Loss: 0.753709\n",
      "\tTesting Loss: 0.791076\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2074/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.791371\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2075/10000000000000]\n",
      "\tTraining Loss: 0.753627\n",
      "\tTesting Loss: 0.791024\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2076/10000000000000]\n",
      "\tTraining Loss: 0.753705\n",
      "\tTesting Loss: 0.790974\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2077/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.791466\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2078/10000000000000]\n",
      "\tTraining Loss: 0.753705\n",
      "\tTesting Loss: 0.791089\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2079/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.791495\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2080/10000000000000]\n",
      "\tTraining Loss: 0.753685\n",
      "\tTesting Loss: 0.791387\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2081/10000000000000]\n",
      "\tTraining Loss: 0.753710\n",
      "\tTesting Loss: 0.790800\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2082/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.790181\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2083/10000000000000]\n",
      "\tTraining Loss: 0.753596\n",
      "\tTesting Loss: 0.791275\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2084/10000000000000]\n",
      "\tTraining Loss: 0.753697\n",
      "\tTesting Loss: 0.790297\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2085/10000000000000]\n",
      "\tTraining Loss: 0.753684\n",
      "\tTesting Loss: 0.791366\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2086/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.790992\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2087/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.791400\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2088/10000000000000]\n",
      "\tTraining Loss: 0.753705\n",
      "\tTesting Loss: 0.790908\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2089/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.792147\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2090/10000000000000]\n",
      "\tTraining Loss: 0.753654\n",
      "\tTesting Loss: 0.790237\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2091/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.791407\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2092/10000000000000]\n",
      "\tTraining Loss: 0.753661\n",
      "\tTesting Loss: 0.789952\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2093/10000000000000]\n",
      "\tTraining Loss: 0.753715\n",
      "\tTesting Loss: 0.791104\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2094/10000000000000]\n",
      "\tTraining Loss: 0.753674\n",
      "\tTesting Loss: 0.791432\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2095/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.791466\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2096/10000000000000]\n",
      "\tTraining Loss: 0.753636\n",
      "\tTesting Loss: 0.790612\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2097/10000000000000]\n",
      "\tTraining Loss: 0.753670\n",
      "\tTesting Loss: 0.791059\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2098/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.790346\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2099/10000000000000]\n",
      "\tTraining Loss: 0.753729\n",
      "\tTesting Loss: 0.790999\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2100/10000000000000]\n",
      "\tTraining Loss: 0.753662\n",
      "\tTesting Loss: 0.791433\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2101/10000000000000]\n",
      "\tTraining Loss: 0.753722\n",
      "\tTesting Loss: 0.791415\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2102/10000000000000]\n",
      "\tTraining Loss: 0.753710\n",
      "\tTesting Loss: 0.791369\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2103/10000000000000]\n",
      "\tTraining Loss: 0.753691\n",
      "\tTesting Loss: 0.791737\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2104/10000000000000]\n",
      "\tTraining Loss: 0.753619\n",
      "\tTesting Loss: 0.790944\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2105/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.791358\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2106/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.790993\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2107/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.790697\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2108/10000000000000]\n",
      "\tTraining Loss: 0.753669\n",
      "\tTesting Loss: 0.791067\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2109/10000000000000]\n",
      "\tTraining Loss: 0.753640\n",
      "\tTesting Loss: 0.790775\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2110/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.791696\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2111/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.791185\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2112/10000000000000]\n",
      "\tTraining Loss: 0.753661\n",
      "\tTesting Loss: 0.790321\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2113/10000000000000]\n",
      "\tTraining Loss: 0.753644\n",
      "\tTesting Loss: 0.791501\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2114/10000000000000]\n",
      "\tTraining Loss: 0.753657\n",
      "\tTesting Loss: 0.790382\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2115/10000000000000]\n",
      "\tTraining Loss: 0.753687\n",
      "\tTesting Loss: 0.791615\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2116/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.790817\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2117/10000000000000]\n",
      "\tTraining Loss: 0.753664\n",
      "\tTesting Loss: 0.791331\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2118/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.791409\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2119/10000000000000]\n",
      "\tTraining Loss: 0.753596\n",
      "\tTesting Loss: 0.791100\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2120/10000000000000]\n",
      "\tTraining Loss: 0.753687\n",
      "\tTesting Loss: 0.791360\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2121/10000000000000]\n",
      "\tTraining Loss: 0.753631\n",
      "\tTesting Loss: 0.790796\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2122/10000000000000]\n",
      "\tTraining Loss: 0.753680\n",
      "\tTesting Loss: 0.791104\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2123/10000000000000]\n",
      "\tTraining Loss: 0.753713\n",
      "\tTesting Loss: 0.791276\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2124/10000000000000]\n",
      "\tTraining Loss: 0.753684\n",
      "\tTesting Loss: 0.792010\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2125/10000000000000]\n",
      "\tTraining Loss: 0.753661\n",
      "\tTesting Loss: 0.790727\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2126/10000000000000]\n",
      "\tTraining Loss: 0.753622\n",
      "\tTesting Loss: 0.790720\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2127/10000000000000]\n",
      "\tTraining Loss: 0.753711\n",
      "\tTesting Loss: 0.791141\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2128/10000000000000]\n",
      "\tTraining Loss: 0.753666\n",
      "\tTesting Loss: 0.791355\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2129/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.791218\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2130/10000000000000]\n",
      "\tTraining Loss: 0.753626\n",
      "\tTesting Loss: 0.790537\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2131/10000000000000]\n",
      "\tTraining Loss: 0.753700\n",
      "\tTesting Loss: 0.790626\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2132/10000000000000]\n",
      "\tTraining Loss: 0.753700\n",
      "\tTesting Loss: 0.791018\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2133/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.791100\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2134/10000000000000]\n",
      "\tTraining Loss: 0.753686\n",
      "\tTesting Loss: 0.790455\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2135/10000000000000]\n",
      "\tTraining Loss: 0.753661\n",
      "\tTesting Loss: 0.791522\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2136/10000000000000]\n",
      "\tTraining Loss: 0.753705\n",
      "\tTesting Loss: 0.791336\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2137/10000000000000]\n",
      "\tTraining Loss: 0.753627\n",
      "\tTesting Loss: 0.791017\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2138/10000000000000]\n",
      "\tTraining Loss: 0.753597\n",
      "\tTesting Loss: 0.790550\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2139/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.790895\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2140/10000000000000]\n",
      "\tTraining Loss: 0.753699\n",
      "\tTesting Loss: 0.791046\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2141/10000000000000]\n",
      "\tTraining Loss: 0.753728\n",
      "\tTesting Loss: 0.790971\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2142/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.791346\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2143/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.790651\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2144/10000000000000]\n",
      "\tTraining Loss: 0.753697\n",
      "\tTesting Loss: 0.791270\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2145/10000000000000]\n",
      "\tTraining Loss: 0.753596\n",
      "\tTesting Loss: 0.790848\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2146/10000000000000]\n",
      "\tTraining Loss: 0.753652\n",
      "\tTesting Loss: 0.791032\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2147/10000000000000]\n",
      "\tTraining Loss: 0.753704\n",
      "\tTesting Loss: 0.791519\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2148/10000000000000]\n",
      "\tTraining Loss: 0.753630\n",
      "\tTesting Loss: 0.790648\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2149/10000000000000]\n",
      "\tTraining Loss: 0.753715\n",
      "\tTesting Loss: 0.791380\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2150/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.790483\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2151/10000000000000]\n",
      "\tTraining Loss: 0.753644\n",
      "\tTesting Loss: 0.791182\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2152/10000000000000]\n",
      "\tTraining Loss: 0.753598\n",
      "\tTesting Loss: 0.790992\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2153/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.790403\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2154/10000000000000]\n",
      "\tTraining Loss: 0.753676\n",
      "\tTesting Loss: 0.791146\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2155/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.790480\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2156/10000000000000]\n",
      "\tTraining Loss: 0.753676\n",
      "\tTesting Loss: 0.790596\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2157/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.790965\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2158/10000000000000]\n",
      "\tTraining Loss: 0.753689\n",
      "\tTesting Loss: 0.792390\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2159/10000000000000]\n",
      "\tTraining Loss: 0.753705\n",
      "\tTesting Loss: 0.790894\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2160/10000000000000]\n",
      "\tTraining Loss: 0.753608\n",
      "\tTesting Loss: 0.791659\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2161/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.790945\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2162/10000000000000]\n",
      "\tTraining Loss: 0.753668\n",
      "\tTesting Loss: 0.790258\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2163/10000000000000]\n",
      "\tTraining Loss: 0.753709\n",
      "\tTesting Loss: 0.790796\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2164/10000000000000]\n",
      "\tTraining Loss: 0.753615\n",
      "\tTesting Loss: 0.790854\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2165/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.791151\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2166/10000000000000]\n",
      "\tTraining Loss: 0.753706\n",
      "\tTesting Loss: 0.791451\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2167/10000000000000]\n",
      "\tTraining Loss: 0.753712\n",
      "\tTesting Loss: 0.791226\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2168/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.791582\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2169/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.791145\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2170/10000000000000]\n",
      "\tTraining Loss: 0.753716\n",
      "\tTesting Loss: 0.791312\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2171/10000000000000]\n",
      "\tTraining Loss: 0.753655\n",
      "\tTesting Loss: 0.791511\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2172/10000000000000]\n",
      "\tTraining Loss: 0.753644\n",
      "\tTesting Loss: 0.791996\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2173/10000000000000]\n",
      "\tTraining Loss: 0.753695\n",
      "\tTesting Loss: 0.791772\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2174/10000000000000]\n",
      "\tTraining Loss: 0.753701\n",
      "\tTesting Loss: 0.791920\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2175/10000000000000]\n",
      "\tTraining Loss: 0.753691\n",
      "\tTesting Loss: 0.791404\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2176/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.789739\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2177/10000000000000]\n",
      "\tTraining Loss: 0.753640\n",
      "\tTesting Loss: 0.791059\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2178/10000000000000]\n",
      "\tTraining Loss: 0.753653\n",
      "\tTesting Loss: 0.791354\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2179/10000000000000]\n",
      "\tTraining Loss: 0.753695\n",
      "\tTesting Loss: 0.791720\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2180/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.790993\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2181/10000000000000]\n",
      "\tTraining Loss: 0.753630\n",
      "\tTesting Loss: 0.791681\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2182/10000000000000]\n",
      "\tTraining Loss: 0.753737\n",
      "\tTesting Loss: 0.791238\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2183/10000000000000]\n",
      "\tTraining Loss: 0.753632\n",
      "\tTesting Loss: 0.790688\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2184/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.791445\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2185/10000000000000]\n",
      "\tTraining Loss: 0.753609\n",
      "\tTesting Loss: 0.790005\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2186/10000000000000]\n",
      "\tTraining Loss: 0.753723\n",
      "\tTesting Loss: 0.790933\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2187/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.790650\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2188/10000000000000]\n",
      "\tTraining Loss: 0.753664\n",
      "\tTesting Loss: 0.790961\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2189/10000000000000]\n",
      "\tTraining Loss: 0.753644\n",
      "\tTesting Loss: 0.790606\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2190/10000000000000]\n",
      "\tTraining Loss: 0.753630\n",
      "\tTesting Loss: 0.790047\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2191/10000000000000]\n",
      "\tTraining Loss: 0.753715\n",
      "\tTesting Loss: 0.790577\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2192/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.790329\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2193/10000000000000]\n",
      "\tTraining Loss: 0.753664\n",
      "\tTesting Loss: 0.790991\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2194/10000000000000]\n",
      "\tTraining Loss: 0.753686\n",
      "\tTesting Loss: 0.791402\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2195/10000000000000]\n",
      "\tTraining Loss: 0.753654\n",
      "\tTesting Loss: 0.791469\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2196/10000000000000]\n",
      "\tTraining Loss: 0.753705\n",
      "\tTesting Loss: 0.790460\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2197/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.790947\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2198/10000000000000]\n",
      "\tTraining Loss: 0.753664\n",
      "\tTesting Loss: 0.791094\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2199/10000000000000]\n",
      "\tTraining Loss: 0.753623\n",
      "\tTesting Loss: 0.790612\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2200/10000000000000]\n",
      "\tTraining Loss: 0.753653\n",
      "\tTesting Loss: 0.791487\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2201/10000000000000]\n",
      "\tTraining Loss: 0.753657\n",
      "\tTesting Loss: 0.790250\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2202/10000000000000]\n",
      "\tTraining Loss: 0.753666\n",
      "\tTesting Loss: 0.791382\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2203/10000000000000]\n",
      "\tTraining Loss: 0.753685\n",
      "\tTesting Loss: 0.791258\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2204/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.790872\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2205/10000000000000]\n",
      "\tTraining Loss: 0.753639\n",
      "\tTesting Loss: 0.791476\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2206/10000000000000]\n",
      "\tTraining Loss: 0.753669\n",
      "\tTesting Loss: 0.790473\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2207/10000000000000]\n",
      "\tTraining Loss: 0.753639\n",
      "\tTesting Loss: 0.791164\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2208/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.791224\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2209/10000000000000]\n",
      "\tTraining Loss: 0.753626\n",
      "\tTesting Loss: 0.790810\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2210/10000000000000]\n",
      "\tTraining Loss: 0.753662\n",
      "\tTesting Loss: 0.791508\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2211/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.791349\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2212/10000000000000]\n",
      "\tTraining Loss: 0.753756\n",
      "\tTesting Loss: 0.791881\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2213/10000000000000]\n",
      "\tTraining Loss: 0.753681\n",
      "\tTesting Loss: 0.792063\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2214/10000000000000]\n",
      "\tTraining Loss: 0.753712\n",
      "\tTesting Loss: 0.791221\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2215/10000000000000]\n",
      "\tTraining Loss: 0.753660\n",
      "\tTesting Loss: 0.791676\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2216/10000000000000]\n",
      "\tTraining Loss: 0.753677\n",
      "\tTesting Loss: 0.791202\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2217/10000000000000]\n",
      "\tTraining Loss: 0.753715\n",
      "\tTesting Loss: 0.790484\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2218/10000000000000]\n",
      "\tTraining Loss: 0.753710\n",
      "\tTesting Loss: 0.790647\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2219/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.790999\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2220/10000000000000]\n",
      "\tTraining Loss: 0.753709\n",
      "\tTesting Loss: 0.790486\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2221/10000000000000]\n",
      "\tTraining Loss: 0.753629\n",
      "\tTesting Loss: 0.790608\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2222/10000000000000]\n",
      "\tTraining Loss: 0.753657\n",
      "\tTesting Loss: 0.789541\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2223/10000000000000]\n",
      "\tTraining Loss: 0.753624\n",
      "\tTesting Loss: 0.790060\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2224/10000000000000]\n",
      "\tTraining Loss: 0.753701\n",
      "\tTesting Loss: 0.791046\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2225/10000000000000]\n",
      "\tTraining Loss: 0.753647\n",
      "\tTesting Loss: 0.790463\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2226/10000000000000]\n",
      "\tTraining Loss: 0.753687\n",
      "\tTesting Loss: 0.791367\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2227/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.791733\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2228/10000000000000]\n",
      "\tTraining Loss: 0.753661\n",
      "\tTesting Loss: 0.790932\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2229/10000000000000]\n",
      "\tTraining Loss: 0.753623\n",
      "\tTesting Loss: 0.791463\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2230/10000000000000]\n",
      "\tTraining Loss: 0.753687\n",
      "\tTesting Loss: 0.790510\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2231/10000000000000]\n",
      "\tTraining Loss: 0.753755\n",
      "\tTesting Loss: 0.790504\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2232/10000000000000]\n",
      "\tTraining Loss: 0.753668\n",
      "\tTesting Loss: 0.790996\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2233/10000000000000]\n",
      "\tTraining Loss: 0.753653\n",
      "\tTesting Loss: 0.791437\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2234/10000000000000]\n",
      "\tTraining Loss: 0.753653\n",
      "\tTesting Loss: 0.790556\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2235/10000000000000]\n",
      "\tTraining Loss: 0.753697\n",
      "\tTesting Loss: 0.791942\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2236/10000000000000]\n",
      "\tTraining Loss: 0.753638\n",
      "\tTesting Loss: 0.790604\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2237/10000000000000]\n",
      "\tTraining Loss: 0.753689\n",
      "\tTesting Loss: 0.790660\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2238/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.790649\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2239/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.790573\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2240/10000000000000]\n",
      "\tTraining Loss: 0.753624\n",
      "\tTesting Loss: 0.790092\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2241/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.790865\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2242/10000000000000]\n",
      "\tTraining Loss: 0.753720\n",
      "\tTesting Loss: 0.790722\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2243/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.790527\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2244/10000000000000]\n",
      "\tTraining Loss: 0.753702\n",
      "\tTesting Loss: 0.791310\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2245/10000000000000]\n",
      "\tTraining Loss: 0.753701\n",
      "\tTesting Loss: 0.791001\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2246/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.790525\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2247/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.789857\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2248/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.790932\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2249/10000000000000]\n",
      "\tTraining Loss: 0.753645\n",
      "\tTesting Loss: 0.790650\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2250/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.791159\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2251/10000000000000]\n",
      "\tTraining Loss: 0.753685\n",
      "\tTesting Loss: 0.790700\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2252/10000000000000]\n",
      "\tTraining Loss: 0.753706\n",
      "\tTesting Loss: 0.789834\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2253/10000000000000]\n",
      "\tTraining Loss: 0.753647\n",
      "\tTesting Loss: 0.791152\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2254/10000000000000]\n",
      "\tTraining Loss: 0.753621\n",
      "\tTesting Loss: 0.790582\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2255/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.790042\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2256/10000000000000]\n",
      "\tTraining Loss: 0.753670\n",
      "\tTesting Loss: 0.791080\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2257/10000000000000]\n",
      "\tTraining Loss: 0.753733\n",
      "\tTesting Loss: 0.791555\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2258/10000000000000]\n",
      "\tTraining Loss: 0.753712\n",
      "\tTesting Loss: 0.791341\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2259/10000000000000]\n",
      "\tTraining Loss: 0.753603\n",
      "\tTesting Loss: 0.791494\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2260/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.790394\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2261/10000000000000]\n",
      "\tTraining Loss: 0.753700\n",
      "\tTesting Loss: 0.790059\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2262/10000000000000]\n",
      "\tTraining Loss: 0.753686\n",
      "\tTesting Loss: 0.790297\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2263/10000000000000]\n",
      "\tTraining Loss: 0.753625\n",
      "\tTesting Loss: 0.790245\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2264/10000000000000]\n",
      "\tTraining Loss: 0.753601\n",
      "\tTesting Loss: 0.791458\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2265/10000000000000]\n",
      "\tTraining Loss: 0.753626\n",
      "\tTesting Loss: 0.791117\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2266/10000000000000]\n",
      "\tTraining Loss: 0.753660\n",
      "\tTesting Loss: 0.790292\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2267/10000000000000]\n",
      "\tTraining Loss: 0.753699\n",
      "\tTesting Loss: 0.790881\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2268/10000000000000]\n",
      "\tTraining Loss: 0.753638\n",
      "\tTesting Loss: 0.791557\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2269/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.790973\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2270/10000000000000]\n",
      "\tTraining Loss: 0.753711\n",
      "\tTesting Loss: 0.790805\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2271/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.790989\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2272/10000000000000]\n",
      "\tTraining Loss: 0.753703\n",
      "\tTesting Loss: 0.791022\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2273/10000000000000]\n",
      "\tTraining Loss: 0.753643\n",
      "\tTesting Loss: 0.791314\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2274/10000000000000]\n",
      "\tTraining Loss: 0.753684\n",
      "\tTesting Loss: 0.791759\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2275/10000000000000]\n",
      "\tTraining Loss: 0.753690\n",
      "\tTesting Loss: 0.790857\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2276/10000000000000]\n",
      "\tTraining Loss: 0.753679\n",
      "\tTesting Loss: 0.791243\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2277/10000000000000]\n",
      "\tTraining Loss: 0.753706\n",
      "\tTesting Loss: 0.791539\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2278/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.790564\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2279/10000000000000]\n",
      "\tTraining Loss: 0.753703\n",
      "\tTesting Loss: 0.790939\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2280/10000000000000]\n",
      "\tTraining Loss: 0.753674\n",
      "\tTesting Loss: 0.790414\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2281/10000000000000]\n",
      "\tTraining Loss: 0.753640\n",
      "\tTesting Loss: 0.790582\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2282/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.791258\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2283/10000000000000]\n",
      "\tTraining Loss: 0.753697\n",
      "\tTesting Loss: 0.790508\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2284/10000000000000]\n",
      "\tTraining Loss: 0.753594\n",
      "\tTesting Loss: 0.791326\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2285/10000000000000]\n",
      "\tTraining Loss: 0.753697\n",
      "\tTesting Loss: 0.791041\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2286/10000000000000]\n",
      "\tTraining Loss: 0.753623\n",
      "\tTesting Loss: 0.791137\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2287/10000000000000]\n",
      "\tTraining Loss: 0.753657\n",
      "\tTesting Loss: 0.791251\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2288/10000000000000]\n",
      "\tTraining Loss: 0.753701\n",
      "\tTesting Loss: 0.792300\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2289/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.790852\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2290/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.791383\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2291/10000000000000]\n",
      "\tTraining Loss: 0.753639\n",
      "\tTesting Loss: 0.791068\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2292/10000000000000]\n",
      "\tTraining Loss: 0.753708\n",
      "\tTesting Loss: 0.791805\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2293/10000000000000]\n",
      "\tTraining Loss: 0.753680\n",
      "\tTesting Loss: 0.790300\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2294/10000000000000]\n",
      "\tTraining Loss: 0.753637\n",
      "\tTesting Loss: 0.791926\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2295/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.791098\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2296/10000000000000]\n",
      "\tTraining Loss: 0.753701\n",
      "\tTesting Loss: 0.791022\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2297/10000000000000]\n",
      "\tTraining Loss: 0.753642\n",
      "\tTesting Loss: 0.790387\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2298/10000000000000]\n",
      "\tTraining Loss: 0.753724\n",
      "\tTesting Loss: 0.790445\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2299/10000000000000]\n",
      "\tTraining Loss: 0.753664\n",
      "\tTesting Loss: 0.791255\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2300/10000000000000]\n",
      "\tTraining Loss: 0.753674\n",
      "\tTesting Loss: 0.791593\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2301/10000000000000]\n",
      "\tTraining Loss: 0.753655\n",
      "\tTesting Loss: 0.791143\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2302/10000000000000]\n",
      "\tTraining Loss: 0.753708\n",
      "\tTesting Loss: 0.790578\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2303/10000000000000]\n",
      "\tTraining Loss: 0.753701\n",
      "\tTesting Loss: 0.790966\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2304/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.791290\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2305/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.789851\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2306/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.790817\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2307/10000000000000]\n",
      "\tTraining Loss: 0.753706\n",
      "\tTesting Loss: 0.791547\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2308/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.792169\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2309/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.791138\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2310/10000000000000]\n",
      "\tTraining Loss: 0.753679\n",
      "\tTesting Loss: 0.791469\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2311/10000000000000]\n",
      "\tTraining Loss: 0.753685\n",
      "\tTesting Loss: 0.791628\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2312/10000000000000]\n",
      "\tTraining Loss: 0.753668\n",
      "\tTesting Loss: 0.791936\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2313/10000000000000]\n",
      "\tTraining Loss: 0.753714\n",
      "\tTesting Loss: 0.791590\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2314/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.791320\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2315/10000000000000]\n",
      "\tTraining Loss: 0.753695\n",
      "\tTesting Loss: 0.790733\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2316/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.790861\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2317/10000000000000]\n",
      "\tTraining Loss: 0.753710\n",
      "\tTesting Loss: 0.791648\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2318/10000000000000]\n",
      "\tTraining Loss: 0.753738\n",
      "\tTesting Loss: 0.791493\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2319/10000000000000]\n",
      "\tTraining Loss: 0.753615\n",
      "\tTesting Loss: 0.791233\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2320/10000000000000]\n",
      "\tTraining Loss: 0.753708\n",
      "\tTesting Loss: 0.791311\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2321/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.791653\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2322/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.790705\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2323/10000000000000]\n",
      "\tTraining Loss: 0.753676\n",
      "\tTesting Loss: 0.790823\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2324/10000000000000]\n",
      "\tTraining Loss: 0.753590\n",
      "\tTesting Loss: 0.791236\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2325/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.790486\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2326/10000000000000]\n",
      "\tTraining Loss: 0.753691\n",
      "\tTesting Loss: 0.791154\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2327/10000000000000]\n",
      "\tTraining Loss: 0.753687\n",
      "\tTesting Loss: 0.790270\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2328/10000000000000]\n",
      "\tTraining Loss: 0.753719\n",
      "\tTesting Loss: 0.791477\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2329/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.790667\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2330/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.790672\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2331/10000000000000]\n",
      "\tTraining Loss: 0.753670\n",
      "\tTesting Loss: 0.790532\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2332/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.791033\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2333/10000000000000]\n",
      "\tTraining Loss: 0.753681\n",
      "\tTesting Loss: 0.790578\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2334/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.790284\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2335/10000000000000]\n",
      "\tTraining Loss: 0.753655\n",
      "\tTesting Loss: 0.790623\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2336/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.790763\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2337/10000000000000]\n",
      "\tTraining Loss: 0.753659\n",
      "\tTesting Loss: 0.792759\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2338/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.791295\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2339/10000000000000]\n",
      "\tTraining Loss: 0.753654\n",
      "\tTesting Loss: 0.791296\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2340/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.790803\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2341/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.789986\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2342/10000000000000]\n",
      "\tTraining Loss: 0.753716\n",
      "\tTesting Loss: 0.790554\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2343/10000000000000]\n",
      "\tTraining Loss: 0.753718\n",
      "\tTesting Loss: 0.791002\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2344/10000000000000]\n",
      "\tTraining Loss: 0.753714\n",
      "\tTesting Loss: 0.791268\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2345/10000000000000]\n",
      "\tTraining Loss: 0.753712\n",
      "\tTesting Loss: 0.790201\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2346/10000000000000]\n",
      "\tTraining Loss: 0.753637\n",
      "\tTesting Loss: 0.791066\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2347/10000000000000]\n",
      "\tTraining Loss: 0.753690\n",
      "\tTesting Loss: 0.791360\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2348/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.790917\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2349/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.790527\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2350/10000000000000]\n",
      "\tTraining Loss: 0.753730\n",
      "\tTesting Loss: 0.790179\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2351/10000000000000]\n",
      "\tTraining Loss: 0.753654\n",
      "\tTesting Loss: 0.791127\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2352/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.791752\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2353/10000000000000]\n",
      "\tTraining Loss: 0.753679\n",
      "\tTesting Loss: 0.791410\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2354/10000000000000]\n",
      "\tTraining Loss: 0.753668\n",
      "\tTesting Loss: 0.790841\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2355/10000000000000]\n",
      "\tTraining Loss: 0.753670\n",
      "\tTesting Loss: 0.790169\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2356/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.791777\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2357/10000000000000]\n",
      "\tTraining Loss: 0.753739\n",
      "\tTesting Loss: 0.790747\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2358/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.790836\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2359/10000000000000]\n",
      "\tTraining Loss: 0.753681\n",
      "\tTesting Loss: 0.791148\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2360/10000000000000]\n",
      "\tTraining Loss: 0.753636\n",
      "\tTesting Loss: 0.792471\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2361/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.790487\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2362/10000000000000]\n",
      "\tTraining Loss: 0.753679\n",
      "\tTesting Loss: 0.790172\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2363/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.790240\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2364/10000000000000]\n",
      "\tTraining Loss: 0.753620\n",
      "\tTesting Loss: 0.791791\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2365/10000000000000]\n",
      "\tTraining Loss: 0.753662\n",
      "\tTesting Loss: 0.790582\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2366/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.791420\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2367/10000000000000]\n",
      "\tTraining Loss: 0.753611\n",
      "\tTesting Loss: 0.790867\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2368/10000000000000]\n",
      "\tTraining Loss: 0.753641\n",
      "\tTesting Loss: 0.791138\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2369/10000000000000]\n",
      "\tTraining Loss: 0.753691\n",
      "\tTesting Loss: 0.790970\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2370/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.791262\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2371/10000000000000]\n",
      "\tTraining Loss: 0.753664\n",
      "\tTesting Loss: 0.791284\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2372/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.791994\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2373/10000000000000]\n",
      "\tTraining Loss: 0.753630\n",
      "\tTesting Loss: 0.790837\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2374/10000000000000]\n",
      "\tTraining Loss: 0.753640\n",
      "\tTesting Loss: 0.790985\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2375/10000000000000]\n",
      "\tTraining Loss: 0.753587\n",
      "\tTesting Loss: 0.789915\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2376/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.791373\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2377/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.791795\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2378/10000000000000]\n",
      "\tTraining Loss: 0.753727\n",
      "\tTesting Loss: 0.791133\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2379/10000000000000]\n",
      "\tTraining Loss: 0.753646\n",
      "\tTesting Loss: 0.791453\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2380/10000000000000]\n",
      "\tTraining Loss: 0.753719\n",
      "\tTesting Loss: 0.791085\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2381/10000000000000]\n",
      "\tTraining Loss: 0.753638\n",
      "\tTesting Loss: 0.790370\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2382/10000000000000]\n",
      "\tTraining Loss: 0.753701\n",
      "\tTesting Loss: 0.789832\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2383/10000000000000]\n",
      "\tTraining Loss: 0.753679\n",
      "\tTesting Loss: 0.790897\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2384/10000000000000]\n",
      "\tTraining Loss: 0.753613\n",
      "\tTesting Loss: 0.791236\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2385/10000000000000]\n",
      "\tTraining Loss: 0.753686\n",
      "\tTesting Loss: 0.790683\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2386/10000000000000]\n",
      "\tTraining Loss: 0.753582\n",
      "\tTesting Loss: 0.791785\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2387/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.790688\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2388/10000000000000]\n",
      "\tTraining Loss: 0.753694\n",
      "\tTesting Loss: 0.790589\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2389/10000000000000]\n",
      "\tTraining Loss: 0.753657\n",
      "\tTesting Loss: 0.791336\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2390/10000000000000]\n",
      "\tTraining Loss: 0.753590\n",
      "\tTesting Loss: 0.791245\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2391/10000000000000]\n",
      "\tTraining Loss: 0.753629\n",
      "\tTesting Loss: 0.791760\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2392/10000000000000]\n",
      "\tTraining Loss: 0.753637\n",
      "\tTesting Loss: 0.790878\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2393/10000000000000]\n",
      "\tTraining Loss: 0.753632\n",
      "\tTesting Loss: 0.791175\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2394/10000000000000]\n",
      "\tTraining Loss: 0.753624\n",
      "\tTesting Loss: 0.790042\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2395/10000000000000]\n",
      "\tTraining Loss: 0.753640\n",
      "\tTesting Loss: 0.790606\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2396/10000000000000]\n",
      "\tTraining Loss: 0.753662\n",
      "\tTesting Loss: 0.791179\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2397/10000000000000]\n",
      "\tTraining Loss: 0.753702\n",
      "\tTesting Loss: 0.791130\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2398/10000000000000]\n",
      "\tTraining Loss: 0.753677\n",
      "\tTesting Loss: 0.790997\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2399/10000000000000]\n",
      "\tTraining Loss: 0.753681\n",
      "\tTesting Loss: 0.790411\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2400/10000000000000]\n",
      "\tTraining Loss: 0.753693\n",
      "\tTesting Loss: 0.791024\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2401/10000000000000]\n",
      "\tTraining Loss: 0.753711\n",
      "\tTesting Loss: 0.790841\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2402/10000000000000]\n",
      "\tTraining Loss: 0.753618\n",
      "\tTesting Loss: 0.790917\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2403/10000000000000]\n",
      "\tTraining Loss: 0.753637\n",
      "\tTesting Loss: 0.790674\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2404/10000000000000]\n",
      "\tTraining Loss: 0.753579\n",
      "\tTesting Loss: 0.791124\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2405/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.790210\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2406/10000000000000]\n",
      "\tTraining Loss: 0.753727\n",
      "\tTesting Loss: 0.790995\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2407/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.790630\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2408/10000000000000]\n",
      "\tTraining Loss: 0.753637\n",
      "\tTesting Loss: 0.790177\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2409/10000000000000]\n",
      "\tTraining Loss: 0.753636\n",
      "\tTesting Loss: 0.791538\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2410/10000000000000]\n",
      "\tTraining Loss: 0.753676\n",
      "\tTesting Loss: 0.790671\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2411/10000000000000]\n",
      "\tTraining Loss: 0.753693\n",
      "\tTesting Loss: 0.791531\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2412/10000000000000]\n",
      "\tTraining Loss: 0.753676\n",
      "\tTesting Loss: 0.791075\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2413/10000000000000]\n",
      "\tTraining Loss: 0.753706\n",
      "\tTesting Loss: 0.791120\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2414/10000000000000]\n",
      "\tTraining Loss: 0.753659\n",
      "\tTesting Loss: 0.791130\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2415/10000000000000]\n",
      "\tTraining Loss: 0.753661\n",
      "\tTesting Loss: 0.791218\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2416/10000000000000]\n",
      "\tTraining Loss: 0.753693\n",
      "\tTesting Loss: 0.790855\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2417/10000000000000]\n",
      "\tTraining Loss: 0.753676\n",
      "\tTesting Loss: 0.790825\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2418/10000000000000]\n",
      "\tTraining Loss: 0.753634\n",
      "\tTesting Loss: 0.791086\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2419/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.790640\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2420/10000000000000]\n",
      "\tTraining Loss: 0.753652\n",
      "\tTesting Loss: 0.790456\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2421/10000000000000]\n",
      "\tTraining Loss: 0.753679\n",
      "\tTesting Loss: 0.791185\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2422/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.791253\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2423/10000000000000]\n",
      "\tTraining Loss: 0.753755\n",
      "\tTesting Loss: 0.791406\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2424/10000000000000]\n",
      "\tTraining Loss: 0.753661\n",
      "\tTesting Loss: 0.791151\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2425/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.791357\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2426/10000000000000]\n",
      "\tTraining Loss: 0.753684\n",
      "\tTesting Loss: 0.790757\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2427/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.790820\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2428/10000000000000]\n",
      "\tTraining Loss: 0.753690\n",
      "\tTesting Loss: 0.791108\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2429/10000000000000]\n",
      "\tTraining Loss: 0.753684\n",
      "\tTesting Loss: 0.790254\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2430/10000000000000]\n",
      "\tTraining Loss: 0.753611\n",
      "\tTesting Loss: 0.791439\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2431/10000000000000]\n",
      "\tTraining Loss: 0.753668\n",
      "\tTesting Loss: 0.791109\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2432/10000000000000]\n",
      "\tTraining Loss: 0.753697\n",
      "\tTesting Loss: 0.791347\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2433/10000000000000]\n",
      "\tTraining Loss: 0.753679\n",
      "\tTesting Loss: 0.789995\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2434/10000000000000]\n",
      "\tTraining Loss: 0.753736\n",
      "\tTesting Loss: 0.791479\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2435/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.791353\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2436/10000000000000]\n",
      "\tTraining Loss: 0.753644\n",
      "\tTesting Loss: 0.790082\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2437/10000000000000]\n",
      "\tTraining Loss: 0.753670\n",
      "\tTesting Loss: 0.791387\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2438/10000000000000]\n",
      "\tTraining Loss: 0.753639\n",
      "\tTesting Loss: 0.791090\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2439/10000000000000]\n",
      "\tTraining Loss: 0.753660\n",
      "\tTesting Loss: 0.790935\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2440/10000000000000]\n",
      "\tTraining Loss: 0.753659\n",
      "\tTesting Loss: 0.790763\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2441/10000000000000]\n",
      "\tTraining Loss: 0.753661\n",
      "\tTesting Loss: 0.791248\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2442/10000000000000]\n",
      "\tTraining Loss: 0.753701\n",
      "\tTesting Loss: 0.790740\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2443/10000000000000]\n",
      "\tTraining Loss: 0.753585\n",
      "\tTesting Loss: 0.791535\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2444/10000000000000]\n",
      "\tTraining Loss: 0.753694\n",
      "\tTesting Loss: 0.791528\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2445/10000000000000]\n",
      "\tTraining Loss: 0.753660\n",
      "\tTesting Loss: 0.790507\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2446/10000000000000]\n",
      "\tTraining Loss: 0.753623\n",
      "\tTesting Loss: 0.791273\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2447/10000000000000]\n",
      "\tTraining Loss: 0.753700\n",
      "\tTesting Loss: 0.791644\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2448/10000000000000]\n",
      "\tTraining Loss: 0.753627\n",
      "\tTesting Loss: 0.791069\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2449/10000000000000]\n",
      "\tTraining Loss: 0.753640\n",
      "\tTesting Loss: 0.791375\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2450/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.791034\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2451/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.791383\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2452/10000000000000]\n",
      "\tTraining Loss: 0.753639\n",
      "\tTesting Loss: 0.791821\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2453/10000000000000]\n",
      "\tTraining Loss: 0.753588\n",
      "\tTesting Loss: 0.791163\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2454/10000000000000]\n",
      "\tTraining Loss: 0.753718\n",
      "\tTesting Loss: 0.789822\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2455/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.790529\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2456/10000000000000]\n",
      "\tTraining Loss: 0.753633\n",
      "\tTesting Loss: 0.791271\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2457/10000000000000]\n",
      "\tTraining Loss: 0.753691\n",
      "\tTesting Loss: 0.790916\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2458/10000000000000]\n",
      "\tTraining Loss: 0.753632\n",
      "\tTesting Loss: 0.790160\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2459/10000000000000]\n",
      "\tTraining Loss: 0.753705\n",
      "\tTesting Loss: 0.791160\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2460/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.791229\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2461/10000000000000]\n",
      "\tTraining Loss: 0.753691\n",
      "\tTesting Loss: 0.791229\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2462/10000000000000]\n",
      "\tTraining Loss: 0.753721\n",
      "\tTesting Loss: 0.791094\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2463/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.791159\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2464/10000000000000]\n",
      "\tTraining Loss: 0.753676\n",
      "\tTesting Loss: 0.789815\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2465/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.791486\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2466/10000000000000]\n",
      "\tTraining Loss: 0.753644\n",
      "\tTesting Loss: 0.791309\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2467/10000000000000]\n",
      "\tTraining Loss: 0.753662\n",
      "\tTesting Loss: 0.791056\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2468/10000000000000]\n",
      "\tTraining Loss: 0.753702\n",
      "\tTesting Loss: 0.790923\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2469/10000000000000]\n",
      "\tTraining Loss: 0.753635\n",
      "\tTesting Loss: 0.790915\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2470/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.790921\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2471/10000000000000]\n",
      "\tTraining Loss: 0.753606\n",
      "\tTesting Loss: 0.790814\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2472/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.790743\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2473/10000000000000]\n",
      "\tTraining Loss: 0.753697\n",
      "\tTesting Loss: 0.790839\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2474/10000000000000]\n",
      "\tTraining Loss: 0.753635\n",
      "\tTesting Loss: 0.791314\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2475/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.790736\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2476/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.792398\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2477/10000000000000]\n",
      "\tTraining Loss: 0.753707\n",
      "\tTesting Loss: 0.790531\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2478/10000000000000]\n",
      "\tTraining Loss: 0.753645\n",
      "\tTesting Loss: 0.791117\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2479/10000000000000]\n",
      "\tTraining Loss: 0.753697\n",
      "\tTesting Loss: 0.791117\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2480/10000000000000]\n",
      "\tTraining Loss: 0.753653\n",
      "\tTesting Loss: 0.790586\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2481/10000000000000]\n",
      "\tTraining Loss: 0.753677\n",
      "\tTesting Loss: 0.791255\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2482/10000000000000]\n",
      "\tTraining Loss: 0.753633\n",
      "\tTesting Loss: 0.790586\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2483/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.790770\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2484/10000000000000]\n",
      "\tTraining Loss: 0.753618\n",
      "\tTesting Loss: 0.791590\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2485/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.791645\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2486/10000000000000]\n",
      "\tTraining Loss: 0.753631\n",
      "\tTesting Loss: 0.791660\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2487/10000000000000]\n",
      "\tTraining Loss: 0.753712\n",
      "\tTesting Loss: 0.790992\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2488/10000000000000]\n",
      "\tTraining Loss: 0.753654\n",
      "\tTesting Loss: 0.790829\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2489/10000000000000]\n",
      "\tTraining Loss: 0.753752\n",
      "\tTesting Loss: 0.791153\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2490/10000000000000]\n",
      "\tTraining Loss: 0.753620\n",
      "\tTesting Loss: 0.791198\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2491/10000000000000]\n",
      "\tTraining Loss: 0.753704\n",
      "\tTesting Loss: 0.790652\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2492/10000000000000]\n",
      "\tTraining Loss: 0.753617\n",
      "\tTesting Loss: 0.791519\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2493/10000000000000]\n",
      "\tTraining Loss: 0.753668\n",
      "\tTesting Loss: 0.790918\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2494/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.790488\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2495/10000000000000]\n",
      "\tTraining Loss: 0.753605\n",
      "\tTesting Loss: 0.790601\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2496/10000000000000]\n",
      "\tTraining Loss: 0.753629\n",
      "\tTesting Loss: 0.790882\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2497/10000000000000]\n",
      "\tTraining Loss: 0.753632\n",
      "\tTesting Loss: 0.790973\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2498/10000000000000]\n",
      "\tTraining Loss: 0.753636\n",
      "\tTesting Loss: 0.790421\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2499/10000000000000]\n",
      "\tTraining Loss: 0.753690\n",
      "\tTesting Loss: 0.790683\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2500/10000000000000]\n",
      "\tTraining Loss: 0.753602\n",
      "\tTesting Loss: 0.791060\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2501/10000000000000]\n",
      "\tTraining Loss: 0.753693\n",
      "\tTesting Loss: 0.791462\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2502/10000000000000]\n",
      "\tTraining Loss: 0.753676\n",
      "\tTesting Loss: 0.791871\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2503/10000000000000]\n",
      "\tTraining Loss: 0.753689\n",
      "\tTesting Loss: 0.790808\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2504/10000000000000]\n",
      "\tTraining Loss: 0.753706\n",
      "\tTesting Loss: 0.791323\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2505/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.791228\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2506/10000000000000]\n",
      "\tTraining Loss: 0.753641\n",
      "\tTesting Loss: 0.791079\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2507/10000000000000]\n",
      "\tTraining Loss: 0.753634\n",
      "\tTesting Loss: 0.790009\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2508/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.790973\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2509/10000000000000]\n",
      "\tTraining Loss: 0.753662\n",
      "\tTesting Loss: 0.790508\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2510/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.791276\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2511/10000000000000]\n",
      "\tTraining Loss: 0.753642\n",
      "\tTesting Loss: 0.791395\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2512/10000000000000]\n",
      "\tTraining Loss: 0.753696\n",
      "\tTesting Loss: 0.790887\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2513/10000000000000]\n",
      "\tTraining Loss: 0.753717\n",
      "\tTesting Loss: 0.791353\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2514/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.790151\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2515/10000000000000]\n",
      "\tTraining Loss: 0.753699\n",
      "\tTesting Loss: 0.790931\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2516/10000000000000]\n",
      "\tTraining Loss: 0.753659\n",
      "\tTesting Loss: 0.790376\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2517/10000000000000]\n",
      "\tTraining Loss: 0.753685\n",
      "\tTesting Loss: 0.790942\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2518/10000000000000]\n",
      "\tTraining Loss: 0.753629\n",
      "\tTesting Loss: 0.791765\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2519/10000000000000]\n",
      "\tTraining Loss: 0.753709\n",
      "\tTesting Loss: 0.789988\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2520/10000000000000]\n",
      "\tTraining Loss: 0.753705\n",
      "\tTesting Loss: 0.790715\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2521/10000000000000]\n",
      "\tTraining Loss: 0.753679\n",
      "\tTesting Loss: 0.790729\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2522/10000000000000]\n",
      "\tTraining Loss: 0.753722\n",
      "\tTesting Loss: 0.791757\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2523/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.791345\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2524/10000000000000]\n",
      "\tTraining Loss: 0.753653\n",
      "\tTesting Loss: 0.791018\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2525/10000000000000]\n",
      "\tTraining Loss: 0.753643\n",
      "\tTesting Loss: 0.791073\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2526/10000000000000]\n",
      "\tTraining Loss: 0.753689\n",
      "\tTesting Loss: 0.790848\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2527/10000000000000]\n",
      "\tTraining Loss: 0.753680\n",
      "\tTesting Loss: 0.791399\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2528/10000000000000]\n",
      "\tTraining Loss: 0.753617\n",
      "\tTesting Loss: 0.791475\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2529/10000000000000]\n",
      "\tTraining Loss: 0.753713\n",
      "\tTesting Loss: 0.791237\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2530/10000000000000]\n",
      "\tTraining Loss: 0.753591\n",
      "\tTesting Loss: 0.790444\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2531/10000000000000]\n",
      "\tTraining Loss: 0.753700\n",
      "\tTesting Loss: 0.790675\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2532/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.791396\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2533/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.790730\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2534/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.790671\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2535/10000000000000]\n",
      "\tTraining Loss: 0.753681\n",
      "\tTesting Loss: 0.790537\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2536/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.791701\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2537/10000000000000]\n",
      "\tTraining Loss: 0.753681\n",
      "\tTesting Loss: 0.790278\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2538/10000000000000]\n",
      "\tTraining Loss: 0.753712\n",
      "\tTesting Loss: 0.790874\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2539/10000000000000]\n",
      "\tTraining Loss: 0.753657\n",
      "\tTesting Loss: 0.790559\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2540/10000000000000]\n",
      "\tTraining Loss: 0.753644\n",
      "\tTesting Loss: 0.790716\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2541/10000000000000]\n",
      "\tTraining Loss: 0.753704\n",
      "\tTesting Loss: 0.791026\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2542/10000000000000]\n",
      "\tTraining Loss: 0.753662\n",
      "\tTesting Loss: 0.790481\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2543/10000000000000]\n",
      "\tTraining Loss: 0.753604\n",
      "\tTesting Loss: 0.791374\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2544/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.790343\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2545/10000000000000]\n",
      "\tTraining Loss: 0.753680\n",
      "\tTesting Loss: 0.791536\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2546/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.790827\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2547/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.791523\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2548/10000000000000]\n",
      "\tTraining Loss: 0.753699\n",
      "\tTesting Loss: 0.790436\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2549/10000000000000]\n",
      "\tTraining Loss: 0.753636\n",
      "\tTesting Loss: 0.791717\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2550/10000000000000]\n",
      "\tTraining Loss: 0.753700\n",
      "\tTesting Loss: 0.791428\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2551/10000000000000]\n",
      "\tTraining Loss: 0.753713\n",
      "\tTesting Loss: 0.790439\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2552/10000000000000]\n",
      "\tTraining Loss: 0.753687\n",
      "\tTesting Loss: 0.790911\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2553/10000000000000]\n",
      "\tTraining Loss: 0.753631\n",
      "\tTesting Loss: 0.790499\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2554/10000000000000]\n",
      "\tTraining Loss: 0.753687\n",
      "\tTesting Loss: 0.791210\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2555/10000000000000]\n",
      "\tTraining Loss: 0.753709\n",
      "\tTesting Loss: 0.791095\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2556/10000000000000]\n",
      "\tTraining Loss: 0.753660\n",
      "\tTesting Loss: 0.791321\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2557/10000000000000]\n",
      "\tTraining Loss: 0.753700\n",
      "\tTesting Loss: 0.791390\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2558/10000000000000]\n",
      "\tTraining Loss: 0.753713\n",
      "\tTesting Loss: 0.791100\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2559/10000000000000]\n",
      "\tTraining Loss: 0.753704\n",
      "\tTesting Loss: 0.791365\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2560/10000000000000]\n",
      "\tTraining Loss: 0.753635\n",
      "\tTesting Loss: 0.791219\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2561/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.790933\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2562/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.790958\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2563/10000000000000]\n",
      "\tTraining Loss: 0.753626\n",
      "\tTesting Loss: 0.791099\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2564/10000000000000]\n",
      "\tTraining Loss: 0.753669\n",
      "\tTesting Loss: 0.791562\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2565/10000000000000]\n",
      "\tTraining Loss: 0.753717\n",
      "\tTesting Loss: 0.792257\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2566/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.791717\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2567/10000000000000]\n",
      "\tTraining Loss: 0.753595\n",
      "\tTesting Loss: 0.791786\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2568/10000000000000]\n",
      "\tTraining Loss: 0.753634\n",
      "\tTesting Loss: 0.790463\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2569/10000000000000]\n",
      "\tTraining Loss: 0.753628\n",
      "\tTesting Loss: 0.790718\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2570/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.791355\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2571/10000000000000]\n",
      "\tTraining Loss: 0.753645\n",
      "\tTesting Loss: 0.791060\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2572/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.791061\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2573/10000000000000]\n",
      "\tTraining Loss: 0.753605\n",
      "\tTesting Loss: 0.790935\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2574/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.790453\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2575/10000000000000]\n",
      "\tTraining Loss: 0.753700\n",
      "\tTesting Loss: 0.789903\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2576/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.790753\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2577/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.790094\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2578/10000000000000]\n",
      "\tTraining Loss: 0.753648\n",
      "\tTesting Loss: 0.790157\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2579/10000000000000]\n",
      "\tTraining Loss: 0.753608\n",
      "\tTesting Loss: 0.791357\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2580/10000000000000]\n",
      "\tTraining Loss: 0.753646\n",
      "\tTesting Loss: 0.791095\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2581/10000000000000]\n",
      "\tTraining Loss: 0.753715\n",
      "\tTesting Loss: 0.791539\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2582/10000000000000]\n",
      "\tTraining Loss: 0.753666\n",
      "\tTesting Loss: 0.790790\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2583/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.790634\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2584/10000000000000]\n",
      "\tTraining Loss: 0.753715\n",
      "\tTesting Loss: 0.790679\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2585/10000000000000]\n",
      "\tTraining Loss: 0.753641\n",
      "\tTesting Loss: 0.791178\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2586/10000000000000]\n",
      "\tTraining Loss: 0.753695\n",
      "\tTesting Loss: 0.790709\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2587/10000000000000]\n",
      "\tTraining Loss: 0.753733\n",
      "\tTesting Loss: 0.789871\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2588/10000000000000]\n",
      "\tTraining Loss: 0.753608\n",
      "\tTesting Loss: 0.791660\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2589/10000000000000]\n",
      "\tTraining Loss: 0.753684\n",
      "\tTesting Loss: 0.791079\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2590/10000000000000]\n",
      "\tTraining Loss: 0.753664\n",
      "\tTesting Loss: 0.791283\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2591/10000000000000]\n",
      "\tTraining Loss: 0.753711\n",
      "\tTesting Loss: 0.791399\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2592/10000000000000]\n",
      "\tTraining Loss: 0.753719\n",
      "\tTesting Loss: 0.790940\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2593/10000000000000]\n",
      "\tTraining Loss: 0.753697\n",
      "\tTesting Loss: 0.791247\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2594/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.790577\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2595/10000000000000]\n",
      "\tTraining Loss: 0.753710\n",
      "\tTesting Loss: 0.791390\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2596/10000000000000]\n",
      "\tTraining Loss: 0.753666\n",
      "\tTesting Loss: 0.791300\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2597/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.791730\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2598/10000000000000]\n",
      "\tTraining Loss: 0.753716\n",
      "\tTesting Loss: 0.790787\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2599/10000000000000]\n",
      "\tTraining Loss: 0.753686\n",
      "\tTesting Loss: 0.791718\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2600/10000000000000]\n",
      "\tTraining Loss: 0.753654\n",
      "\tTesting Loss: 0.790911\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2601/10000000000000]\n",
      "\tTraining Loss: 0.753695\n",
      "\tTesting Loss: 0.791007\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2602/10000000000000]\n",
      "\tTraining Loss: 0.753631\n",
      "\tTesting Loss: 0.790764\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2603/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.791452\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2604/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.791578\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2605/10000000000000]\n",
      "\tTraining Loss: 0.753662\n",
      "\tTesting Loss: 0.791976\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2606/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.791154\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2607/10000000000000]\n",
      "\tTraining Loss: 0.753708\n",
      "\tTesting Loss: 0.790369\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2608/10000000000000]\n",
      "\tTraining Loss: 0.753691\n",
      "\tTesting Loss: 0.791365\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2609/10000000000000]\n",
      "\tTraining Loss: 0.753653\n",
      "\tTesting Loss: 0.791120\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2610/10000000000000]\n",
      "\tTraining Loss: 0.753668\n",
      "\tTesting Loss: 0.791166\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2611/10000000000000]\n",
      "\tTraining Loss: 0.753668\n",
      "\tTesting Loss: 0.791171\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2612/10000000000000]\n",
      "\tTraining Loss: 0.753612\n",
      "\tTesting Loss: 0.790474\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2613/10000000000000]\n",
      "\tTraining Loss: 0.753615\n",
      "\tTesting Loss: 0.791559\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2614/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.791432\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2615/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.790913\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2616/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.790607\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2617/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.790622\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2618/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.791005\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2619/10000000000000]\n",
      "\tTraining Loss: 0.753696\n",
      "\tTesting Loss: 0.790813\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2620/10000000000000]\n",
      "\tTraining Loss: 0.753612\n",
      "\tTesting Loss: 0.791298\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2621/10000000000000]\n",
      "\tTraining Loss: 0.753724\n",
      "\tTesting Loss: 0.790742\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2622/10000000000000]\n",
      "\tTraining Loss: 0.753694\n",
      "\tTesting Loss: 0.790328\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2623/10000000000000]\n",
      "\tTraining Loss: 0.753730\n",
      "\tTesting Loss: 0.791324\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2624/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.790301\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2625/10000000000000]\n",
      "\tTraining Loss: 0.753596\n",
      "\tTesting Loss: 0.790589\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2626/10000000000000]\n",
      "\tTraining Loss: 0.753638\n",
      "\tTesting Loss: 0.790857\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2627/10000000000000]\n",
      "\tTraining Loss: 0.753711\n",
      "\tTesting Loss: 0.791506\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2628/10000000000000]\n",
      "\tTraining Loss: 0.753668\n",
      "\tTesting Loss: 0.790869\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2629/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.791317\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2630/10000000000000]\n",
      "\tTraining Loss: 0.753706\n",
      "\tTesting Loss: 0.790574\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2631/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.790931\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2632/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.790820\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2633/10000000000000]\n",
      "\tTraining Loss: 0.753700\n",
      "\tTesting Loss: 0.790396\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2634/10000000000000]\n",
      "\tTraining Loss: 0.753624\n",
      "\tTesting Loss: 0.790817\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2635/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.790761\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2636/10000000000000]\n",
      "\tTraining Loss: 0.753702\n",
      "\tTesting Loss: 0.791637\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2637/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.791010\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2638/10000000000000]\n",
      "\tTraining Loss: 0.753632\n",
      "\tTesting Loss: 0.789925\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2639/10000000000000]\n",
      "\tTraining Loss: 0.753737\n",
      "\tTesting Loss: 0.791121\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2640/10000000000000]\n",
      "\tTraining Loss: 0.753657\n",
      "\tTesting Loss: 0.791633\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2641/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.791260\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2642/10000000000000]\n",
      "\tTraining Loss: 0.753633\n",
      "\tTesting Loss: 0.791520\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2643/10000000000000]\n",
      "\tTraining Loss: 0.753634\n",
      "\tTesting Loss: 0.791428\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2644/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.791618\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2645/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.791512\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2646/10000000000000]\n",
      "\tTraining Loss: 0.753684\n",
      "\tTesting Loss: 0.791162\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2647/10000000000000]\n",
      "\tTraining Loss: 0.753668\n",
      "\tTesting Loss: 0.791256\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2648/10000000000000]\n",
      "\tTraining Loss: 0.753619\n",
      "\tTesting Loss: 0.791179\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2649/10000000000000]\n",
      "\tTraining Loss: 0.753689\n",
      "\tTesting Loss: 0.790927\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2650/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.790612\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2651/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.790379\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2652/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.790765\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2653/10000000000000]\n",
      "\tTraining Loss: 0.753666\n",
      "\tTesting Loss: 0.790719\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2654/10000000000000]\n",
      "\tTraining Loss: 0.753660\n",
      "\tTesting Loss: 0.790454\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2655/10000000000000]\n",
      "\tTraining Loss: 0.753602\n",
      "\tTesting Loss: 0.791479\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2656/10000000000000]\n",
      "\tTraining Loss: 0.753668\n",
      "\tTesting Loss: 0.791277\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2657/10000000000000]\n",
      "\tTraining Loss: 0.753585\n",
      "\tTesting Loss: 0.790885\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2658/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.791000\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2659/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.791323\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2660/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.790393\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2661/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.791085\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2662/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.791010\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2663/10000000000000]\n",
      "\tTraining Loss: 0.753685\n",
      "\tTesting Loss: 0.790510\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2664/10000000000000]\n",
      "\tTraining Loss: 0.753640\n",
      "\tTesting Loss: 0.790833\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2665/10000000000000]\n",
      "\tTraining Loss: 0.753619\n",
      "\tTesting Loss: 0.791564\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2666/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.790633\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2667/10000000000000]\n",
      "\tTraining Loss: 0.753643\n",
      "\tTesting Loss: 0.790340\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2668/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.790796\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2669/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.790343\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2670/10000000000000]\n",
      "\tTraining Loss: 0.753639\n",
      "\tTesting Loss: 0.791593\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2671/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.791072\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2672/10000000000000]\n",
      "\tTraining Loss: 0.753685\n",
      "\tTesting Loss: 0.790602\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2673/10000000000000]\n",
      "\tTraining Loss: 0.753644\n",
      "\tTesting Loss: 0.791956\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2674/10000000000000]\n",
      "\tTraining Loss: 0.753716\n",
      "\tTesting Loss: 0.791580\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2675/10000000000000]\n",
      "\tTraining Loss: 0.753621\n",
      "\tTesting Loss: 0.791061\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2676/10000000000000]\n",
      "\tTraining Loss: 0.753615\n",
      "\tTesting Loss: 0.790710\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2677/10000000000000]\n",
      "\tTraining Loss: 0.753655\n",
      "\tTesting Loss: 0.790999\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2678/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.791812\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2679/10000000000000]\n",
      "\tTraining Loss: 0.753660\n",
      "\tTesting Loss: 0.791474\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2680/10000000000000]\n",
      "\tTraining Loss: 0.753643\n",
      "\tTesting Loss: 0.790186\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2681/10000000000000]\n",
      "\tTraining Loss: 0.753684\n",
      "\tTesting Loss: 0.791163\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2682/10000000000000]\n",
      "\tTraining Loss: 0.753717\n",
      "\tTesting Loss: 0.791165\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2683/10000000000000]\n",
      "\tTraining Loss: 0.753614\n",
      "\tTesting Loss: 0.790109\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2684/10000000000000]\n",
      "\tTraining Loss: 0.753620\n",
      "\tTesting Loss: 0.790646\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2685/10000000000000]\n",
      "\tTraining Loss: 0.753706\n",
      "\tTesting Loss: 0.791691\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2686/10000000000000]\n",
      "\tTraining Loss: 0.753693\n",
      "\tTesting Loss: 0.790708\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2687/10000000000000]\n",
      "\tTraining Loss: 0.753691\n",
      "\tTesting Loss: 0.790726\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2688/10000000000000]\n",
      "\tTraining Loss: 0.753635\n",
      "\tTesting Loss: 0.791857\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2689/10000000000000]\n",
      "\tTraining Loss: 0.753697\n",
      "\tTesting Loss: 0.791894\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2690/10000000000000]\n",
      "\tTraining Loss: 0.753691\n",
      "\tTesting Loss: 0.791394\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2691/10000000000000]\n",
      "\tTraining Loss: 0.753715\n",
      "\tTesting Loss: 0.790749\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2692/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.790438\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2693/10000000000000]\n",
      "\tTraining Loss: 0.753661\n",
      "\tTesting Loss: 0.790975\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2694/10000000000000]\n",
      "\tTraining Loss: 0.753708\n",
      "\tTesting Loss: 0.790764\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2695/10000000000000]\n",
      "\tTraining Loss: 0.753699\n",
      "\tTesting Loss: 0.791053\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2696/10000000000000]\n",
      "\tTraining Loss: 0.753636\n",
      "\tTesting Loss: 0.790934\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2697/10000000000000]\n",
      "\tTraining Loss: 0.753660\n",
      "\tTesting Loss: 0.791900\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2698/10000000000000]\n",
      "\tTraining Loss: 0.753689\n",
      "\tTesting Loss: 0.791129\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2699/10000000000000]\n",
      "\tTraining Loss: 0.753644\n",
      "\tTesting Loss: 0.790901\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2700/10000000000000]\n",
      "\tTraining Loss: 0.753666\n",
      "\tTesting Loss: 0.790805\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2701/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.790280\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2702/10000000000000]\n",
      "\tTraining Loss: 0.753627\n",
      "\tTesting Loss: 0.790130\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2703/10000000000000]\n",
      "\tTraining Loss: 0.753718\n",
      "\tTesting Loss: 0.791176\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2704/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.790374\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2705/10000000000000]\n",
      "\tTraining Loss: 0.753618\n",
      "\tTesting Loss: 0.791006\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2706/10000000000000]\n",
      "\tTraining Loss: 0.753681\n",
      "\tTesting Loss: 0.790866\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2707/10000000000000]\n",
      "\tTraining Loss: 0.753691\n",
      "\tTesting Loss: 0.790578\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2708/10000000000000]\n",
      "\tTraining Loss: 0.753596\n",
      "\tTesting Loss: 0.790472\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2709/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.791963\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2710/10000000000000]\n",
      "\tTraining Loss: 0.753625\n",
      "\tTesting Loss: 0.791551\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2711/10000000000000]\n",
      "\tTraining Loss: 0.753710\n",
      "\tTesting Loss: 0.791001\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2712/10000000000000]\n",
      "\tTraining Loss: 0.753612\n",
      "\tTesting Loss: 0.791169\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2713/10000000000000]\n",
      "\tTraining Loss: 0.753626\n",
      "\tTesting Loss: 0.790740\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2714/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.791475\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2715/10000000000000]\n",
      "\tTraining Loss: 0.753719\n",
      "\tTesting Loss: 0.791317\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2716/10000000000000]\n",
      "\tTraining Loss: 0.753664\n",
      "\tTesting Loss: 0.790773\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2717/10000000000000]\n",
      "\tTraining Loss: 0.753648\n",
      "\tTesting Loss: 0.790393\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2718/10000000000000]\n",
      "\tTraining Loss: 0.753619\n",
      "\tTesting Loss: 0.790371\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2719/10000000000000]\n",
      "\tTraining Loss: 0.753642\n",
      "\tTesting Loss: 0.790678\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2720/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.790746\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2721/10000000000000]\n",
      "\tTraining Loss: 0.753633\n",
      "\tTesting Loss: 0.790764\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2722/10000000000000]\n",
      "\tTraining Loss: 0.753700\n",
      "\tTesting Loss: 0.791273\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2723/10000000000000]\n",
      "\tTraining Loss: 0.753630\n",
      "\tTesting Loss: 0.791415\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2724/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.791165\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2725/10000000000000]\n",
      "\tTraining Loss: 0.753647\n",
      "\tTesting Loss: 0.791257\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2726/10000000000000]\n",
      "\tTraining Loss: 0.753639\n",
      "\tTesting Loss: 0.791328\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2727/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.790610\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2728/10000000000000]\n",
      "\tTraining Loss: 0.753666\n",
      "\tTesting Loss: 0.790486\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2729/10000000000000]\n",
      "\tTraining Loss: 0.753566\n",
      "\tTesting Loss: 0.791397\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2730/10000000000000]\n",
      "\tTraining Loss: 0.753655\n",
      "\tTesting Loss: 0.791055\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2731/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.791392\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2732/10000000000000]\n",
      "\tTraining Loss: 0.753654\n",
      "\tTesting Loss: 0.791310\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2733/10000000000000]\n",
      "\tTraining Loss: 0.753616\n",
      "\tTesting Loss: 0.790897\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2734/10000000000000]\n",
      "\tTraining Loss: 0.753677\n",
      "\tTesting Loss: 0.791413\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2735/10000000000000]\n",
      "\tTraining Loss: 0.753635\n",
      "\tTesting Loss: 0.790951\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2736/10000000000000]\n",
      "\tTraining Loss: 0.753645\n",
      "\tTesting Loss: 0.790236\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2737/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.790587\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2738/10000000000000]\n",
      "\tTraining Loss: 0.753592\n",
      "\tTesting Loss: 0.791506\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2739/10000000000000]\n",
      "\tTraining Loss: 0.753647\n",
      "\tTesting Loss: 0.790865\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2740/10000000000000]\n",
      "\tTraining Loss: 0.753641\n",
      "\tTesting Loss: 0.791799\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2741/10000000000000]\n",
      "\tTraining Loss: 0.753677\n",
      "\tTesting Loss: 0.790619\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2742/10000000000000]\n",
      "\tTraining Loss: 0.753666\n",
      "\tTesting Loss: 0.790534\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2743/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.791137\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2744/10000000000000]\n",
      "\tTraining Loss: 0.753699\n",
      "\tTesting Loss: 0.790457\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2745/10000000000000]\n",
      "\tTraining Loss: 0.753654\n",
      "\tTesting Loss: 0.789736\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2746/10000000000000]\n",
      "\tTraining Loss: 0.753689\n",
      "\tTesting Loss: 0.790794\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2747/10000000000000]\n",
      "\tTraining Loss: 0.753636\n",
      "\tTesting Loss: 0.790720\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2748/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.791248\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2749/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.791553\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2750/10000000000000]\n",
      "\tTraining Loss: 0.753635\n",
      "\tTesting Loss: 0.790488\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2751/10000000000000]\n",
      "\tTraining Loss: 0.753644\n",
      "\tTesting Loss: 0.791048\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2752/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.791064\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2753/10000000000000]\n",
      "\tTraining Loss: 0.753687\n",
      "\tTesting Loss: 0.791538\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2754/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.790453\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2755/10000000000000]\n",
      "\tTraining Loss: 0.753661\n",
      "\tTesting Loss: 0.791020\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2756/10000000000000]\n",
      "\tTraining Loss: 0.753697\n",
      "\tTesting Loss: 0.790672\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2757/10000000000000]\n",
      "\tTraining Loss: 0.753661\n",
      "\tTesting Loss: 0.790824\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2758/10000000000000]\n",
      "\tTraining Loss: 0.753696\n",
      "\tTesting Loss: 0.790056\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2759/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.791266\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2760/10000000000000]\n",
      "\tTraining Loss: 0.753616\n",
      "\tTesting Loss: 0.790972\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2761/10000000000000]\n",
      "\tTraining Loss: 0.753733\n",
      "\tTesting Loss: 0.791189\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2762/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.790619\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2763/10000000000000]\n",
      "\tTraining Loss: 0.753636\n",
      "\tTesting Loss: 0.790441\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2764/10000000000000]\n",
      "\tTraining Loss: 0.753640\n",
      "\tTesting Loss: 0.790681\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2765/10000000000000]\n",
      "\tTraining Loss: 0.753666\n",
      "\tTesting Loss: 0.790768\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2766/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.790331\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2767/10000000000000]\n",
      "\tTraining Loss: 0.753589\n",
      "\tTesting Loss: 0.790596\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2768/10000000000000]\n",
      "\tTraining Loss: 0.753635\n",
      "\tTesting Loss: 0.791408\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2769/10000000000000]\n",
      "\tTraining Loss: 0.753641\n",
      "\tTesting Loss: 0.790646\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2770/10000000000000]\n",
      "\tTraining Loss: 0.753669\n",
      "\tTesting Loss: 0.790889\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2771/10000000000000]\n",
      "\tTraining Loss: 0.753674\n",
      "\tTesting Loss: 0.790104\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2772/10000000000000]\n",
      "\tTraining Loss: 0.753643\n",
      "\tTesting Loss: 0.790578\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2773/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.791090\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2774/10000000000000]\n",
      "\tTraining Loss: 0.753677\n",
      "\tTesting Loss: 0.791231\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2775/10000000000000]\n",
      "\tTraining Loss: 0.753615\n",
      "\tTesting Loss: 0.791211\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2776/10000000000000]\n",
      "\tTraining Loss: 0.753680\n",
      "\tTesting Loss: 0.790910\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2777/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.791249\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2778/10000000000000]\n",
      "\tTraining Loss: 0.753636\n",
      "\tTesting Loss: 0.791720\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2779/10000000000000]\n",
      "\tTraining Loss: 0.753664\n",
      "\tTesting Loss: 0.791366\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2780/10000000000000]\n",
      "\tTraining Loss: 0.753655\n",
      "\tTesting Loss: 0.791442\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2781/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.791674\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2782/10000000000000]\n",
      "\tTraining Loss: 0.753644\n",
      "\tTesting Loss: 0.790698\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2783/10000000000000]\n",
      "\tTraining Loss: 0.753659\n",
      "\tTesting Loss: 0.791540\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2784/10000000000000]\n",
      "\tTraining Loss: 0.753710\n",
      "\tTesting Loss: 0.791611\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2785/10000000000000]\n",
      "\tTraining Loss: 0.753643\n",
      "\tTesting Loss: 0.790607\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2786/10000000000000]\n",
      "\tTraining Loss: 0.753689\n",
      "\tTesting Loss: 0.790993\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2787/10000000000000]\n",
      "\tTraining Loss: 0.753625\n",
      "\tTesting Loss: 0.790914\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2788/10000000000000]\n",
      "\tTraining Loss: 0.753693\n",
      "\tTesting Loss: 0.790809\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2789/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.790667\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2790/10000000000000]\n",
      "\tTraining Loss: 0.753605\n",
      "\tTesting Loss: 0.791525\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2791/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.792059\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2792/10000000000000]\n",
      "\tTraining Loss: 0.753710\n",
      "\tTesting Loss: 0.790413\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2793/10000000000000]\n",
      "\tTraining Loss: 0.753681\n",
      "\tTesting Loss: 0.791024\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2794/10000000000000]\n",
      "\tTraining Loss: 0.753641\n",
      "\tTesting Loss: 0.790533\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2795/10000000000000]\n",
      "\tTraining Loss: 0.753646\n",
      "\tTesting Loss: 0.790193\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2796/10000000000000]\n",
      "\tTraining Loss: 0.753614\n",
      "\tTesting Loss: 0.791478\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2797/10000000000000]\n",
      "\tTraining Loss: 0.753636\n",
      "\tTesting Loss: 0.790394\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2798/10000000000000]\n",
      "\tTraining Loss: 0.753674\n",
      "\tTesting Loss: 0.790734\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2799/10000000000000]\n",
      "\tTraining Loss: 0.753715\n",
      "\tTesting Loss: 0.790856\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2800/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.791459\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2801/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.790215\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2802/10000000000000]\n",
      "\tTraining Loss: 0.753609\n",
      "\tTesting Loss: 0.791048\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2803/10000000000000]\n",
      "\tTraining Loss: 0.753693\n",
      "\tTesting Loss: 0.791007\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2804/10000000000000]\n",
      "\tTraining Loss: 0.753679\n",
      "\tTesting Loss: 0.790957\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2805/10000000000000]\n",
      "\tTraining Loss: 0.753648\n",
      "\tTesting Loss: 0.791222\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2806/10000000000000]\n",
      "\tTraining Loss: 0.753677\n",
      "\tTesting Loss: 0.791838\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2807/10000000000000]\n",
      "\tTraining Loss: 0.753632\n",
      "\tTesting Loss: 0.791134\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2808/10000000000000]\n",
      "\tTraining Loss: 0.753616\n",
      "\tTesting Loss: 0.790353\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2809/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.790837\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2810/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.791585\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2811/10000000000000]\n",
      "\tTraining Loss: 0.753718\n",
      "\tTesting Loss: 0.791774\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2812/10000000000000]\n",
      "\tTraining Loss: 0.753611\n",
      "\tTesting Loss: 0.790627\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2813/10000000000000]\n",
      "\tTraining Loss: 0.753661\n",
      "\tTesting Loss: 0.790339\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2814/10000000000000]\n",
      "\tTraining Loss: 0.753680\n",
      "\tTesting Loss: 0.791243\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2815/10000000000000]\n",
      "\tTraining Loss: 0.753699\n",
      "\tTesting Loss: 0.790248\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2816/10000000000000]\n",
      "\tTraining Loss: 0.753716\n",
      "\tTesting Loss: 0.791055\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2817/10000000000000]\n",
      "\tTraining Loss: 0.753670\n",
      "\tTesting Loss: 0.791201\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2818/10000000000000]\n",
      "\tTraining Loss: 0.753614\n",
      "\tTesting Loss: 0.791095\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2819/10000000000000]\n",
      "\tTraining Loss: 0.753657\n",
      "\tTesting Loss: 0.791329\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2820/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.791555\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2821/10000000000000]\n",
      "\tTraining Loss: 0.753638\n",
      "\tTesting Loss: 0.791182\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2822/10000000000000]\n",
      "\tTraining Loss: 0.753642\n",
      "\tTesting Loss: 0.790196\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2823/10000000000000]\n",
      "\tTraining Loss: 0.753657\n",
      "\tTesting Loss: 0.790669\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2824/10000000000000]\n",
      "\tTraining Loss: 0.753676\n",
      "\tTesting Loss: 0.791496\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2825/10000000000000]\n",
      "\tTraining Loss: 0.753648\n",
      "\tTesting Loss: 0.791341\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2826/10000000000000]\n",
      "\tTraining Loss: 0.753635\n",
      "\tTesting Loss: 0.791876\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2827/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.790800\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2828/10000000000000]\n",
      "\tTraining Loss: 0.753676\n",
      "\tTesting Loss: 0.791443\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2829/10000000000000]\n",
      "\tTraining Loss: 0.753648\n",
      "\tTesting Loss: 0.791336\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2830/10000000000000]\n",
      "\tTraining Loss: 0.753639\n",
      "\tTesting Loss: 0.791490\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2831/10000000000000]\n",
      "\tTraining Loss: 0.753670\n",
      "\tTesting Loss: 0.790520\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2832/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.792032\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2833/10000000000000]\n",
      "\tTraining Loss: 0.753687\n",
      "\tTesting Loss: 0.790686\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2834/10000000000000]\n",
      "\tTraining Loss: 0.753684\n",
      "\tTesting Loss: 0.790494\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2835/10000000000000]\n",
      "\tTraining Loss: 0.753676\n",
      "\tTesting Loss: 0.791405\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2836/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.791033\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2837/10000000000000]\n",
      "\tTraining Loss: 0.753699\n",
      "\tTesting Loss: 0.790983\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2838/10000000000000]\n",
      "\tTraining Loss: 0.753632\n",
      "\tTesting Loss: 0.790932\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2839/10000000000000]\n",
      "\tTraining Loss: 0.753630\n",
      "\tTesting Loss: 0.791252\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2840/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.791612\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2841/10000000000000]\n",
      "\tTraining Loss: 0.753661\n",
      "\tTesting Loss: 0.790751\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2842/10000000000000]\n",
      "\tTraining Loss: 0.753603\n",
      "\tTesting Loss: 0.791070\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2843/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.791574\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2844/10000000000000]\n",
      "\tTraining Loss: 0.753652\n",
      "\tTesting Loss: 0.790674\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2845/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.791565\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2846/10000000000000]\n",
      "\tTraining Loss: 0.753613\n",
      "\tTesting Loss: 0.790901\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2847/10000000000000]\n",
      "\tTraining Loss: 0.753695\n",
      "\tTesting Loss: 0.790947\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2848/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.791229\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2849/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.791549\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2850/10000000000000]\n",
      "\tTraining Loss: 0.753674\n",
      "\tTesting Loss: 0.791265\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2851/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.790456\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2852/10000000000000]\n",
      "\tTraining Loss: 0.753606\n",
      "\tTesting Loss: 0.791169\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2853/10000000000000]\n",
      "\tTraining Loss: 0.753661\n",
      "\tTesting Loss: 0.790500\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2854/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.790124\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2855/10000000000000]\n",
      "\tTraining Loss: 0.753646\n",
      "\tTesting Loss: 0.790446\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2856/10000000000000]\n",
      "\tTraining Loss: 0.753704\n",
      "\tTesting Loss: 0.791631\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2857/10000000000000]\n",
      "\tTraining Loss: 0.753664\n",
      "\tTesting Loss: 0.791182\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2858/10000000000000]\n",
      "\tTraining Loss: 0.753626\n",
      "\tTesting Loss: 0.790870\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2859/10000000000000]\n",
      "\tTraining Loss: 0.753661\n",
      "\tTesting Loss: 0.791326\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2860/10000000000000]\n",
      "\tTraining Loss: 0.753655\n",
      "\tTesting Loss: 0.790450\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2861/10000000000000]\n",
      "\tTraining Loss: 0.753668\n",
      "\tTesting Loss: 0.791001\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2862/10000000000000]\n",
      "\tTraining Loss: 0.753710\n",
      "\tTesting Loss: 0.791100\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2863/10000000000000]\n",
      "\tTraining Loss: 0.753684\n",
      "\tTesting Loss: 0.791387\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2864/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.791331\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2865/10000000000000]\n",
      "\tTraining Loss: 0.753676\n",
      "\tTesting Loss: 0.790291\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2866/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.790141\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2867/10000000000000]\n",
      "\tTraining Loss: 0.753707\n",
      "\tTesting Loss: 0.791978\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2868/10000000000000]\n",
      "\tTraining Loss: 0.753726\n",
      "\tTesting Loss: 0.791960\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2869/10000000000000]\n",
      "\tTraining Loss: 0.753655\n",
      "\tTesting Loss: 0.790924\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2870/10000000000000]\n",
      "\tTraining Loss: 0.753628\n",
      "\tTesting Loss: 0.791017\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2871/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.791886\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2872/10000000000000]\n",
      "\tTraining Loss: 0.753664\n",
      "\tTesting Loss: 0.791610\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2873/10000000000000]\n",
      "\tTraining Loss: 0.753633\n",
      "\tTesting Loss: 0.790244\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2874/10000000000000]\n",
      "\tTraining Loss: 0.753634\n",
      "\tTesting Loss: 0.790352\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2875/10000000000000]\n",
      "\tTraining Loss: 0.753693\n",
      "\tTesting Loss: 0.790551\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2876/10000000000000]\n",
      "\tTraining Loss: 0.753634\n",
      "\tTesting Loss: 0.790418\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2877/10000000000000]\n",
      "\tTraining Loss: 0.753711\n",
      "\tTesting Loss: 0.791398\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2878/10000000000000]\n",
      "\tTraining Loss: 0.753627\n",
      "\tTesting Loss: 0.790486\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2879/10000000000000]\n",
      "\tTraining Loss: 0.753652\n",
      "\tTesting Loss: 0.790960\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2880/10000000000000]\n",
      "\tTraining Loss: 0.753664\n",
      "\tTesting Loss: 0.791358\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2881/10000000000000]\n",
      "\tTraining Loss: 0.753628\n",
      "\tTesting Loss: 0.791081\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2882/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.790646\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2883/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.790980\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2884/10000000000000]\n",
      "\tTraining Loss: 0.753677\n",
      "\tTesting Loss: 0.791882\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2885/10000000000000]\n",
      "\tTraining Loss: 0.753691\n",
      "\tTesting Loss: 0.790691\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2886/10000000000000]\n",
      "\tTraining Loss: 0.753670\n",
      "\tTesting Loss: 0.790424\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2887/10000000000000]\n",
      "\tTraining Loss: 0.753679\n",
      "\tTesting Loss: 0.791423\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2888/10000000000000]\n",
      "\tTraining Loss: 0.753645\n",
      "\tTesting Loss: 0.791615\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2889/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.791393\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2890/10000000000000]\n",
      "\tTraining Loss: 0.753704\n",
      "\tTesting Loss: 0.789802\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2891/10000000000000]\n",
      "\tTraining Loss: 0.753677\n",
      "\tTesting Loss: 0.790982\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2892/10000000000000]\n",
      "\tTraining Loss: 0.753684\n",
      "\tTesting Loss: 0.790700\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2893/10000000000000]\n",
      "\tTraining Loss: 0.753644\n",
      "\tTesting Loss: 0.790961\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2894/10000000000000]\n",
      "\tTraining Loss: 0.753630\n",
      "\tTesting Loss: 0.790754\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2895/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.791470\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2896/10000000000000]\n",
      "\tTraining Loss: 0.753669\n",
      "\tTesting Loss: 0.791124\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2897/10000000000000]\n",
      "\tTraining Loss: 0.753612\n",
      "\tTesting Loss: 0.790740\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2898/10000000000000]\n",
      "\tTraining Loss: 0.753703\n",
      "\tTesting Loss: 0.790880\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2899/10000000000000]\n",
      "\tTraining Loss: 0.753666\n",
      "\tTesting Loss: 0.790936\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2900/10000000000000]\n",
      "\tTraining Loss: 0.753664\n",
      "\tTesting Loss: 0.791033\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2901/10000000000000]\n",
      "\tTraining Loss: 0.753606\n",
      "\tTesting Loss: 0.791622\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2902/10000000000000]\n",
      "\tTraining Loss: 0.753646\n",
      "\tTesting Loss: 0.790952\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2903/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.790607\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2904/10000000000000]\n",
      "\tTraining Loss: 0.753639\n",
      "\tTesting Loss: 0.791599\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2905/10000000000000]\n",
      "\tTraining Loss: 0.753686\n",
      "\tTesting Loss: 0.791233\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2906/10000000000000]\n",
      "\tTraining Loss: 0.753669\n",
      "\tTesting Loss: 0.789891\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2907/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.791354\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2908/10000000000000]\n",
      "\tTraining Loss: 0.753646\n",
      "\tTesting Loss: 0.791081\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2909/10000000000000]\n",
      "\tTraining Loss: 0.753679\n",
      "\tTesting Loss: 0.791362\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2910/10000000000000]\n",
      "\tTraining Loss: 0.753669\n",
      "\tTesting Loss: 0.790827\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2911/10000000000000]\n",
      "\tTraining Loss: 0.753709\n",
      "\tTesting Loss: 0.791340\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2912/10000000000000]\n",
      "\tTraining Loss: 0.753643\n",
      "\tTesting Loss: 0.791555\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2913/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.790639\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2914/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.791535\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2915/10000000000000]\n",
      "\tTraining Loss: 0.753620\n",
      "\tTesting Loss: 0.790919\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2916/10000000000000]\n",
      "\tTraining Loss: 0.753647\n",
      "\tTesting Loss: 0.791246\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2917/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.791139\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2918/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.791133\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2919/10000000000000]\n",
      "\tTraining Loss: 0.753724\n",
      "\tTesting Loss: 0.791426\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2920/10000000000000]\n",
      "\tTraining Loss: 0.753693\n",
      "\tTesting Loss: 0.790144\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2921/10000000000000]\n",
      "\tTraining Loss: 0.753626\n",
      "\tTesting Loss: 0.791538\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2922/10000000000000]\n",
      "\tTraining Loss: 0.753712\n",
      "\tTesting Loss: 0.790510\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2923/10000000000000]\n",
      "\tTraining Loss: 0.753674\n",
      "\tTesting Loss: 0.792625\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2924/10000000000000]\n",
      "\tTraining Loss: 0.753646\n",
      "\tTesting Loss: 0.790752\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2925/10000000000000]\n",
      "\tTraining Loss: 0.753604\n",
      "\tTesting Loss: 0.791438\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2926/10000000000000]\n",
      "\tTraining Loss: 0.753687\n",
      "\tTesting Loss: 0.790368\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2927/10000000000000]\n",
      "\tTraining Loss: 0.753640\n",
      "\tTesting Loss: 0.790925\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2928/10000000000000]\n",
      "\tTraining Loss: 0.753680\n",
      "\tTesting Loss: 0.790896\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2929/10000000000000]\n",
      "\tTraining Loss: 0.753634\n",
      "\tTesting Loss: 0.790417\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2930/10000000000000]\n",
      "\tTraining Loss: 0.753674\n",
      "\tTesting Loss: 0.790886\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2931/10000000000000]\n",
      "\tTraining Loss: 0.753629\n",
      "\tTesting Loss: 0.791271\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2932/10000000000000]\n",
      "\tTraining Loss: 0.753686\n",
      "\tTesting Loss: 0.791251\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2933/10000000000000]\n",
      "\tTraining Loss: 0.753718\n",
      "\tTesting Loss: 0.790946\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2934/10000000000000]\n",
      "\tTraining Loss: 0.753710\n",
      "\tTesting Loss: 0.790818\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2935/10000000000000]\n",
      "\tTraining Loss: 0.753653\n",
      "\tTesting Loss: 0.791607\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2936/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.791367\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2937/10000000000000]\n",
      "\tTraining Loss: 0.753684\n",
      "\tTesting Loss: 0.789521\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2938/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.791602\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2939/10000000000000]\n",
      "\tTraining Loss: 0.753642\n",
      "\tTesting Loss: 0.791565\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2940/10000000000000]\n",
      "\tTraining Loss: 0.753612\n",
      "\tTesting Loss: 0.791619\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2941/10000000000000]\n",
      "\tTraining Loss: 0.753640\n",
      "\tTesting Loss: 0.789971\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2942/10000000000000]\n",
      "\tTraining Loss: 0.753641\n",
      "\tTesting Loss: 0.791360\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2943/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.790663\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2944/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.789314\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2945/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.790926\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2946/10000000000000]\n",
      "\tTraining Loss: 0.753623\n",
      "\tTesting Loss: 0.791255\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2947/10000000000000]\n",
      "\tTraining Loss: 0.753635\n",
      "\tTesting Loss: 0.790566\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2948/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.791009\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2949/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.791126\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2950/10000000000000]\n",
      "\tTraining Loss: 0.753679\n",
      "\tTesting Loss: 0.790980\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2951/10000000000000]\n",
      "\tTraining Loss: 0.753686\n",
      "\tTesting Loss: 0.791069\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2952/10000000000000]\n",
      "\tTraining Loss: 0.753677\n",
      "\tTesting Loss: 0.790740\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2953/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.791019\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2954/10000000000000]\n",
      "\tTraining Loss: 0.753636\n",
      "\tTesting Loss: 0.790885\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2955/10000000000000]\n",
      "\tTraining Loss: 0.753697\n",
      "\tTesting Loss: 0.791488\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2956/10000000000000]\n",
      "\tTraining Loss: 0.753635\n",
      "\tTesting Loss: 0.790501\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2957/10000000000000]\n",
      "\tTraining Loss: 0.753677\n",
      "\tTesting Loss: 0.791426\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2958/10000000000000]\n",
      "\tTraining Loss: 0.753632\n",
      "\tTesting Loss: 0.791378\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2959/10000000000000]\n",
      "\tTraining Loss: 0.753641\n",
      "\tTesting Loss: 0.790559\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2960/10000000000000]\n",
      "\tTraining Loss: 0.753666\n",
      "\tTesting Loss: 0.791466\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2961/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.791505\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2962/10000000000000]\n",
      "\tTraining Loss: 0.753679\n",
      "\tTesting Loss: 0.791109\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2963/10000000000000]\n",
      "\tTraining Loss: 0.753638\n",
      "\tTesting Loss: 0.790778\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2964/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.791268\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2965/10000000000000]\n",
      "\tTraining Loss: 0.753684\n",
      "\tTesting Loss: 0.789788\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2966/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.791741\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2967/10000000000000]\n",
      "\tTraining Loss: 0.753648\n",
      "\tTesting Loss: 0.790885\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2968/10000000000000]\n",
      "\tTraining Loss: 0.753696\n",
      "\tTesting Loss: 0.791521\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2969/10000000000000]\n",
      "\tTraining Loss: 0.753674\n",
      "\tTesting Loss: 0.790926\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2970/10000000000000]\n",
      "\tTraining Loss: 0.753639\n",
      "\tTesting Loss: 0.791006\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2971/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.790455\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2972/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.791156\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2973/10000000000000]\n",
      "\tTraining Loss: 0.753587\n",
      "\tTesting Loss: 0.791360\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2974/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.790997\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2975/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.791634\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2976/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.791283\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2977/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.790648\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2978/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.791048\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2979/10000000000000]\n",
      "\tTraining Loss: 0.753653\n",
      "\tTesting Loss: 0.791639\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2980/10000000000000]\n",
      "\tTraining Loss: 0.753727\n",
      "\tTesting Loss: 0.791811\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2981/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.790468\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2982/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.791558\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2983/10000000000000]\n",
      "\tTraining Loss: 0.753696\n",
      "\tTesting Loss: 0.790483\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2984/10000000000000]\n",
      "\tTraining Loss: 0.753662\n",
      "\tTesting Loss: 0.791929\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2985/10000000000000]\n",
      "\tTraining Loss: 0.753669\n",
      "\tTesting Loss: 0.791157\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2986/10000000000000]\n",
      "\tTraining Loss: 0.753696\n",
      "\tTesting Loss: 0.791126\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2987/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.790807\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2988/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.790279\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2989/10000000000000]\n",
      "\tTraining Loss: 0.753699\n",
      "\tTesting Loss: 0.790967\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2990/10000000000000]\n",
      "\tTraining Loss: 0.753625\n",
      "\tTesting Loss: 0.790546\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2991/10000000000000]\n",
      "\tTraining Loss: 0.753615\n",
      "\tTesting Loss: 0.791312\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2992/10000000000000]\n",
      "\tTraining Loss: 0.753659\n",
      "\tTesting Loss: 0.790642\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2993/10000000000000]\n",
      "\tTraining Loss: 0.753636\n",
      "\tTesting Loss: 0.792122\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2994/10000000000000]\n",
      "\tTraining Loss: 0.753654\n",
      "\tTesting Loss: 0.790642\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2995/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.791753\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2996/10000000000000]\n",
      "\tTraining Loss: 0.753724\n",
      "\tTesting Loss: 0.790745\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2997/10000000000000]\n",
      "\tTraining Loss: 0.753639\n",
      "\tTesting Loss: 0.790677\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2998/10000000000000]\n",
      "\tTraining Loss: 0.753622\n",
      "\tTesting Loss: 0.791876\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [2999/10000000000000]\n",
      "\tTraining Loss: 0.753691\n",
      "\tTesting Loss: 0.791127\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3000/10000000000000]\n",
      "\tTraining Loss: 0.753709\n",
      "\tTesting Loss: 0.790713\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3001/10000000000000]\n",
      "\tTraining Loss: 0.753627\n",
      "\tTesting Loss: 0.790926\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3002/10000000000000]\n",
      "\tTraining Loss: 0.753643\n",
      "\tTesting Loss: 0.791044\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3003/10000000000000]\n",
      "\tTraining Loss: 0.753724\n",
      "\tTesting Loss: 0.789988\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3004/10000000000000]\n",
      "\tTraining Loss: 0.753701\n",
      "\tTesting Loss: 0.790126\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3005/10000000000000]\n",
      "\tTraining Loss: 0.753670\n",
      "\tTesting Loss: 0.790084\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3006/10000000000000]\n",
      "\tTraining Loss: 0.753619\n",
      "\tTesting Loss: 0.791448\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3007/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.791061\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3008/10000000000000]\n",
      "\tTraining Loss: 0.753654\n",
      "\tTesting Loss: 0.789699\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3009/10000000000000]\n",
      "\tTraining Loss: 0.753613\n",
      "\tTesting Loss: 0.791621\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3010/10000000000000]\n",
      "\tTraining Loss: 0.753676\n",
      "\tTesting Loss: 0.791544\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3011/10000000000000]\n",
      "\tTraining Loss: 0.753644\n",
      "\tTesting Loss: 0.790631\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3012/10000000000000]\n",
      "\tTraining Loss: 0.753657\n",
      "\tTesting Loss: 0.790322\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3013/10000000000000]\n",
      "\tTraining Loss: 0.753680\n",
      "\tTesting Loss: 0.790075\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3014/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.791172\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3015/10000000000000]\n",
      "\tTraining Loss: 0.753680\n",
      "\tTesting Loss: 0.789892\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3016/10000000000000]\n",
      "\tTraining Loss: 0.753647\n",
      "\tTesting Loss: 0.791063\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3017/10000000000000]\n",
      "\tTraining Loss: 0.753677\n",
      "\tTesting Loss: 0.791344\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3018/10000000000000]\n",
      "\tTraining Loss: 0.753631\n",
      "\tTesting Loss: 0.790502\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3019/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.791487\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3020/10000000000000]\n",
      "\tTraining Loss: 0.753645\n",
      "\tTesting Loss: 0.791858\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3021/10000000000000]\n",
      "\tTraining Loss: 0.753648\n",
      "\tTesting Loss: 0.790907\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3022/10000000000000]\n",
      "\tTraining Loss: 0.753616\n",
      "\tTesting Loss: 0.792075\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3023/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.790138\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3024/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.791039\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3025/10000000000000]\n",
      "\tTraining Loss: 0.753696\n",
      "\tTesting Loss: 0.790989\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3026/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.789749\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3027/10000000000000]\n",
      "\tTraining Loss: 0.753599\n",
      "\tTesting Loss: 0.790691\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3028/10000000000000]\n",
      "\tTraining Loss: 0.753626\n",
      "\tTesting Loss: 0.791245\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3029/10000000000000]\n",
      "\tTraining Loss: 0.753685\n",
      "\tTesting Loss: 0.790155\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3030/10000000000000]\n",
      "\tTraining Loss: 0.753717\n",
      "\tTesting Loss: 0.790331\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3031/10000000000000]\n",
      "\tTraining Loss: 0.753702\n",
      "\tTesting Loss: 0.790586\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3032/10000000000000]\n",
      "\tTraining Loss: 0.753669\n",
      "\tTesting Loss: 0.791478\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3033/10000000000000]\n",
      "\tTraining Loss: 0.753593\n",
      "\tTesting Loss: 0.792034\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3034/10000000000000]\n",
      "\tTraining Loss: 0.753699\n",
      "\tTesting Loss: 0.790379\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3035/10000000000000]\n",
      "\tTraining Loss: 0.753713\n",
      "\tTesting Loss: 0.790023\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3036/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.791360\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3037/10000000000000]\n",
      "\tTraining Loss: 0.753652\n",
      "\tTesting Loss: 0.790217\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3038/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.791025\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3039/10000000000000]\n",
      "\tTraining Loss: 0.753652\n",
      "\tTesting Loss: 0.791299\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3040/10000000000000]\n",
      "\tTraining Loss: 0.753718\n",
      "\tTesting Loss: 0.791897\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3041/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.791703\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3042/10000000000000]\n",
      "\tTraining Loss: 0.753751\n",
      "\tTesting Loss: 0.790633\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3043/10000000000000]\n",
      "\tTraining Loss: 0.753679\n",
      "\tTesting Loss: 0.790921\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3044/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.791201\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3045/10000000000000]\n",
      "\tTraining Loss: 0.753669\n",
      "\tTesting Loss: 0.791247\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3046/10000000000000]\n",
      "\tTraining Loss: 0.753647\n",
      "\tTesting Loss: 0.791437\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3047/10000000000000]\n",
      "\tTraining Loss: 0.753643\n",
      "\tTesting Loss: 0.791558\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3048/10000000000000]\n",
      "\tTraining Loss: 0.753693\n",
      "\tTesting Loss: 0.791410\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3049/10000000000000]\n",
      "\tTraining Loss: 0.753726\n",
      "\tTesting Loss: 0.791330\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3050/10000000000000]\n",
      "\tTraining Loss: 0.753586\n",
      "\tTesting Loss: 0.790648\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3051/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.790089\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3052/10000000000000]\n",
      "\tTraining Loss: 0.753632\n",
      "\tTesting Loss: 0.790079\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3053/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.791442\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3054/10000000000000]\n",
      "\tTraining Loss: 0.753635\n",
      "\tTesting Loss: 0.790946\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3055/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.791000\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3056/10000000000000]\n",
      "\tTraining Loss: 0.753684\n",
      "\tTesting Loss: 0.791428\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3057/10000000000000]\n",
      "\tTraining Loss: 0.753749\n",
      "\tTesting Loss: 0.792714\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3058/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.791284\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3059/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.791586\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3060/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.791592\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3061/10000000000000]\n",
      "\tTraining Loss: 0.753680\n",
      "\tTesting Loss: 0.791217\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3062/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.791135\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3063/10000000000000]\n",
      "\tTraining Loss: 0.753712\n",
      "\tTesting Loss: 0.792277\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3064/10000000000000]\n",
      "\tTraining Loss: 0.753646\n",
      "\tTesting Loss: 0.790785\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3065/10000000000000]\n",
      "\tTraining Loss: 0.753642\n",
      "\tTesting Loss: 0.791017\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3066/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.790035\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3067/10000000000000]\n",
      "\tTraining Loss: 0.753711\n",
      "\tTesting Loss: 0.790369\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3068/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.791300\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3069/10000000000000]\n",
      "\tTraining Loss: 0.753592\n",
      "\tTesting Loss: 0.790827\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3070/10000000000000]\n",
      "\tTraining Loss: 0.753623\n",
      "\tTesting Loss: 0.790145\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3071/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.791229\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3072/10000000000000]\n",
      "\tTraining Loss: 0.753621\n",
      "\tTesting Loss: 0.791796\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3073/10000000000000]\n",
      "\tTraining Loss: 0.753652\n",
      "\tTesting Loss: 0.791555\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3074/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.791135\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3075/10000000000000]\n",
      "\tTraining Loss: 0.753685\n",
      "\tTesting Loss: 0.789995\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3076/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.790432\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3077/10000000000000]\n",
      "\tTraining Loss: 0.753653\n",
      "\tTesting Loss: 0.790224\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3078/10000000000000]\n",
      "\tTraining Loss: 0.753602\n",
      "\tTesting Loss: 0.791228\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3079/10000000000000]\n",
      "\tTraining Loss: 0.753685\n",
      "\tTesting Loss: 0.790607\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3080/10000000000000]\n",
      "\tTraining Loss: 0.753747\n",
      "\tTesting Loss: 0.791310\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3081/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.791302\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3082/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.790753\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3083/10000000000000]\n",
      "\tTraining Loss: 0.753625\n",
      "\tTesting Loss: 0.789914\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3084/10000000000000]\n",
      "\tTraining Loss: 0.753585\n",
      "\tTesting Loss: 0.790171\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3085/10000000000000]\n",
      "\tTraining Loss: 0.753625\n",
      "\tTesting Loss: 0.791400\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3086/10000000000000]\n",
      "\tTraining Loss: 0.753631\n",
      "\tTesting Loss: 0.790454\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3087/10000000000000]\n",
      "\tTraining Loss: 0.753625\n",
      "\tTesting Loss: 0.791271\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3088/10000000000000]\n",
      "\tTraining Loss: 0.753703\n",
      "\tTesting Loss: 0.790817\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3089/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.790317\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3090/10000000000000]\n",
      "\tTraining Loss: 0.753616\n",
      "\tTesting Loss: 0.790970\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3091/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.790706\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3092/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.790647\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3093/10000000000000]\n",
      "\tTraining Loss: 0.753635\n",
      "\tTesting Loss: 0.790111\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3094/10000000000000]\n",
      "\tTraining Loss: 0.753643\n",
      "\tTesting Loss: 0.790915\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3095/10000000000000]\n",
      "\tTraining Loss: 0.753645\n",
      "\tTesting Loss: 0.791926\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3096/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.791756\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3097/10000000000000]\n",
      "\tTraining Loss: 0.753674\n",
      "\tTesting Loss: 0.791742\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3098/10000000000000]\n",
      "\tTraining Loss: 0.753630\n",
      "\tTesting Loss: 0.791442\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3099/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.790979\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3100/10000000000000]\n",
      "\tTraining Loss: 0.753627\n",
      "\tTesting Loss: 0.791516\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3101/10000000000000]\n",
      "\tTraining Loss: 0.753653\n",
      "\tTesting Loss: 0.791641\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3102/10000000000000]\n",
      "\tTraining Loss: 0.753666\n",
      "\tTesting Loss: 0.791422\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3103/10000000000000]\n",
      "\tTraining Loss: 0.753628\n",
      "\tTesting Loss: 0.790670\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3104/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.790895\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3105/10000000000000]\n",
      "\tTraining Loss: 0.753659\n",
      "\tTesting Loss: 0.790942\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3106/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.791274\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3107/10000000000000]\n",
      "\tTraining Loss: 0.753613\n",
      "\tTesting Loss: 0.791404\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3108/10000000000000]\n",
      "\tTraining Loss: 0.753689\n",
      "\tTesting Loss: 0.791732\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3109/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.790922\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3110/10000000000000]\n",
      "\tTraining Loss: 0.753694\n",
      "\tTesting Loss: 0.791167\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3111/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.790525\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3112/10000000000000]\n",
      "\tTraining Loss: 0.753646\n",
      "\tTesting Loss: 0.790818\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3113/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.791604\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3114/10000000000000]\n",
      "\tTraining Loss: 0.753657\n",
      "\tTesting Loss: 0.790848\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3115/10000000000000]\n",
      "\tTraining Loss: 0.753659\n",
      "\tTesting Loss: 0.790574\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3116/10000000000000]\n",
      "\tTraining Loss: 0.753674\n",
      "\tTesting Loss: 0.790801\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3117/10000000000000]\n",
      "\tTraining Loss: 0.753612\n",
      "\tTesting Loss: 0.791593\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3118/10000000000000]\n",
      "\tTraining Loss: 0.753630\n",
      "\tTesting Loss: 0.791896\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3119/10000000000000]\n",
      "\tTraining Loss: 0.753654\n",
      "\tTesting Loss: 0.790816\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3120/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.791093\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3121/10000000000000]\n",
      "\tTraining Loss: 0.753668\n",
      "\tTesting Loss: 0.791258\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3122/10000000000000]\n",
      "\tTraining Loss: 0.753612\n",
      "\tTesting Loss: 0.790574\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3123/10000000000000]\n",
      "\tTraining Loss: 0.753602\n",
      "\tTesting Loss: 0.791700\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3124/10000000000000]\n",
      "\tTraining Loss: 0.753634\n",
      "\tTesting Loss: 0.790184\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3125/10000000000000]\n",
      "\tTraining Loss: 0.753696\n",
      "\tTesting Loss: 0.791100\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3126/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.790482\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3127/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.791438\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3128/10000000000000]\n",
      "\tTraining Loss: 0.753685\n",
      "\tTesting Loss: 0.790801\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3129/10000000000000]\n",
      "\tTraining Loss: 0.753636\n",
      "\tTesting Loss: 0.790425\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3130/10000000000000]\n",
      "\tTraining Loss: 0.753604\n",
      "\tTesting Loss: 0.790475\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3131/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.790939\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3132/10000000000000]\n",
      "\tTraining Loss: 0.753621\n",
      "\tTesting Loss: 0.791171\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3133/10000000000000]\n",
      "\tTraining Loss: 0.753647\n",
      "\tTesting Loss: 0.791762\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3134/10000000000000]\n",
      "\tTraining Loss: 0.753644\n",
      "\tTesting Loss: 0.791404\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3135/10000000000000]\n",
      "\tTraining Loss: 0.753691\n",
      "\tTesting Loss: 0.791129\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3136/10000000000000]\n",
      "\tTraining Loss: 0.753632\n",
      "\tTesting Loss: 0.791465\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3137/10000000000000]\n",
      "\tTraining Loss: 0.753540\n",
      "\tTesting Loss: 0.790718\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3138/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.790129\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3139/10000000000000]\n",
      "\tTraining Loss: 0.753595\n",
      "\tTesting Loss: 0.791446\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3140/10000000000000]\n",
      "\tTraining Loss: 0.753646\n",
      "\tTesting Loss: 0.791461\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3141/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.790221\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3142/10000000000000]\n",
      "\tTraining Loss: 0.753684\n",
      "\tTesting Loss: 0.791526\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3143/10000000000000]\n",
      "\tTraining Loss: 0.753655\n",
      "\tTesting Loss: 0.790576\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3144/10000000000000]\n",
      "\tTraining Loss: 0.753704\n",
      "\tTesting Loss: 0.791490\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3145/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.791952\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3146/10000000000000]\n",
      "\tTraining Loss: 0.753621\n",
      "\tTesting Loss: 0.791370\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3147/10000000000000]\n",
      "\tTraining Loss: 0.753677\n",
      "\tTesting Loss: 0.790225\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3148/10000000000000]\n",
      "\tTraining Loss: 0.753631\n",
      "\tTesting Loss: 0.790616\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3149/10000000000000]\n",
      "\tTraining Loss: 0.753603\n",
      "\tTesting Loss: 0.791222\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3150/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.791291\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3151/10000000000000]\n",
      "\tTraining Loss: 0.753677\n",
      "\tTesting Loss: 0.791417\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3152/10000000000000]\n",
      "\tTraining Loss: 0.753664\n",
      "\tTesting Loss: 0.791214\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3153/10000000000000]\n",
      "\tTraining Loss: 0.753713\n",
      "\tTesting Loss: 0.790571\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3154/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.790453\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3155/10000000000000]\n",
      "\tTraining Loss: 0.753660\n",
      "\tTesting Loss: 0.791644\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3156/10000000000000]\n",
      "\tTraining Loss: 0.753666\n",
      "\tTesting Loss: 0.791542\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3157/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.790882\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3158/10000000000000]\n",
      "\tTraining Loss: 0.753630\n",
      "\tTesting Loss: 0.791390\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3159/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.791687\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3160/10000000000000]\n",
      "\tTraining Loss: 0.753644\n",
      "\tTesting Loss: 0.790279\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3161/10000000000000]\n",
      "\tTraining Loss: 0.753707\n",
      "\tTesting Loss: 0.791596\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3162/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.790991\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3163/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.790831\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3164/10000000000000]\n",
      "\tTraining Loss: 0.753702\n",
      "\tTesting Loss: 0.791664\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3165/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.791302\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3166/10000000000000]\n",
      "\tTraining Loss: 0.753630\n",
      "\tTesting Loss: 0.790324\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3167/10000000000000]\n",
      "\tTraining Loss: 0.753670\n",
      "\tTesting Loss: 0.791597\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3168/10000000000000]\n",
      "\tTraining Loss: 0.753624\n",
      "\tTesting Loss: 0.791047\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3169/10000000000000]\n",
      "\tTraining Loss: 0.753679\n",
      "\tTesting Loss: 0.790103\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3170/10000000000000]\n",
      "\tTraining Loss: 0.753707\n",
      "\tTesting Loss: 0.789569\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3171/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.790630\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3172/10000000000000]\n",
      "\tTraining Loss: 0.753634\n",
      "\tTesting Loss: 0.790805\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3173/10000000000000]\n",
      "\tTraining Loss: 0.753701\n",
      "\tTesting Loss: 0.790640\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3174/10000000000000]\n",
      "\tTraining Loss: 0.753666\n",
      "\tTesting Loss: 0.791001\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3175/10000000000000]\n",
      "\tTraining Loss: 0.753611\n",
      "\tTesting Loss: 0.791076\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3176/10000000000000]\n",
      "\tTraining Loss: 0.753636\n",
      "\tTesting Loss: 0.790760\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3177/10000000000000]\n",
      "\tTraining Loss: 0.753607\n",
      "\tTesting Loss: 0.791366\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3178/10000000000000]\n",
      "\tTraining Loss: 0.753629\n",
      "\tTesting Loss: 0.791656\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3179/10000000000000]\n",
      "\tTraining Loss: 0.753702\n",
      "\tTesting Loss: 0.791260\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3180/10000000000000]\n",
      "\tTraining Loss: 0.753612\n",
      "\tTesting Loss: 0.791023\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3181/10000000000000]\n",
      "\tTraining Loss: 0.753626\n",
      "\tTesting Loss: 0.791080\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3182/10000000000000]\n",
      "\tTraining Loss: 0.753597\n",
      "\tTesting Loss: 0.791475\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3183/10000000000000]\n",
      "\tTraining Loss: 0.753622\n",
      "\tTesting Loss: 0.791381\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3184/10000000000000]\n",
      "\tTraining Loss: 0.753647\n",
      "\tTesting Loss: 0.791420\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3185/10000000000000]\n",
      "\tTraining Loss: 0.753653\n",
      "\tTesting Loss: 0.790876\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3186/10000000000000]\n",
      "\tTraining Loss: 0.753619\n",
      "\tTesting Loss: 0.789839\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3187/10000000000000]\n",
      "\tTraining Loss: 0.753660\n",
      "\tTesting Loss: 0.791992\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3188/10000000000000]\n",
      "\tTraining Loss: 0.753615\n",
      "\tTesting Loss: 0.791284\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3189/10000000000000]\n",
      "\tTraining Loss: 0.753618\n",
      "\tTesting Loss: 0.790733\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3190/10000000000000]\n",
      "\tTraining Loss: 0.753703\n",
      "\tTesting Loss: 0.791030\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3191/10000000000000]\n",
      "\tTraining Loss: 0.753659\n",
      "\tTesting Loss: 0.791025\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3192/10000000000000]\n",
      "\tTraining Loss: 0.753653\n",
      "\tTesting Loss: 0.790937\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3193/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.791175\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3194/10000000000000]\n",
      "\tTraining Loss: 0.753593\n",
      "\tTesting Loss: 0.790790\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3195/10000000000000]\n",
      "\tTraining Loss: 0.753674\n",
      "\tTesting Loss: 0.791601\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3196/10000000000000]\n",
      "\tTraining Loss: 0.753647\n",
      "\tTesting Loss: 0.791402\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3197/10000000000000]\n",
      "\tTraining Loss: 0.753630\n",
      "\tTesting Loss: 0.791534\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3198/10000000000000]\n",
      "\tTraining Loss: 0.753694\n",
      "\tTesting Loss: 0.790488\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3199/10000000000000]\n",
      "\tTraining Loss: 0.753642\n",
      "\tTesting Loss: 0.791326\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3200/10000000000000]\n",
      "\tTraining Loss: 0.753626\n",
      "\tTesting Loss: 0.790990\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3201/10000000000000]\n",
      "\tTraining Loss: 0.753610\n",
      "\tTesting Loss: 0.791765\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3202/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.791171\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3203/10000000000000]\n",
      "\tTraining Loss: 0.753679\n",
      "\tTesting Loss: 0.790718\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3204/10000000000000]\n",
      "\tTraining Loss: 0.753605\n",
      "\tTesting Loss: 0.791451\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3205/10000000000000]\n",
      "\tTraining Loss: 0.753604\n",
      "\tTesting Loss: 0.791567\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3206/10000000000000]\n",
      "\tTraining Loss: 0.753669\n",
      "\tTesting Loss: 0.791160\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3207/10000000000000]\n",
      "\tTraining Loss: 0.753684\n",
      "\tTesting Loss: 0.790657\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3208/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.791521\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3209/10000000000000]\n",
      "\tTraining Loss: 0.753585\n",
      "\tTesting Loss: 0.791378\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3210/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.790813\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3211/10000000000000]\n",
      "\tTraining Loss: 0.753700\n",
      "\tTesting Loss: 0.791541\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3212/10000000000000]\n",
      "\tTraining Loss: 0.753633\n",
      "\tTesting Loss: 0.790625\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3213/10000000000000]\n",
      "\tTraining Loss: 0.753689\n",
      "\tTesting Loss: 0.791262\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3214/10000000000000]\n",
      "\tTraining Loss: 0.753660\n",
      "\tTesting Loss: 0.790958\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3215/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.790144\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3216/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.790751\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3217/10000000000000]\n",
      "\tTraining Loss: 0.753701\n",
      "\tTesting Loss: 0.791305\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3218/10000000000000]\n",
      "\tTraining Loss: 0.753636\n",
      "\tTesting Loss: 0.790681\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3219/10000000000000]\n",
      "\tTraining Loss: 0.753715\n",
      "\tTesting Loss: 0.790735\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3220/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.790680\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3221/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.790513\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3222/10000000000000]\n",
      "\tTraining Loss: 0.753602\n",
      "\tTesting Loss: 0.790779\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3223/10000000000000]\n",
      "\tTraining Loss: 0.753630\n",
      "\tTesting Loss: 0.790837\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3224/10000000000000]\n",
      "\tTraining Loss: 0.753574\n",
      "\tTesting Loss: 0.791114\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3225/10000000000000]\n",
      "\tTraining Loss: 0.753617\n",
      "\tTesting Loss: 0.790238\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3226/10000000000000]\n",
      "\tTraining Loss: 0.753686\n",
      "\tTesting Loss: 0.790699\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3227/10000000000000]\n",
      "\tTraining Loss: 0.753634\n",
      "\tTesting Loss: 0.791161\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3228/10000000000000]\n",
      "\tTraining Loss: 0.753589\n",
      "\tTesting Loss: 0.791066\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3229/10000000000000]\n",
      "\tTraining Loss: 0.753664\n",
      "\tTesting Loss: 0.791239\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3230/10000000000000]\n",
      "\tTraining Loss: 0.753740\n",
      "\tTesting Loss: 0.790733\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3231/10000000000000]\n",
      "\tTraining Loss: 0.753666\n",
      "\tTesting Loss: 0.791058\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3232/10000000000000]\n",
      "\tTraining Loss: 0.753696\n",
      "\tTesting Loss: 0.790872\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3233/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.790409\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3234/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.792287\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3235/10000000000000]\n",
      "\tTraining Loss: 0.753603\n",
      "\tTesting Loss: 0.791676\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3236/10000000000000]\n",
      "\tTraining Loss: 0.753660\n",
      "\tTesting Loss: 0.790746\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3237/10000000000000]\n",
      "\tTraining Loss: 0.753632\n",
      "\tTesting Loss: 0.790400\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3238/10000000000000]\n",
      "\tTraining Loss: 0.753720\n",
      "\tTesting Loss: 0.791532\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3239/10000000000000]\n",
      "\tTraining Loss: 0.753668\n",
      "\tTesting Loss: 0.790216\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3240/10000000000000]\n",
      "\tTraining Loss: 0.753653\n",
      "\tTesting Loss: 0.791669\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3241/10000000000000]\n",
      "\tTraining Loss: 0.753633\n",
      "\tTesting Loss: 0.790911\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3242/10000000000000]\n",
      "\tTraining Loss: 0.753701\n",
      "\tTesting Loss: 0.791151\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3243/10000000000000]\n",
      "\tTraining Loss: 0.753623\n",
      "\tTesting Loss: 0.791507\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3244/10000000000000]\n",
      "\tTraining Loss: 0.753611\n",
      "\tTesting Loss: 0.791933\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3245/10000000000000]\n",
      "\tTraining Loss: 0.753579\n",
      "\tTesting Loss: 0.790970\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3246/10000000000000]\n",
      "\tTraining Loss: 0.753639\n",
      "\tTesting Loss: 0.791389\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3247/10000000000000]\n",
      "\tTraining Loss: 0.753617\n",
      "\tTesting Loss: 0.791550\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3248/10000000000000]\n",
      "\tTraining Loss: 0.753633\n",
      "\tTesting Loss: 0.790695\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3249/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.791212\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3250/10000000000000]\n",
      "\tTraining Loss: 0.753641\n",
      "\tTesting Loss: 0.791005\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3251/10000000000000]\n",
      "\tTraining Loss: 0.753662\n",
      "\tTesting Loss: 0.790785\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3252/10000000000000]\n",
      "\tTraining Loss: 0.753529\n",
      "\tTesting Loss: 0.790611\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3253/10000000000000]\n",
      "\tTraining Loss: 0.753623\n",
      "\tTesting Loss: 0.791682\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3254/10000000000000]\n",
      "\tTraining Loss: 0.753648\n",
      "\tTesting Loss: 0.791085\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3255/10000000000000]\n",
      "\tTraining Loss: 0.753633\n",
      "\tTesting Loss: 0.790773\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3256/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.791829\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3257/10000000000000]\n",
      "\tTraining Loss: 0.753703\n",
      "\tTesting Loss: 0.791343\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3258/10000000000000]\n",
      "\tTraining Loss: 0.753637\n",
      "\tTesting Loss: 0.790386\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3259/10000000000000]\n",
      "\tTraining Loss: 0.753677\n",
      "\tTesting Loss: 0.790825\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3260/10000000000000]\n",
      "\tTraining Loss: 0.753640\n",
      "\tTesting Loss: 0.790540\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3261/10000000000000]\n",
      "\tTraining Loss: 0.753587\n",
      "\tTesting Loss: 0.790651\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3262/10000000000000]\n",
      "\tTraining Loss: 0.753685\n",
      "\tTesting Loss: 0.790195\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3263/10000000000000]\n",
      "\tTraining Loss: 0.753581\n",
      "\tTesting Loss: 0.791308\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3264/10000000000000]\n",
      "\tTraining Loss: 0.753664\n",
      "\tTesting Loss: 0.790063\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3265/10000000000000]\n",
      "\tTraining Loss: 0.753609\n",
      "\tTesting Loss: 0.789771\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3266/10000000000000]\n",
      "\tTraining Loss: 0.753694\n",
      "\tTesting Loss: 0.790391\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3267/10000000000000]\n",
      "\tTraining Loss: 0.753641\n",
      "\tTesting Loss: 0.790350\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3268/10000000000000]\n",
      "\tTraining Loss: 0.753653\n",
      "\tTesting Loss: 0.790417\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3269/10000000000000]\n",
      "\tTraining Loss: 0.753637\n",
      "\tTesting Loss: 0.791063\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3270/10000000000000]\n",
      "\tTraining Loss: 0.753625\n",
      "\tTesting Loss: 0.790930\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3271/10000000000000]\n",
      "\tTraining Loss: 0.753680\n",
      "\tTesting Loss: 0.790159\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3272/10000000000000]\n",
      "\tTraining Loss: 0.753620\n",
      "\tTesting Loss: 0.791886\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3273/10000000000000]\n",
      "\tTraining Loss: 0.753666\n",
      "\tTesting Loss: 0.791229\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3274/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.790918\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3275/10000000000000]\n",
      "\tTraining Loss: 0.753648\n",
      "\tTesting Loss: 0.790313\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3276/10000000000000]\n",
      "\tTraining Loss: 0.753714\n",
      "\tTesting Loss: 0.792015\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3277/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.791062\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3278/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.790778\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3279/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.791205\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3280/10000000000000]\n",
      "\tTraining Loss: 0.753607\n",
      "\tTesting Loss: 0.791144\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3281/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.791613\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3282/10000000000000]\n",
      "\tTraining Loss: 0.753689\n",
      "\tTesting Loss: 0.791011\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3283/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.791136\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3284/10000000000000]\n",
      "\tTraining Loss: 0.753654\n",
      "\tTesting Loss: 0.791931\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3285/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.790750\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3286/10000000000000]\n",
      "\tTraining Loss: 0.753597\n",
      "\tTesting Loss: 0.791349\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3287/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.789981\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3288/10000000000000]\n",
      "\tTraining Loss: 0.753638\n",
      "\tTesting Loss: 0.791265\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3289/10000000000000]\n",
      "\tTraining Loss: 0.753683\n",
      "\tTesting Loss: 0.790500\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3290/10000000000000]\n",
      "\tTraining Loss: 0.753623\n",
      "\tTesting Loss: 0.791139\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3291/10000000000000]\n",
      "\tTraining Loss: 0.753638\n",
      "\tTesting Loss: 0.791891\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3292/10000000000000]\n",
      "\tTraining Loss: 0.753597\n",
      "\tTesting Loss: 0.791030\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3293/10000000000000]\n",
      "\tTraining Loss: 0.753642\n",
      "\tTesting Loss: 0.791108\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3294/10000000000000]\n",
      "\tTraining Loss: 0.753692\n",
      "\tTesting Loss: 0.790653\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3295/10000000000000]\n",
      "\tTraining Loss: 0.753632\n",
      "\tTesting Loss: 0.791028\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3296/10000000000000]\n",
      "\tTraining Loss: 0.753654\n",
      "\tTesting Loss: 0.790505\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3297/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.791735\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3298/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.790965\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3299/10000000000000]\n",
      "\tTraining Loss: 0.753635\n",
      "\tTesting Loss: 0.791021\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3300/10000000000000]\n",
      "\tTraining Loss: 0.753657\n",
      "\tTesting Loss: 0.790198\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3301/10000000000000]\n",
      "\tTraining Loss: 0.753629\n",
      "\tTesting Loss: 0.791453\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3302/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.791010\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3303/10000000000000]\n",
      "\tTraining Loss: 0.753691\n",
      "\tTesting Loss: 0.790560\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3304/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.790848\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3305/10000000000000]\n",
      "\tTraining Loss: 0.753631\n",
      "\tTesting Loss: 0.791573\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3306/10000000000000]\n",
      "\tTraining Loss: 0.753623\n",
      "\tTesting Loss: 0.790876\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3307/10000000000000]\n",
      "\tTraining Loss: 0.753657\n",
      "\tTesting Loss: 0.791920\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3308/10000000000000]\n",
      "\tTraining Loss: 0.753702\n",
      "\tTesting Loss: 0.791826\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3309/10000000000000]\n",
      "\tTraining Loss: 0.753637\n",
      "\tTesting Loss: 0.790832\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3310/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.791164\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3311/10000000000000]\n",
      "\tTraining Loss: 0.753630\n",
      "\tTesting Loss: 0.790712\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3312/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.791055\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3313/10000000000000]\n",
      "\tTraining Loss: 0.753636\n",
      "\tTesting Loss: 0.791973\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3314/10000000000000]\n",
      "\tTraining Loss: 0.753668\n",
      "\tTesting Loss: 0.791266\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3315/10000000000000]\n",
      "\tTraining Loss: 0.753627\n",
      "\tTesting Loss: 0.792111\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3316/10000000000000]\n",
      "\tTraining Loss: 0.753642\n",
      "\tTesting Loss: 0.790150\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3317/10000000000000]\n",
      "\tTraining Loss: 0.753642\n",
      "\tTesting Loss: 0.791664\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3318/10000000000000]\n",
      "\tTraining Loss: 0.753641\n",
      "\tTesting Loss: 0.790095\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3319/10000000000000]\n",
      "\tTraining Loss: 0.753688\n",
      "\tTesting Loss: 0.790612\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3320/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.790545\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3321/10000000000000]\n",
      "\tTraining Loss: 0.753608\n",
      "\tTesting Loss: 0.790426\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3322/10000000000000]\n",
      "\tTraining Loss: 0.753600\n",
      "\tTesting Loss: 0.791187\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3323/10000000000000]\n",
      "\tTraining Loss: 0.753661\n",
      "\tTesting Loss: 0.791293\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3324/10000000000000]\n",
      "\tTraining Loss: 0.753620\n",
      "\tTesting Loss: 0.790245\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3325/10000000000000]\n",
      "\tTraining Loss: 0.753628\n",
      "\tTesting Loss: 0.791940\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3326/10000000000000]\n",
      "\tTraining Loss: 0.753677\n",
      "\tTesting Loss: 0.790643\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3327/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.790504\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3328/10000000000000]\n",
      "\tTraining Loss: 0.753657\n",
      "\tTesting Loss: 0.791213\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3329/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.790751\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3330/10000000000000]\n",
      "\tTraining Loss: 0.753695\n",
      "\tTesting Loss: 0.791125\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3331/10000000000000]\n",
      "\tTraining Loss: 0.753600\n",
      "\tTesting Loss: 0.791180\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3332/10000000000000]\n",
      "\tTraining Loss: 0.753669\n",
      "\tTesting Loss: 0.792164\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3333/10000000000000]\n",
      "\tTraining Loss: 0.753638\n",
      "\tTesting Loss: 0.791199\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3334/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.790597\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3335/10000000000000]\n",
      "\tTraining Loss: 0.753578\n",
      "\tTesting Loss: 0.790986\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3336/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.791477\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3337/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.791376\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3338/10000000000000]\n",
      "\tTraining Loss: 0.753659\n",
      "\tTesting Loss: 0.790470\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3339/10000000000000]\n",
      "\tTraining Loss: 0.753596\n",
      "\tTesting Loss: 0.792266\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3340/10000000000000]\n",
      "\tTraining Loss: 0.753638\n",
      "\tTesting Loss: 0.791177\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3341/10000000000000]\n",
      "\tTraining Loss: 0.753641\n",
      "\tTesting Loss: 0.790758\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3342/10000000000000]\n",
      "\tTraining Loss: 0.753686\n",
      "\tTesting Loss: 0.791408\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3343/10000000000000]\n",
      "\tTraining Loss: 0.753638\n",
      "\tTesting Loss: 0.791144\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3344/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.791163\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3345/10000000000000]\n",
      "\tTraining Loss: 0.753691\n",
      "\tTesting Loss: 0.790832\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3346/10000000000000]\n",
      "\tTraining Loss: 0.753644\n",
      "\tTesting Loss: 0.790819\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3347/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.790781\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3348/10000000000000]\n",
      "\tTraining Loss: 0.753659\n",
      "\tTesting Loss: 0.792007\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3349/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.789911\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3350/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.791098\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3351/10000000000000]\n",
      "\tTraining Loss: 0.753616\n",
      "\tTesting Loss: 0.790846\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3352/10000000000000]\n",
      "\tTraining Loss: 0.753611\n",
      "\tTesting Loss: 0.791321\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3353/10000000000000]\n",
      "\tTraining Loss: 0.753613\n",
      "\tTesting Loss: 0.791130\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3354/10000000000000]\n",
      "\tTraining Loss: 0.753609\n",
      "\tTesting Loss: 0.791135\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3355/10000000000000]\n",
      "\tTraining Loss: 0.753624\n",
      "\tTesting Loss: 0.790920\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3356/10000000000000]\n",
      "\tTraining Loss: 0.753646\n",
      "\tTesting Loss: 0.790948\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3357/10000000000000]\n",
      "\tTraining Loss: 0.753674\n",
      "\tTesting Loss: 0.791606\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3358/10000000000000]\n",
      "\tTraining Loss: 0.753669\n",
      "\tTesting Loss: 0.791183\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3359/10000000000000]\n",
      "\tTraining Loss: 0.753646\n",
      "\tTesting Loss: 0.790736\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3360/10000000000000]\n",
      "\tTraining Loss: 0.753635\n",
      "\tTesting Loss: 0.791598\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3361/10000000000000]\n",
      "\tTraining Loss: 0.753632\n",
      "\tTesting Loss: 0.790597\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3362/10000000000000]\n",
      "\tTraining Loss: 0.753634\n",
      "\tTesting Loss: 0.791435\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3363/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.790572\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3364/10000000000000]\n",
      "\tTraining Loss: 0.753674\n",
      "\tTesting Loss: 0.790986\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3365/10000000000000]\n",
      "\tTraining Loss: 0.753637\n",
      "\tTesting Loss: 0.791356\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3366/10000000000000]\n",
      "\tTraining Loss: 0.753613\n",
      "\tTesting Loss: 0.791216\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3367/10000000000000]\n",
      "\tTraining Loss: 0.753603\n",
      "\tTesting Loss: 0.791074\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3368/10000000000000]\n",
      "\tTraining Loss: 0.753695\n",
      "\tTesting Loss: 0.791095\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3369/10000000000000]\n",
      "\tTraining Loss: 0.753684\n",
      "\tTesting Loss: 0.791053\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3370/10000000000000]\n",
      "\tTraining Loss: 0.753662\n",
      "\tTesting Loss: 0.791522\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3371/10000000000000]\n",
      "\tTraining Loss: 0.753653\n",
      "\tTesting Loss: 0.791214\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3372/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.792161\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3373/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.790463\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3374/10000000000000]\n",
      "\tTraining Loss: 0.753617\n",
      "\tTesting Loss: 0.790781\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3375/10000000000000]\n",
      "\tTraining Loss: 0.753676\n",
      "\tTesting Loss: 0.791952\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3376/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.790520\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3377/10000000000000]\n",
      "\tTraining Loss: 0.753617\n",
      "\tTesting Loss: 0.790971\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3378/10000000000000]\n",
      "\tTraining Loss: 0.753640\n",
      "\tTesting Loss: 0.790796\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3379/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.791231\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3380/10000000000000]\n",
      "\tTraining Loss: 0.753629\n",
      "\tTesting Loss: 0.791303\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3381/10000000000000]\n",
      "\tTraining Loss: 0.753641\n",
      "\tTesting Loss: 0.791873\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3382/10000000000000]\n",
      "\tTraining Loss: 0.753617\n",
      "\tTesting Loss: 0.791092\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3383/10000000000000]\n",
      "\tTraining Loss: 0.753668\n",
      "\tTesting Loss: 0.791929\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3384/10000000000000]\n",
      "\tTraining Loss: 0.753622\n",
      "\tTesting Loss: 0.791012\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3385/10000000000000]\n",
      "\tTraining Loss: 0.753598\n",
      "\tTesting Loss: 0.791111\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3386/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.791912\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3387/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.791473\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3388/10000000000000]\n",
      "\tTraining Loss: 0.753612\n",
      "\tTesting Loss: 0.790786\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3389/10000000000000]\n",
      "\tTraining Loss: 0.753645\n",
      "\tTesting Loss: 0.790783\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3390/10000000000000]\n",
      "\tTraining Loss: 0.753610\n",
      "\tTesting Loss: 0.790769\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3391/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.791317\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3392/10000000000000]\n",
      "\tTraining Loss: 0.753608\n",
      "\tTesting Loss: 0.791577\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3393/10000000000000]\n",
      "\tTraining Loss: 0.753621\n",
      "\tTesting Loss: 0.789790\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3394/10000000000000]\n",
      "\tTraining Loss: 0.753710\n",
      "\tTesting Loss: 0.791565\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3395/10000000000000]\n",
      "\tTraining Loss: 0.753691\n",
      "\tTesting Loss: 0.790250\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3396/10000000000000]\n",
      "\tTraining Loss: 0.753652\n",
      "\tTesting Loss: 0.792275\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3397/10000000000000]\n",
      "\tTraining Loss: 0.753632\n",
      "\tTesting Loss: 0.790716\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3398/10000000000000]\n",
      "\tTraining Loss: 0.753632\n",
      "\tTesting Loss: 0.790929\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3399/10000000000000]\n",
      "\tTraining Loss: 0.753636\n",
      "\tTesting Loss: 0.790279\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3400/10000000000000]\n",
      "\tTraining Loss: 0.753654\n",
      "\tTesting Loss: 0.790727\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3401/10000000000000]\n",
      "\tTraining Loss: 0.753686\n",
      "\tTesting Loss: 0.791418\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3402/10000000000000]\n",
      "\tTraining Loss: 0.753667\n",
      "\tTesting Loss: 0.791812\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3403/10000000000000]\n",
      "\tTraining Loss: 0.753622\n",
      "\tTesting Loss: 0.791168\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3404/10000000000000]\n",
      "\tTraining Loss: 0.753611\n",
      "\tTesting Loss: 0.790872\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3405/10000000000000]\n",
      "\tTraining Loss: 0.753695\n",
      "\tTesting Loss: 0.790843\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3406/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.790909\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3407/10000000000000]\n",
      "\tTraining Loss: 0.753630\n",
      "\tTesting Loss: 0.790766\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3408/10000000000000]\n",
      "\tTraining Loss: 0.753632\n",
      "\tTesting Loss: 0.791322\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3409/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.790488\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3410/10000000000000]\n",
      "\tTraining Loss: 0.753613\n",
      "\tTesting Loss: 0.790240\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3411/10000000000000]\n",
      "\tTraining Loss: 0.753694\n",
      "\tTesting Loss: 0.790299\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3412/10000000000000]\n",
      "\tTraining Loss: 0.753602\n",
      "\tTesting Loss: 0.790998\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3413/10000000000000]\n",
      "\tTraining Loss: 0.753653\n",
      "\tTesting Loss: 0.791086\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3414/10000000000000]\n",
      "\tTraining Loss: 0.753635\n",
      "\tTesting Loss: 0.790664\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3415/10000000000000]\n",
      "\tTraining Loss: 0.753641\n",
      "\tTesting Loss: 0.791363\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3416/10000000000000]\n",
      "\tTraining Loss: 0.753633\n",
      "\tTesting Loss: 0.790770\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3417/10000000000000]\n",
      "\tTraining Loss: 0.753655\n",
      "\tTesting Loss: 0.791247\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3418/10000000000000]\n",
      "\tTraining Loss: 0.753624\n",
      "\tTesting Loss: 0.791793\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3419/10000000000000]\n",
      "\tTraining Loss: 0.753700\n",
      "\tTesting Loss: 0.790620\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3420/10000000000000]\n",
      "\tTraining Loss: 0.753703\n",
      "\tTesting Loss: 0.790868\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3421/10000000000000]\n",
      "\tTraining Loss: 0.753655\n",
      "\tTesting Loss: 0.790645\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3422/10000000000000]\n",
      "\tTraining Loss: 0.753675\n",
      "\tTesting Loss: 0.791492\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3423/10000000000000]\n",
      "\tTraining Loss: 0.753652\n",
      "\tTesting Loss: 0.791224\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3424/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.791132\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3425/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.791498\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3426/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.790804\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3427/10000000000000]\n",
      "\tTraining Loss: 0.753730\n",
      "\tTesting Loss: 0.789277\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3428/10000000000000]\n",
      "\tTraining Loss: 0.753636\n",
      "\tTesting Loss: 0.790603\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3429/10000000000000]\n",
      "\tTraining Loss: 0.753619\n",
      "\tTesting Loss: 0.791002\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3430/10000000000000]\n",
      "\tTraining Loss: 0.753635\n",
      "\tTesting Loss: 0.790437\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3431/10000000000000]\n",
      "\tTraining Loss: 0.753662\n",
      "\tTesting Loss: 0.790956\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3432/10000000000000]\n",
      "\tTraining Loss: 0.753679\n",
      "\tTesting Loss: 0.790543\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3433/10000000000000]\n",
      "\tTraining Loss: 0.753657\n",
      "\tTesting Loss: 0.791332\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3434/10000000000000]\n",
      "\tTraining Loss: 0.753652\n",
      "\tTesting Loss: 0.791588\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3435/10000000000000]\n",
      "\tTraining Loss: 0.753690\n",
      "\tTesting Loss: 0.790989\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3436/10000000000000]\n",
      "\tTraining Loss: 0.753642\n",
      "\tTesting Loss: 0.791423\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3437/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.791531\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3438/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.791243\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3439/10000000000000]\n",
      "\tTraining Loss: 0.753712\n",
      "\tTesting Loss: 0.791241\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3440/10000000000000]\n",
      "\tTraining Loss: 0.753631\n",
      "\tTesting Loss: 0.791151\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3441/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.792294\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3442/10000000000000]\n",
      "\tTraining Loss: 0.753634\n",
      "\tTesting Loss: 0.791424\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3443/10000000000000]\n",
      "\tTraining Loss: 0.753704\n",
      "\tTesting Loss: 0.790548\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3444/10000000000000]\n",
      "\tTraining Loss: 0.753676\n",
      "\tTesting Loss: 0.791144\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3445/10000000000000]\n",
      "\tTraining Loss: 0.753596\n",
      "\tTesting Loss: 0.791178\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3446/10000000000000]\n",
      "\tTraining Loss: 0.753635\n",
      "\tTesting Loss: 0.791020\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3447/10000000000000]\n",
      "\tTraining Loss: 0.753673\n",
      "\tTesting Loss: 0.791144\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3448/10000000000000]\n",
      "\tTraining Loss: 0.753613\n",
      "\tTesting Loss: 0.790749\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3449/10000000000000]\n",
      "\tTraining Loss: 0.753649\n",
      "\tTesting Loss: 0.790542\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3450/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.791141\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3451/10000000000000]\n",
      "\tTraining Loss: 0.753648\n",
      "\tTesting Loss: 0.791764\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3452/10000000000000]\n",
      "\tTraining Loss: 0.753671\n",
      "\tTesting Loss: 0.790522\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3453/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.790727\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3454/10000000000000]\n",
      "\tTraining Loss: 0.753644\n",
      "\tTesting Loss: 0.791294\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3455/10000000000000]\n",
      "\tTraining Loss: 0.753623\n",
      "\tTesting Loss: 0.791632\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3456/10000000000000]\n",
      "\tTraining Loss: 0.753718\n",
      "\tTesting Loss: 0.790426\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3457/10000000000000]\n",
      "\tTraining Loss: 0.753723\n",
      "\tTesting Loss: 0.791075\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3458/10000000000000]\n",
      "\tTraining Loss: 0.753613\n",
      "\tTesting Loss: 0.791267\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3459/10000000000000]\n",
      "\tTraining Loss: 0.753573\n",
      "\tTesting Loss: 0.791207\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3460/10000000000000]\n",
      "\tTraining Loss: 0.753647\n",
      "\tTesting Loss: 0.791105\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3461/10000000000000]\n",
      "\tTraining Loss: 0.753582\n",
      "\tTesting Loss: 0.791587\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3462/10000000000000]\n",
      "\tTraining Loss: 0.753641\n",
      "\tTesting Loss: 0.791128\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3463/10000000000000]\n",
      "\tTraining Loss: 0.753677\n",
      "\tTesting Loss: 0.791569\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3464/10000000000000]\n",
      "\tTraining Loss: 0.753546\n",
      "\tTesting Loss: 0.790507\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3465/10000000000000]\n",
      "\tTraining Loss: 0.753693\n",
      "\tTesting Loss: 0.791502\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3466/10000000000000]\n",
      "\tTraining Loss: 0.753637\n",
      "\tTesting Loss: 0.790399\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3467/10000000000000]\n",
      "\tTraining Loss: 0.753631\n",
      "\tTesting Loss: 0.791967\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3468/10000000000000]\n",
      "\tTraining Loss: 0.753633\n",
      "\tTesting Loss: 0.791063\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3469/10000000000000]\n",
      "\tTraining Loss: 0.753623\n",
      "\tTesting Loss: 0.791252\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3470/10000000000000]\n",
      "\tTraining Loss: 0.753693\n",
      "\tTesting Loss: 0.791506\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3471/10000000000000]\n",
      "\tTraining Loss: 0.753614\n",
      "\tTesting Loss: 0.791616\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3472/10000000000000]\n",
      "\tTraining Loss: 0.753672\n",
      "\tTesting Loss: 0.790628\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3473/10000000000000]\n",
      "\tTraining Loss: 0.753637\n",
      "\tTesting Loss: 0.792156\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3474/10000000000000]\n",
      "\tTraining Loss: 0.753636\n",
      "\tTesting Loss: 0.790246\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3475/10000000000000]\n",
      "\tTraining Loss: 0.753579\n",
      "\tTesting Loss: 0.790897\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3476/10000000000000]\n",
      "\tTraining Loss: 0.753664\n",
      "\tTesting Loss: 0.791411\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3477/10000000000000]\n",
      "\tTraining Loss: 0.753604\n",
      "\tTesting Loss: 0.789810\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3478/10000000000000]\n",
      "\tTraining Loss: 0.753681\n",
      "\tTesting Loss: 0.791035\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3479/10000000000000]\n",
      "\tTraining Loss: 0.753614\n",
      "\tTesting Loss: 0.789886\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3480/10000000000000]\n",
      "\tTraining Loss: 0.753622\n",
      "\tTesting Loss: 0.790704\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3481/10000000000000]\n",
      "\tTraining Loss: 0.753633\n",
      "\tTesting Loss: 0.791365\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3482/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.791228\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3483/10000000000000]\n",
      "\tTraining Loss: 0.753647\n",
      "\tTesting Loss: 0.790595\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3484/10000000000000]\n",
      "\tTraining Loss: 0.753644\n",
      "\tTesting Loss: 0.791058\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3485/10000000000000]\n",
      "\tTraining Loss: 0.753684\n",
      "\tTesting Loss: 0.790786\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3486/10000000000000]\n",
      "\tTraining Loss: 0.753645\n",
      "\tTesting Loss: 0.791211\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3487/10000000000000]\n",
      "\tTraining Loss: 0.753655\n",
      "\tTesting Loss: 0.791457\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3488/10000000000000]\n",
      "\tTraining Loss: 0.753681\n",
      "\tTesting Loss: 0.791567\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3489/10000000000000]\n",
      "\tTraining Loss: 0.753625\n",
      "\tTesting Loss: 0.790805\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3490/10000000000000]\n",
      "\tTraining Loss: 0.753632\n",
      "\tTesting Loss: 0.791004\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3491/10000000000000]\n",
      "\tTraining Loss: 0.753624\n",
      "\tTesting Loss: 0.792046\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3492/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.790618\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3493/10000000000000]\n",
      "\tTraining Loss: 0.753644\n",
      "\tTesting Loss: 0.791062\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3494/10000000000000]\n",
      "\tTraining Loss: 0.753698\n",
      "\tTesting Loss: 0.790862\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3495/10000000000000]\n",
      "\tTraining Loss: 0.753621\n",
      "\tTesting Loss: 0.790679\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3496/10000000000000]\n",
      "\tTraining Loss: 0.753619\n",
      "\tTesting Loss: 0.790810\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3497/10000000000000]\n",
      "\tTraining Loss: 0.753614\n",
      "\tTesting Loss: 0.790710\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3498/10000000000000]\n",
      "\tTraining Loss: 0.753707\n",
      "\tTesting Loss: 0.790593\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3499/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.791268\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3500/10000000000000]\n",
      "\tTraining Loss: 0.753641\n",
      "\tTesting Loss: 0.790793\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3501/10000000000000]\n",
      "\tTraining Loss: 0.753603\n",
      "\tTesting Loss: 0.791577\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3502/10000000000000]\n",
      "\tTraining Loss: 0.753690\n",
      "\tTesting Loss: 0.790705\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3503/10000000000000]\n",
      "\tTraining Loss: 0.753622\n",
      "\tTesting Loss: 0.790886\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3504/10000000000000]\n",
      "\tTraining Loss: 0.753653\n",
      "\tTesting Loss: 0.790180\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3505/10000000000000]\n",
      "\tTraining Loss: 0.753714\n",
      "\tTesting Loss: 0.790557\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3506/10000000000000]\n",
      "\tTraining Loss: 0.753656\n",
      "\tTesting Loss: 0.790601\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3507/10000000000000]\n",
      "\tTraining Loss: 0.753660\n",
      "\tTesting Loss: 0.791632\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3508/10000000000000]\n",
      "\tTraining Loss: 0.753650\n",
      "\tTesting Loss: 0.790813\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3509/10000000000000]\n",
      "\tTraining Loss: 0.753648\n",
      "\tTesting Loss: 0.791429\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3510/10000000000000]\n",
      "\tTraining Loss: 0.753636\n",
      "\tTesting Loss: 0.791088\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3511/10000000000000]\n",
      "\tTraining Loss: 0.753634\n",
      "\tTesting Loss: 0.791502\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3512/10000000000000]\n",
      "\tTraining Loss: 0.753637\n",
      "\tTesting Loss: 0.790847\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3513/10000000000000]\n",
      "\tTraining Loss: 0.753610\n",
      "\tTesting Loss: 0.790800\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3514/10000000000000]\n",
      "\tTraining Loss: 0.753660\n",
      "\tTesting Loss: 0.790831\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3515/10000000000000]\n",
      "\tTraining Loss: 0.753607\n",
      "\tTesting Loss: 0.791141\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3516/10000000000000]\n",
      "\tTraining Loss: 0.753639\n",
      "\tTesting Loss: 0.790702\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3517/10000000000000]\n",
      "\tTraining Loss: 0.753641\n",
      "\tTesting Loss: 0.790731\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3518/10000000000000]\n",
      "\tTraining Loss: 0.753616\n",
      "\tTesting Loss: 0.791096\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3519/10000000000000]\n",
      "\tTraining Loss: 0.753634\n",
      "\tTesting Loss: 0.791281\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3520/10000000000000]\n",
      "\tTraining Loss: 0.753682\n",
      "\tTesting Loss: 0.790851\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3521/10000000000000]\n",
      "\tTraining Loss: 0.753624\n",
      "\tTesting Loss: 0.790271\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3522/10000000000000]\n",
      "\tTraining Loss: 0.753646\n",
      "\tTesting Loss: 0.790855\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3523/10000000000000]\n",
      "\tTraining Loss: 0.753637\n",
      "\tTesting Loss: 0.790962\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3524/10000000000000]\n",
      "\tTraining Loss: 0.753635\n",
      "\tTesting Loss: 0.791209\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3525/10000000000000]\n",
      "\tTraining Loss: 0.753642\n",
      "\tTesting Loss: 0.791336\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3526/10000000000000]\n",
      "\tTraining Loss: 0.753624\n",
      "\tTesting Loss: 0.790930\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3527/10000000000000]\n",
      "\tTraining Loss: 0.753646\n",
      "\tTesting Loss: 0.789788\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3528/10000000000000]\n",
      "\tTraining Loss: 0.753589\n",
      "\tTesting Loss: 0.791338\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3529/10000000000000]\n",
      "\tTraining Loss: 0.753697\n",
      "\tTesting Loss: 0.791104\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3530/10000000000000]\n",
      "\tTraining Loss: 0.753704\n",
      "\tTesting Loss: 0.791222\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3531/10000000000000]\n",
      "\tTraining Loss: 0.753634\n",
      "\tTesting Loss: 0.790768\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3532/10000000000000]\n",
      "\tTraining Loss: 0.753624\n",
      "\tTesting Loss: 0.791333\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3533/10000000000000]\n",
      "\tTraining Loss: 0.753614\n",
      "\tTesting Loss: 0.791106\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3534/10000000000000]\n",
      "\tTraining Loss: 0.753617\n",
      "\tTesting Loss: 0.791385\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3535/10000000000000]\n",
      "\tTraining Loss: 0.753600\n",
      "\tTesting Loss: 0.791262\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3536/10000000000000]\n",
      "\tTraining Loss: 0.753601\n",
      "\tTesting Loss: 0.791373\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3537/10000000000000]\n",
      "\tTraining Loss: 0.753665\n",
      "\tTesting Loss: 0.790965\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3538/10000000000000]\n",
      "\tTraining Loss: 0.753623\n",
      "\tTesting Loss: 0.791571\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3539/10000000000000]\n",
      "\tTraining Loss: 0.753713\n",
      "\tTesting Loss: 0.790859\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3540/10000000000000]\n",
      "\tTraining Loss: 0.753629\n",
      "\tTesting Loss: 0.791120\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3541/10000000000000]\n",
      "\tTraining Loss: 0.753594\n",
      "\tTesting Loss: 0.792657\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3542/10000000000000]\n",
      "\tTraining Loss: 0.753645\n",
      "\tTesting Loss: 0.791611\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3543/10000000000000]\n",
      "\tTraining Loss: 0.753637\n",
      "\tTesting Loss: 0.790858\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3544/10000000000000]\n",
      "\tTraining Loss: 0.753716\n",
      "\tTesting Loss: 0.790704\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3545/10000000000000]\n",
      "\tTraining Loss: 0.753660\n",
      "\tTesting Loss: 0.790504\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3546/10000000000000]\n",
      "\tTraining Loss: 0.753599\n",
      "\tTesting Loss: 0.790204\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3547/10000000000000]\n",
      "\tTraining Loss: 0.753663\n",
      "\tTesting Loss: 0.790915\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3548/10000000000000]\n",
      "\tTraining Loss: 0.753664\n",
      "\tTesting Loss: 0.790408\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3549/10000000000000]\n",
      "\tTraining Loss: 0.753646\n",
      "\tTesting Loss: 0.791281\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3550/10000000000000]\n",
      "\tTraining Loss: 0.753637\n",
      "\tTesting Loss: 0.789867\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3551/10000000000000]\n",
      "\tTraining Loss: 0.753637\n",
      "\tTesting Loss: 0.791422\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3552/10000000000000]\n",
      "\tTraining Loss: 0.753659\n",
      "\tTesting Loss: 0.791155\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3553/10000000000000]\n",
      "\tTraining Loss: 0.753678\n",
      "\tTesting Loss: 0.791260\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3554/10000000000000]\n",
      "\tTraining Loss: 0.753643\n",
      "\tTesting Loss: 0.791572\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3555/10000000000000]\n",
      "\tTraining Loss: 0.753661\n",
      "\tTesting Loss: 0.790991\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3556/10000000000000]\n",
      "\tTraining Loss: 0.753645\n",
      "\tTesting Loss: 0.791148\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3557/10000000000000]\n",
      "\tTraining Loss: 0.753611\n",
      "\tTesting Loss: 0.790727\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3558/10000000000000]\n",
      "\tTraining Loss: 0.753662\n",
      "\tTesting Loss: 0.790621\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3559/10000000000000]\n",
      "\tTraining Loss: 0.753603\n",
      "\tTesting Loss: 0.790442\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3560/10000000000000]\n",
      "\tTraining Loss: 0.753670\n",
      "\tTesting Loss: 0.791574\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3561/10000000000000]\n",
      "\tTraining Loss: 0.753694\n",
      "\tTesting Loss: 0.790692\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3562/10000000000000]\n",
      "\tTraining Loss: 0.753660\n",
      "\tTesting Loss: 0.790968\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3563/10000000000000]\n",
      "\tTraining Loss: 0.753626\n",
      "\tTesting Loss: 0.790638\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3564/10000000000000]\n",
      "\tTraining Loss: 0.753583\n",
      "\tTesting Loss: 0.791117\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3565/10000000000000]\n",
      "\tTraining Loss: 0.753641\n",
      "\tTesting Loss: 0.790967\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3566/10000000000000]\n",
      "\tTraining Loss: 0.753634\n",
      "\tTesting Loss: 0.791059\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3567/10000000000000]\n",
      "\tTraining Loss: 0.753577\n",
      "\tTesting Loss: 0.790363\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3568/10000000000000]\n",
      "\tTraining Loss: 0.753658\n",
      "\tTesting Loss: 0.791128\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3569/10000000000000]\n",
      "\tTraining Loss: 0.753616\n",
      "\tTesting Loss: 0.790734\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3570/10000000000000]\n",
      "\tTraining Loss: 0.753619\n",
      "\tTesting Loss: 0.790515\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3571/10000000000000]\n",
      "\tTraining Loss: 0.753648\n",
      "\tTesting Loss: 0.790837\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3572/10000000000000]\n",
      "\tTraining Loss: 0.753595\n",
      "\tTesting Loss: 0.790853\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3573/10000000000000]\n",
      "\tTraining Loss: 0.753652\n",
      "\tTesting Loss: 0.790483\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3574/10000000000000]\n",
      "\tTraining Loss: 0.753621\n",
      "\tTesting Loss: 0.790384\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3575/10000000000000]\n",
      "\tTraining Loss: 0.753623\n",
      "\tTesting Loss: 0.791772\n",
      "\tLearning Rate: 0.000000044\n",
      "Epoch [3576/10000000000000]\n",
      "\tTraining Loss: 0.753651\n",
      "\tTesting Loss: 0.790609\n",
      "\tLearning Rate: 0.000000044\n",
      "\n",
      "Training interrupted by user. Saving current progress...\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(model_path) or overwrite:\n",
    "    # Train the model\n",
    "    num_epochs = 10000000000000\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    train_losses, test_losses = train(\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=num_epochs,\n",
    "        device=device\n",
    "    )\n",
    "    # Save model locally\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    torch.save((train_losses, test_losses), loss_path)\n",
    "else:\n",
    "    # load model from model_path\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    train_losses, test_losses = torch.load(loss_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Plot training and testing losses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, (ax1) = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "# Plot losses\n",
    "ax1.plot(train_losses, label='Training Loss')\n",
    "ax1.plot(test_losses, label='Testing Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "#ylim 0 - 1\n",
    "ax1.set_ylim(0, 3)\n",
    "ax1.set_title('Training and Testing Losses')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
