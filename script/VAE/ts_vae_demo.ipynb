{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.abspath('../..')) \n",
    "\n",
    "os.chdir('../..')\n",
    "print(\"New working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.adam import Adam\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"./data/tmp/btcusd_1-min_data.csv\"\n",
    "class BitcoinDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file.\n",
    "        \"\"\"\n",
    "        # Load data from CSV, select columns and drop null values\n",
    "        self.dataframe = pd.read_csv(csv_file)[['High','Low','Open','Close']].dropna()\n",
    "\n",
    "        # Extract the features for easier manipulation\n",
    "        self.features = self.dataframe.values\n",
    "\n",
    "        # Calculate mean and std for normalization\n",
    "        self.mean = self.features.mean(axis=0)\n",
    "        self.std = self.features.std(axis=0)\n",
    "\n",
    "        # Apply normalization to features\n",
    "        self.features = (self.features - self.mean) / self.std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Return the item at index idx in the form of tensor\n",
    "        features = torch.tensor(self.features[idx], dtype=torch.float32).to(device)\n",
    "        \n",
    "        return features\n",
    "\n",
    "\n",
    "# Create an instance of BitcoinDataset and store in variable\n",
    "dataset = BitcoinDataset(csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batchsize and input dimensions\n",
    "batch_size = 64\n",
    "input_dim = dataset.features.shape[1]\n",
    "\n",
    "# Split dataset into train and test in the ratio of 80:20\n",
    "train_dataset, test_dataset = random_split(dataset, [0.8,0.2])\n",
    "\n",
    "# Use DataLoader for batching and shuffling\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=4, hidden_dim=40, latent_dim=3, device=device):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "            nn.LeakyReLU(0.2)\n",
    "            )\n",
    "        \n",
    "        # Latent mean and variance \n",
    "        self.mean_layer = nn.Linear(latent_dim, 1)\n",
    "        self.logvar_layer = nn.Linear(latent_dim, 1)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(1, latent_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "            )\n",
    "     \n",
    "    # Encode function\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mean, log_var = self.mean_layer(x), self.logvar_layer(x)\n",
    "        return mean, log_var\n",
    "    \n",
    "    # Add Reparameterization\n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(device)      \n",
    "        z = mean + var*epsilon\n",
    "        return z\n",
    "\n",
    "    # Decode function\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "    # Forward Function\n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.encode(x)\n",
    "        z = self.reparameterization(mean, log_var)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mean, log_var\n",
    "    \n",
    "    # Reconstruct input from compressed form\n",
    "    def reconstruction(self, mean, log_var):\n",
    "        z = self.reparameterization(mean, log_var)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=4, hidden_dim=40, latent_dim=3, device=device):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "            nn.LeakyReLU(0.2)\n",
    "            )\n",
    "        \n",
    "        # Latent mean and variance \n",
    "        self.mean_layer = nn.Linear(latent_dim, 1)\n",
    "        self.logvar_layer = nn.Linear(latent_dim, 1)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(1, latent_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "            )\n",
    "     \n",
    "    # Encode function\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mean, log_var = self.mean_layer(x), self.logvar_layer(x)\n",
    "        return mean, log_var\n",
    "    \n",
    "    # Add Reparameterization\n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(device)      \n",
    "        z = mean + var*epsilon\n",
    "        return z\n",
    "\n",
    "    # Decode function\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "    # Forward Function\n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.encode(x)\n",
    "        z = self.reparameterization(mean, log_var)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mean, log_var\n",
    "    \n",
    "    # Reconstruct input from compressed form\n",
    "    def reconstruction(self, mean, log_var):\n",
    "        z = self.reparameterization(mean, log_var)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x, x_hat, mean, log_var):\n",
    "    # Reproduction Loss\n",
    "    reproduction_loss = nn.functional.mse_loss(x_hat, x)\n",
    "    \n",
    "    # KL Divergence Loss\n",
    "    KLD = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
    "    return reproduction_loss + KLD\n",
    "\n",
    "# VAE Model created and stored in device\n",
    "model = VAE().to(device)\n",
    "\n",
    "# Optimizer defined\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs, device):\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Loop for each epoch\n",
    "    for epoch in range(epochs):\n",
    "        overall_loss = 0\n",
    "        \n",
    "        # Iterate over the batches formed by DataLoader\n",
    "        for batch_idx, x in enumerate(train_dataloader):\n",
    "            x = x.to(device)\n",
    "            \n",
    "            # Reset Gradient\n",
    "            optimizer.zero_grad()\n",
    "            x_hat, mean, log_var = model(x)\n",
    "            \n",
    "            # Calculate batch loss and then overall loss\n",
    "            loss = loss_function(x, x_hat, mean, log_var)\n",
    "            overall_loss += loss.item()\n",
    "            \n",
    "            # Backpropagate the loss and train the optimizer\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"\\tEpoch\", epoch + 1, \"\\tAverage Loss: \", overall_loss/(batch_idx*batch_size))\n",
    "    return overall_loss\n",
    "\n",
    "# Train the model for 5 epochs\n",
    "train(model, optimizer, epochs=5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict(model):\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    dataset_size = 0\n",
    "    \n",
    "    # Set up torch so there is no gradient upgrade\n",
    "    with torch.no_grad():\n",
    "        overall_loss=0\n",
    "        all_mean = None\n",
    "        all_log_var = None\n",
    "        \n",
    "        # Iterate over batches of test dataset\n",
    "        for batch_idx, x in enumerate(test_dataloader):\n",
    "            \n",
    "            # Get reconstructed value, mean and log_var\n",
    "            x_hat, mean, log_var  = model(x)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_function(x, x_hat, mean, log_var)\n",
    "\n",
    "            # # plot x and x_hat\n",
    "            # plt.plot(x, label='x')\n",
    "            # plt.plot(x_hat, label='x_hat')\n",
    "            # plt.legend()\n",
    "            # plt.show()\n",
    "            \n",
    "            # Add mean and log_var to arrays\n",
    "            if all_mean is not None:\n",
    "                all_mean = torch.cat((all_mean, mean))\n",
    "                all_log_var = torch.cat((all_log_var, log_var))\n",
    "            else:\n",
    "                all_mean = mean\n",
    "                all_log_var = log_var\n",
    "                \n",
    "            # Add loss \n",
    "            overall_loss += loss.item()\n",
    "            \n",
    "            # Calculate the size of the dataset\n",
    "            size_batch = x.element_size() * x.numel()\n",
    "            dataset_size += size_batch\n",
    "\n",
    "        # Calculate the size after compression of the dataset\n",
    "        compressed_size = (all_mean.element_size() * all_mean.numel()) + (all_log_var.element_size() * all_log_var.numel())\n",
    "\n",
    "        \n",
    "        print(\"\\tAverage Loss: \", overall_loss/(batch_idx*batch_size))\n",
    "        print(f\"\\tDataset Size: {dataset_size} \\n\\tCompressed Size: {compressed_size}\")\n",
    "        print(\"\\tCompression Ratio: \", compressed_size/dataset_size)\n",
    "        \n",
    "        return all_mean, all_log_var\n",
    "    \n",
    "    \n",
    "# Predict the values for test dataset and calculate compression ratio\n",
    "mean,var = predict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
