{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# sys.path.append(os.path.abspath('../VAE'))  # Adds the directory of 'script/VAE' to Python path\n",
    "# from utils import *\n",
    "\n",
    "# os.chdir('../..')\n",
    "# print(os.getcwd())\n",
    "\n",
    "# data_path = './data/PAS Challenge HR Data.xlsx'  \n",
    "# _, _, df_scaled, df = prepare_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 333\n",
      "using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../CLIP')) \n",
    "\n",
    "\n",
    "from config import *\n",
    "from encoder import *\n",
    "from data import *\n",
    "from augmentor import *\n",
    "from describer import *\n",
    "print(\"using device: \", device)\n",
    "random_state = 333\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# import pkg_resources\n",
    "# print(pkg_resources.get_distribution('python-calamine').version)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyqiu/Documents/Documents JoyQiu Work/Research/LLMTimeSeries/llm_nicu_vitalsigns/clip_env/lib/python3.11/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tslearn.metrics import dtw, lcss\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "def process_batch(i, df, sample_size=1000, by = 'dtw'):\n",
    "    n_rows = len(df)\n",
    "    series1 = df.iloc[i].values\n",
    "    \n",
    "    # Sample indices (excluding index i)\n",
    "    available_indices = list(set(range(n_rows)) - {i})\n",
    "    if len(available_indices) > sample_size:\n",
    "        sampled_indices = np.random.choice(available_indices, sample_size, replace=False)\n",
    "    else:\n",
    "        sampled_indices = available_indices\n",
    "    \n",
    "    # Calculate distances for sampled indices\n",
    "    distances = []\n",
    "    for j in sampled_indices:\n",
    "        series2 = df.iloc[j].values\n",
    "        if by == 'dtw':\n",
    "            dist = dtw(series1, series2)  # or use lcss if preferred\n",
    "        elif by == 'lcss':\n",
    "            dist = lcss(series1, series2)\n",
    "        elif by == 'euclidean':\n",
    "            dist = np.linalg.norm(series1 - series2)\n",
    "        distances.append((j, dist))\n",
    "    \n",
    "    # Get top 3 furthest indices\n",
    "    top_3_indices = sorted(distances, key=lambda x: x[1], reverse=True)[:3]\n",
    "    top_indices = [idx for idx, _ in top_3_indices]\n",
    "    \n",
    "    # Free memory\n",
    "    del series1, distances\n",
    "    gc.collect()\n",
    "    \n",
    "    return i, top_indices\n",
    "\n",
    "def find_furthest_neighbors(df, n_jobs=10, sample_size=1000, by = 'dtw'):\n",
    "    n_rows = len(df)\n",
    "    # Process all rows in parallel\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_batch)(i, df, sample_size, by) for i in tqdm(range(n_rows))\n",
    "    )\n",
    "    # Organize results into a dictionary\n",
    "    furthest_neighbors = {i: top_3 for i, top_3 in results}\n",
    "    \n",
    "    return furthest_neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of patients with positive labels:\n",
      "VitalID\n",
      "1018    8\n",
      "5170    8\n",
      "1464    8\n",
      "2361    8\n",
      "2791    8\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=9)]: Using backend LokyBackend with 9 concurrent workers.\n",
      "[Parallel(n_jobs=9)]: Done  32 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=9)]: Done 5138 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=9)]: Done 63704 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=9)]: Done 65353 out of 65353 | elapsed:   12.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This infant will survive.  This infant has gestational age 24 weeks. Birth weight is 360 grams.  No Bradycardia events.  Moderate variability.  Very low amount of consecutive increases. \n",
      "\n",
      "Sample of patients with positive labels:\n",
      "TestID\n",
      "817     8\n",
      "1903    8\n",
      "801     8\n",
      "508     8\n",
      "2518    8\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=9)]: Using backend LokyBackend with 9 concurrent workers.\n",
      "[Parallel(n_jobs=9)]: Done  46 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=9)]: Done 14318 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=9)]: Done 61182 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=9)]: Done 61570 out of 61570 | elapsed:    6.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This infant will survive.  This infant has gestational age 33 weeks. Birth weight is 2630 grams.  No Bradycardia events.  High variability.  Low amount of consecutive increases. \n"
     ]
    }
   ],
   "source": [
    "# -----------------Train Data-----------------\n",
    "df = pd.read_excel('../../data/PAS Challenge HR Data.xlsx', engine=\"calamine\")\n",
    "df.columns = df.columns.astype(str)\n",
    "df_y = pd.read_excel('../../data/PAS Challenge Outcome Data.xlsx', engine=\"calamine\")[['VitalID', 'DiedNICU', 'DeathAge']]\n",
    "df_demo = pd.read_excel('../../data/PAS Challenge Demographic Data.xlsx', engine=\"calamine\")\n",
    "df_x = pd.read_excel('../../data/PAS Challenge Model Data.xlsx', engine=\"calamine\")\n",
    "df = df.merge(df_x[['VitalID', 'VitalTime', 'Age']], on=['VitalID', 'VitalTime'], how='left')\n",
    "df = label_death7d(df, df_y, id_col='VitalID')\n",
    "df = df.merge(df_demo, on='VitalID', how='left')\n",
    "df_desc = generate_descriptions_parallel(ts_df = df.loc[:, '1':'300'], id_df = df.loc[:, ['VitalID', 'VitalTime']])\n",
    "df = df.merge(df_desc, on=['VitalID', 'VitalTime'], how='left')\n",
    "df = text_gen_input_column(df, config_dict['text_config'])\n",
    "df['rowid'] = df.index.to_series() \n",
    "df_train = df\n",
    "\n",
    "# -----------------Test Data-----------------\n",
    "df_y_test = pd.read_excel('../../data/Test Data/Test Demographic Key.xlsx', sheet_name=0, engine=\"calamine\")\n",
    "df_test = pd.read_excel('../../data/Test Data/Test HR Data.xlsx', sheet_name=0, engine=\"calamine\") # test hr with description\n",
    "df_test.columns = df_test.columns.astype(str)\n",
    "df_test = label_death7d(df_test, df_y_test, id_col='TestID')\n",
    "df_demo_test = pd.read_excel('../../data/Test Data/Test Demographic Data.xlsx', sheet_name=0, engine=\"calamine\")\n",
    "df_test = df_test.merge(df_demo_test, on='TestID', how='left')\n",
    "df_test['rowid'] = df_test.index.to_series()\n",
    "df_test['VitalTime'] = df_test['Age']*24*60*60 # convert to second since birth\n",
    "df_test['VitalTime'] = df_test['VitalTime'].astype(int)\n",
    "rename_dict = {'TestID': 'VitalID'}\n",
    "df_test = df_test.rename(columns=rename_dict)\n",
    "\n",
    "df_desc_test = generate_descriptions_parallel(ts_df = df_test.loc[:, '1':'300'], id_df = df_test.loc[:, ['VitalID', 'VitalTime']])\n",
    "df_test = df_test.merge(df_desc_test, on=['VitalID', 'VitalTime'], how='left')\n",
    "df_test = text_gen_input_column(df_test, config_dict['text_config'])\n",
    "# df_test_org = df_test[df.columns]\n",
    "# df_test, df_leftout = train_test_split(df_test_org, test_size=0.5, stratify=df_test_org['cl_event'], random_state=random_state) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/65353 [00:00<?, ?it/s]/Users/joyqiu/Documents/Documents JoyQiu Work/Research/LLMTimeSeries/llm_nicu_vitalsigns/clip_env/lib/python3.11/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n",
      "/Users/joyqiu/Documents/Documents JoyQiu Work/Research/LLMTimeSeries/llm_nicu_vitalsigns/clip_env/lib/python3.11/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n",
      "/Users/joyqiu/Documents/Documents JoyQiu Work/Research/LLMTimeSeries/llm_nicu_vitalsigns/clip_env/lib/python3.11/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n",
      "/Users/joyqiu/Documents/Documents JoyQiu Work/Research/LLMTimeSeries/llm_nicu_vitalsigns/clip_env/lib/python3.11/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n",
      "/Users/joyqiu/Documents/Documents JoyQiu Work/Research/LLMTimeSeries/llm_nicu_vitalsigns/clip_env/lib/python3.11/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n",
      "/Users/joyqiu/Documents/Documents JoyQiu Work/Research/LLMTimeSeries/llm_nicu_vitalsigns/clip_env/lib/python3.11/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n",
      "/Users/joyqiu/Documents/Documents JoyQiu Work/Research/LLMTimeSeries/llm_nicu_vitalsigns/clip_env/lib/python3.11/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n",
      "/Users/joyqiu/Documents/Documents JoyQiu Work/Research/LLMTimeSeries/llm_nicu_vitalsigns/clip_env/lib/python3.11/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n",
      "/Users/joyqiu/Documents/Documents JoyQiu Work/Research/LLMTimeSeries/llm_nicu_vitalsigns/clip_env/lib/python3.11/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n",
      "  0%|          | 20/65353 [00:03<3:10:43,  5.71it/s]/Users/joyqiu/Documents/Documents JoyQiu Work/Research/LLMTimeSeries/llm_nicu_vitalsigns/clip_env/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/Users/joyqiu/Documents/Documents JoyQiu Work/Research/LLMTimeSeries/llm_nicu_vitalsigns/clip_env/lib/python3.11/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n",
      "  0%|          | 30/65353 [00:07<4:32:53,  3.99it/s]/Users/joyqiu/Documents/Documents JoyQiu Work/Research/LLMTimeSeries/llm_nicu_vitalsigns/clip_env/lib/python3.11/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n",
      "/Users/joyqiu/Documents/Documents JoyQiu Work/Research/LLMTimeSeries/llm_nicu_vitalsigns/clip_env/lib/python3.11/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n",
      "/Users/joyqiu/Documents/Documents JoyQiu Work/Research/LLMTimeSeries/llm_nicu_vitalsigns/clip_env/lib/python3.11/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n",
      "/Users/joyqiu/Documents/Documents JoyQiu Work/Research/LLMTimeSeries/llm_nicu_vitalsigns/clip_env/lib/python3.11/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n",
      "/Users/joyqiu/Documents/Documents JoyQiu Work/Research/LLMTimeSeries/llm_nicu_vitalsigns/clip_env/lib/python3.11/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n",
      "/Users/joyqiu/Documents/Documents JoyQiu Work/Research/LLMTimeSeries/llm_nicu_vitalsigns/clip_env/lib/python3.11/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n",
      "  0%|          | 40/65353 [00:10<5:17:00,  3.43it/s]/Users/joyqiu/Documents/Documents JoyQiu Work/Research/LLMTimeSeries/llm_nicu_vitalsigns/clip_env/lib/python3.11/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n",
      "/Users/joyqiu/Documents/Documents JoyQiu Work/Research/LLMTimeSeries/llm_nicu_vitalsigns/clip_env/lib/python3.11/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n",
      "/Users/joyqiu/Documents/Documents JoyQiu Work/Research/LLMTimeSeries/llm_nicu_vitalsigns/clip_env/lib/python3.11/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n",
      " 66%|██████▌   | 42930/65353 [1:37:03<46:43,  8.00it/s]  "
     ]
    }
   ],
   "source": [
    "df = df_train.loc[:, '1':'300'].copy()\n",
    "\n",
    "# Usage \n",
    "if __name__ == '__main__':\n",
    "    for by in ['euclidean']:\n",
    "        # Find furthest neighbors\n",
    "        furthest3 = find_furthest_neighbors(df, n_jobs=10, sample_size=df.shape[0], by = by)\n",
    "        # save the furthest_neighbors\n",
    "        np.save('./script/WSDIST/results/train_'+by+'_furthest3_'+str(df.shape[0])+'.npy', furthest3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_test.loc[:, '1':'300'].copy()\n",
    "\n",
    "# Usage \n",
    "if __name__ == '__main__':\n",
    "    for by in ['euclidean']:\n",
    "        # Find furthest neighbors\n",
    "        furthest3 = find_furthest_neighbors(df, n_jobs=8, sample_size=df.shape[0], by = by)\n",
    "        # save the furthest_neighbors\n",
    "        np.save('./script/WSDIST/results/test_'+by+'_furthest3_'+str(df.shape[0])+'.npy', furthest3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# load the furthest_neighbors\n",
    "euc_furthest3_train = np.load('./script/WSDIST/results/train_euclidean_furthest3_'+str(df.shape[0])+'.npy', allow_pickle=True)\n",
    "euc_furthest3_test = np.load('./script/WSDIST/results/test_euclidean_furthest3_'+str(df.shape[0])+'.npy', allow_pickle=True)\n",
    "\n",
    "# Create 3x2 subplot to take a look at negative samples\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "fig.suptitle('Euclidean vs LCSS Distance Comparisons', fontsize=16)\n",
    "\n",
    "for i in range(3):\n",
    "    # Left column: Euclidean distance comparisons\n",
    "    axes[i,0].plot(df.iloc[i].values, 'b-', label='Original', linewidth=2)\n",
    "    axes[i,0].plot(df.iloc[euc_furthest3_train.item()[i][0]].values, 'r--', alpha=0.7, label='Furthest 1')\n",
    "    axes[i,0].plot(df.iloc[euc_furthest3_train.item()[i][1]].values, 'g--', alpha=0.7, label='Furthest 2')\n",
    "    axes[i,0].plot(df.iloc[euc_furthest3_train.item()[i][2]].values, 'y--', alpha=0.7, label='Furthest 3')\n",
    "    axes[i,0].set_title(f'Euclidean Distance - Series {i}')\n",
    "    axes[i,0].grid(True)\n",
    "    if i == 0:\n",
    "        axes[i,0].legend()\n",
    "    \n",
    "    # Right column: LCSS distance comparisons\n",
    "    axes[i,1].plot(df.iloc[i].values, 'b-', label='Original', linewidth=2)\n",
    "    axes[i,1].plot(df.iloc[euc_furthest3_test.item()[i][0]].values, 'r--', alpha=0.7, label='Furthest 1')\n",
    "    axes[i,1].plot(df.iloc[euc_furthest3_test.item()[i][1]].values, 'g--', alpha=0.7, label='Furthest 2')\n",
    "    axes[i,1].plot(df.iloc[euc_furthest3_test.item()[i][2]].values, 'y--', alpha=0.7, label='Furthest 3')\n",
    "    axes[i,1].set_title(f'LCSS Distance - Series {i}')\n",
    "    axes[i,1].grid(True)\n",
    "    if i == 0:\n",
    "        axes[i,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Convert to dataframe\n",
    "data = []\n",
    "furthest_dict = euc_furthest3_train.item()\n",
    "for i in furthest_dict.keys():\n",
    "    data.append({\n",
    "        'euc_far_rowid1': euc_furthest3_train.item()[i][0],\n",
    "        'euc_far_rowid2': euc_furthest3_train.item()[i][1],\n",
    "        'euc_far_rowid3': euc_furthest3_train.item()[i][2]\n",
    "    })\n",
    "    \n",
    "df_furthest3 = pd.DataFrame(data)\n",
    "df_furthest3.to_csv('./data/train_furthest3.csv', index=False)\n",
    "data = []\n",
    "furthest_dict = euc_furthest3_test.item()\n",
    "for i in furthest_dict.keys():\n",
    "    data.append({\n",
    "        'euc_far_rowid1': euc_furthest3_test.item()[i][0],\n",
    "        'euc_far_rowid2': euc_furthest3_test.item()[i][1],\n",
    "        'euc_far_rowid3': euc_furthest3_test.item()[i][2]\n",
    "    })\n",
    "    \n",
    "df_furthest3 = pd.DataFrame(data)\n",
    "df_furthest3.to_csv('./data/test_furthest3.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
